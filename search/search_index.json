{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hello all and welcome to my blog. I am Sayam Kumar, an AI Research Engineer @ Phaidra where we create AI-powered control systems for industrial processes.</p> <p>I love to work in the field of AI because I am inspired by observing how AI positively impacts various domains like healthcare, astronomy, climate-change, e-commerce, etc.</p> <p>In my free time, I like to travel, play chess and listen to Punjabi/Hindi music.</p> <p>This year I am looking forward to dig deeper into Reinforcement Learning, improve my fitness consistently, and pick up learning violin.</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/","title":"Multilevel Modelling with Variational Inference","text":"In\u00a0[\u00a0]: Copied! <pre>import arviz as az\nimport logging\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc4 as pm\nimport tensorflow as tf\nimport xarray as xr\n\nfrom tensorflow_probability import bijectors as tfb\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n</pre> import arviz as az import logging import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc4 as pm import tensorflow as tf import xarray as xr  from tensorflow_probability import bijectors as tfb logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) In\u00a0[\u00a0]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\nRANDOM_SEED = 8927\nnp.random.seed(RANDOM_SEED)\naz.style.use('arviz-darkgrid')\n</pre> %config InlineBackend.figure_format = 'retina' RANDOM_SEED = 8927 np.random.seed(RANDOM_SEED) az.style.use('arviz-darkgrid') <p>Let's fetch the data and start analysing -</p> In\u00a0[\u00a0]: Copied! <pre>data = pd.read_csv(pm.utils.get_data('radon.csv'))\nu = np.log(data.Uppm).unique()\nmn_counties = data.county.unique()\nfloor = data.floor.values.astype(np.int32)\n\ncounties = len(mn_counties)\ncounty_lookup = dict(zip(mn_counties, range(counties)))\ncounty_idx = data['county_code'].values.astype(np.int32)\n</pre> data = pd.read_csv(pm.utils.get_data('radon.csv')) u = np.log(data.Uppm).unique() mn_counties = data.county.unique() floor = data.floor.values.astype(np.int32)  counties = len(mn_counties) county_lookup = dict(zip(mn_counties, range(counties))) county_idx = data['county_code'].values.astype(np.int32) In\u00a0[\u00a0]: Copied! <pre>data.head()\n</pre> data.head() Out[\u00a0]: Unnamed: 0 idnum state state2 stfips zip region typebldg floor room ... pcterr adjwt dupflag zipflag cntyfips county fips Uppm county_code log_radon 0 0 5081.0 MN MN 27.0 55735 5.0 1.0 1.0 3.0 ... 9.7 1146.499190 1.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.832909 1 1 5082.0 MN MN 27.0 55748 5.0 1.0 0.0 4.0 ... 14.5 471.366223 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.832909 2 2 5083.0 MN MN 27.0 55748 5.0 1.0 0.0 4.0 ... 9.6 433.316718 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 1.098612 3 3 5084.0 MN MN 27.0 56469 5.0 1.0 0.0 4.0 ... 24.3 461.623670 0.0 0.0 1.0 AITKIN 27001.0 0.502054 0 0.095310 4 4 5085.0 MN MN 27.0 55011 3.0 1.0 0.0 4.0 ... 13.8 433.316718 0.0 0.0 3.0 ANOKA 27003.0 0.428565 1 1.163151 <p>5 rows \u00d7 30 columns</p> In\u00a0[\u00a0]: Copied! <pre>@pm.model\ndef pooled_model():\n    a = yield pm.Normal('a', loc=0.0, scale=10.0, batch_stack=2)\n\n    loc = a[0] + a[1]*floor\n    scale = yield pm.Exponential(\"sigma\", rate=1.0)\n\n    y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values)\n</pre> @pm.model def pooled_model():     a = yield pm.Normal('a', loc=0.0, scale=10.0, batch_stack=2)      loc = a[0] + a[1]*floor     scale = yield pm.Exponential(\"sigma\", rate=1.0)      y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values) <p>Before running the model let\u2019s do some prior predictive checks. These help in incorporating scientific knowledge into our model.</p> In\u00a0[\u00a0]: Copied! <pre>prior_checks = pm.sample_prior_predictive(pooled_model())\nprior_checks\n</pre> prior_checks = pm.sample_prior_predictive(pooled_model()) prior_checks Out[\u00a0]: arviz.InferenceData <ul> <li> prior_predictive <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:               (chain: 1, draw: 1000, pooled_model/a_dim_0: 2, pooled_model/y_dim_0: 919)\nCoordinates:\n  * chain                 (chain) int64 0\n  * draw                  (draw) int64 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * pooled_model/a_dim_0  (pooled_model/a_dim_0) int64 0 1\n  * pooled_model/y_dim_0  (pooled_model/y_dim_0) int64 0 1 2 3 ... 916 917 918\nData variables:\n    pooled_model/a        (chain, draw, pooled_model/a_dim_0) float32 3.73589...\n    pooled_model/sigma    (chain, draw) float32 0.124252275 ... 0.1704529\n    pooled_model/y        (chain, draw, pooled_model/y_dim_0) float32 -11.425...\nAttributes:\n    created_at:     2020-09-06T15:12:11.500262\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 1000</li><li>pooled_model/a_dim_0: 2</li><li>pooled_model/y_dim_0: 919</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>pooled_model/a_dim_0(pooled_model/a_dim_0)int640 1<pre>array([0, 1])</pre></li><li>pooled_model/y_dim_0(pooled_model/y_dim_0)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (3)<ul><li>pooled_model/a(chain, draw, pooled_model/a_dim_0)float323.7358973 -15.320486 ... -6.4341984<pre>array([[[  3.7358973 , -15.320486  ],\n        [ -8.676756  ,  12.327385  ],\n        [  7.349168  , -11.632883  ],\n        ...,\n        [-17.763176  ,   3.9943478 ],\n        [-21.435543  ,  -0.07876515],\n        [  6.5087595 ,  -6.4341984 ]]], dtype=float32)</pre></li><li>pooled_model/sigma(chain, draw)float320.124252275 ... 0.1704529<pre>array([[1.24252275e-01, 2.18166053e-01, 1.73134065e+00, 1.87080002e+00,\n        3.86758149e-01, 5.85898086e-02, 6.60767257e-01, 2.19248462e+00,\n        1.47937405e+00, 2.62981963e+00, 4.73047018e+00, 9.72903013e-01,\n        2.52875018e+00, 8.60245943e-01, 8.71204972e-01, 1.35467723e-01,\n        1.69988155e+00, 7.30032027e-02, 9.49807644e-01, 1.09355628e+00,\n        3.70404184e-01, 3.09200644e+00, 2.13690877e-01, 2.92978317e-01,\n        3.86870742e+00, 9.20100808e-01, 1.02149987e+00, 2.77314210e+00,\n        2.70368147e+00, 1.73186255e+00, 1.76331401e+00, 4.39246774e-01,\n        1.77190304e+00, 2.80357766e+00, 5.52719057e-01, 2.90537804e-01,\n        1.94052780e+00, 1.34999350e-01, 2.91218370e-01, 5.11170149e-01,\n        1.15133429e+00, 1.17300548e-01, 5.86179018e-01, 5.14345884e-01,\n        8.83724809e-01, 7.76633382e-01, 1.10540688e+00, 1.08257413e-01,\n        5.25177084e-02, 3.14549160e+00, 6.67104274e-02, 4.64121342e-01,\n        7.31377482e-01, 1.71201837e+00, 1.87058103e+00, 2.11804080e+00,\n        6.23107910e-01, 1.00311029e+00, 1.93300366e-01, 4.71577160e-02,\n        3.09122745e-02, 9.28924203e-01, 9.97797132e-01, 2.21684054e-01,\n        1.07126725e+00, 6.38897419e-01, 8.48415494e-01, 1.69924951e+00,\n        3.50955153e+00, 2.81853676e+00, 1.37188005e+00, 2.09045768e-01,\n        9.41089094e-01, 4.14538980e-01, 2.02648091e+00, 1.84433746e+00,\n        8.94062296e-02, 1.76502556e-01, 4.57081646e-01, 1.40374005e+00,\n...\n        1.14232793e-01, 7.78646648e-01, 3.45841944e-01, 3.75480473e-01,\n        1.28782916e+00, 4.15802926e-01, 8.08096603e-02, 5.22279032e-02,\n        8.44400525e-01, 1.47122109e+00, 2.81988353e-01, 3.39085937e+00,\n        2.95001197e+00, 1.06472814e+00, 3.15820456e-01, 4.45387554e+00,\n        5.07715106e-01, 1.19484656e-01, 1.16057575e+00, 3.00743866e+00,\n        1.65124059e+00, 5.88920474e-01, 3.90195608e-01, 3.77834588e-01,\n        1.22497284e+00, 2.00244546e-01, 4.09505934e-01, 8.67544949e-01,\n        5.33267632e-02, 8.05767238e-01, 2.12663269e+00, 3.38898778e-01,\n        4.54715788e-01, 8.15032780e-01, 4.91176903e-01, 9.15093780e-01,\n        1.57230064e-01, 1.38862503e+00, 1.08501464e-01, 6.69538558e-01,\n        8.17155182e-01, 1.05785382e+00, 9.57605004e-01, 9.24436986e-01,\n        4.19909686e-01, 1.45669365e+00, 4.69646358e+00, 3.91691655e-01,\n        3.20592582e-01, 6.58347085e-02, 1.61505580e-01, 5.05329669e-01,\n        8.30124795e-01, 4.04626340e-01, 1.40436232e+00, 2.45392132e+00,\n        2.53958583e-01, 8.45379829e-01, 3.96697491e-01, 1.41434446e-01,\n        5.29461503e-01, 2.00267315e+00, 6.58604681e-01, 1.73856175e+00,\n        5.13490498e-01, 1.29534507e+00, 5.09042621e-01, 2.11369276e+00,\n        5.72610974e-01, 1.51463377e+00, 1.18161130e+00, 2.33294666e-01,\n        1.43027735e+00, 1.33725750e+00, 3.33844095e-01, 1.70452893e-01]],\n      dtype=float32)</pre></li><li>pooled_model/y(chain, draw, pooled_model/y_dim_0)float32-11.425111 3.614398 ... 6.6331916<pre>array([[[-11.425111  ,   3.614398  ,   3.709121  , ...,   3.78737   ,\n           3.7087603 ,   3.7606945 ],\n        [  3.6845543 ,  -8.816672  ,  -9.096872  , ...,  -8.230866  ,\n          -8.635344  ,  -8.8506    ],\n        [ -1.7254968 ,   9.610426  ,   6.3633847 , ...,   7.3794913 ,\n           6.3264837 ,   7.831397  ],\n        ...,\n        [-12.150305  , -19.475933  , -15.784439  , ..., -14.951879  ,\n         -21.717302  , -16.133024  ],\n        [-21.319092  , -20.926788  , -20.77283   , ..., -21.46507   ,\n         -22.080853  , -21.092203  ],\n        [  0.30990508,   6.4175243 ,   6.4763956 , ...,   6.4609776 ,\n           6.825876  ,   6.6331916 ]]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:11.500262arviz_version :0.9.0</li></ul> </ul> </li> </ul> <p>To make our lives easier during plotting and diagonsing while using ArviZ, we define a function <code>remove_scope</code> for renaming all variables in InferenceData to their actual distribution name.</p> In\u00a0[\u00a0]: Copied! <pre>def remove_scope(idata):\n    for group in idata._groups:\n        for var in getattr(idata, group).variables:\n            if \"/\" in var:\n                idata.rename(name_dict={var: var.split(\"/\")[-1]}, inplace=True)\n    idata.rename(name_dict={\"y_dim_0\": \"obs_id\"}, inplace=True)\n</pre> def remove_scope(idata):     for group in idata._groups:         for var in getattr(idata, group).variables:             if \"/\" in var:                 idata.rename(name_dict={var: var.split(\"/\")[-1]}, inplace=True)     idata.rename(name_dict={\"y_dim_0\": \"obs_id\"}, inplace=True) In\u00a0[\u00a0]: Copied! <pre>remove_scope(prior_checks)\nprior_checks\n</pre> remove_scope(prior_checks) prior_checks Out[\u00a0]: arviz.InferenceData <ul> <li> prior_predictive <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (a_dim_0: 2, chain: 1, draw: 1000, obs_id: 919)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * a_dim_0  (a_dim_0) int64 0 1\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    a        (chain, draw, a_dim_0) float32 3.7358973 -15.320486 ... -6.4341984\n    sigma    (chain, draw) float32 0.124252275 0.21816605 ... 0.1704529\n    y        (chain, draw, obs_id) float32 -11.425111 3.614398 ... 6.6331916\nAttributes:\n    created_at:     2020-09-06T15:12:11.500262\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>a_dim_0: 2</li><li>chain: 1</li><li>draw: 1000</li><li>obs_id: 919</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999<pre>array([  0,   1,   2, ..., 997, 998, 999])</pre></li><li>a_dim_0(a_dim_0)int640 1<pre>array([0, 1])</pre></li><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (3)<ul><li>a(chain, draw, a_dim_0)float323.7358973 -15.320486 ... -6.4341984<pre>array([[[  3.7358973 , -15.320486  ],\n        [ -8.676756  ,  12.327385  ],\n        [  7.349168  , -11.632883  ],\n        ...,\n        [-17.763176  ,   3.9943478 ],\n        [-21.435543  ,  -0.07876515],\n        [  6.5087595 ,  -6.4341984 ]]], dtype=float32)</pre></li><li>sigma(chain, draw)float320.124252275 ... 0.1704529<pre>array([[1.24252275e-01, 2.18166053e-01, 1.73134065e+00, 1.87080002e+00,\n        3.86758149e-01, 5.85898086e-02, 6.60767257e-01, 2.19248462e+00,\n        1.47937405e+00, 2.62981963e+00, 4.73047018e+00, 9.72903013e-01,\n        2.52875018e+00, 8.60245943e-01, 8.71204972e-01, 1.35467723e-01,\n        1.69988155e+00, 7.30032027e-02, 9.49807644e-01, 1.09355628e+00,\n        3.70404184e-01, 3.09200644e+00, 2.13690877e-01, 2.92978317e-01,\n        3.86870742e+00, 9.20100808e-01, 1.02149987e+00, 2.77314210e+00,\n        2.70368147e+00, 1.73186255e+00, 1.76331401e+00, 4.39246774e-01,\n        1.77190304e+00, 2.80357766e+00, 5.52719057e-01, 2.90537804e-01,\n        1.94052780e+00, 1.34999350e-01, 2.91218370e-01, 5.11170149e-01,\n        1.15133429e+00, 1.17300548e-01, 5.86179018e-01, 5.14345884e-01,\n        8.83724809e-01, 7.76633382e-01, 1.10540688e+00, 1.08257413e-01,\n        5.25177084e-02, 3.14549160e+00, 6.67104274e-02, 4.64121342e-01,\n        7.31377482e-01, 1.71201837e+00, 1.87058103e+00, 2.11804080e+00,\n        6.23107910e-01, 1.00311029e+00, 1.93300366e-01, 4.71577160e-02,\n        3.09122745e-02, 9.28924203e-01, 9.97797132e-01, 2.21684054e-01,\n        1.07126725e+00, 6.38897419e-01, 8.48415494e-01, 1.69924951e+00,\n        3.50955153e+00, 2.81853676e+00, 1.37188005e+00, 2.09045768e-01,\n        9.41089094e-01, 4.14538980e-01, 2.02648091e+00, 1.84433746e+00,\n        8.94062296e-02, 1.76502556e-01, 4.57081646e-01, 1.40374005e+00,\n...\n        1.14232793e-01, 7.78646648e-01, 3.45841944e-01, 3.75480473e-01,\n        1.28782916e+00, 4.15802926e-01, 8.08096603e-02, 5.22279032e-02,\n        8.44400525e-01, 1.47122109e+00, 2.81988353e-01, 3.39085937e+00,\n        2.95001197e+00, 1.06472814e+00, 3.15820456e-01, 4.45387554e+00,\n        5.07715106e-01, 1.19484656e-01, 1.16057575e+00, 3.00743866e+00,\n        1.65124059e+00, 5.88920474e-01, 3.90195608e-01, 3.77834588e-01,\n        1.22497284e+00, 2.00244546e-01, 4.09505934e-01, 8.67544949e-01,\n        5.33267632e-02, 8.05767238e-01, 2.12663269e+00, 3.38898778e-01,\n        4.54715788e-01, 8.15032780e-01, 4.91176903e-01, 9.15093780e-01,\n        1.57230064e-01, 1.38862503e+00, 1.08501464e-01, 6.69538558e-01,\n        8.17155182e-01, 1.05785382e+00, 9.57605004e-01, 9.24436986e-01,\n        4.19909686e-01, 1.45669365e+00, 4.69646358e+00, 3.91691655e-01,\n        3.20592582e-01, 6.58347085e-02, 1.61505580e-01, 5.05329669e-01,\n        8.30124795e-01, 4.04626340e-01, 1.40436232e+00, 2.45392132e+00,\n        2.53958583e-01, 8.45379829e-01, 3.96697491e-01, 1.41434446e-01,\n        5.29461503e-01, 2.00267315e+00, 6.58604681e-01, 1.73856175e+00,\n        5.13490498e-01, 1.29534507e+00, 5.09042621e-01, 2.11369276e+00,\n        5.72610974e-01, 1.51463377e+00, 1.18161130e+00, 2.33294666e-01,\n        1.43027735e+00, 1.33725750e+00, 3.33844095e-01, 1.70452893e-01]],\n      dtype=float32)</pre></li><li>y(chain, draw, obs_id)float32-11.425111 3.614398 ... 6.6331916<pre>array([[[-11.425111  ,   3.614398  ,   3.709121  , ...,   3.78737   ,\n           3.7087603 ,   3.7606945 ],\n        [  3.6845543 ,  -8.816672  ,  -9.096872  , ...,  -8.230866  ,\n          -8.635344  ,  -8.8506    ],\n        [ -1.7254968 ,   9.610426  ,   6.3633847 , ...,   7.3794913 ,\n           6.3264837 ,   7.831397  ],\n        ...,\n        [-12.150305  , -19.475933  , -15.784439  , ..., -14.951879  ,\n         -21.717302  , -16.133024  ],\n        [-21.319092  , -20.926788  , -20.77283   , ..., -21.46507   ,\n         -22.080853  , -21.092203  ],\n        [  0.30990508,   6.4175243 ,   6.4763956 , ...,   6.4609776 ,\n           6.825876  ,   6.6331916 ]]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:11.500262arviz_version :0.9.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots()\nprior_checks.assign_coords(coords={\"a_dim_0\": [\"Basement\", \" First Floor\"]}, inplace=True)\nprior_checks.prior_predictive.plot.scatter(x=\"a_dim_0\", y=\"a\", color=\"k\", alpha=0.2, ax=ax)\nax.set(xlabel=\"Level\", ylabel=\"Radon level (Log Scale)\");\n</pre> _, ax = plt.subplots() prior_checks.assign_coords(coords={\"a_dim_0\": [\"Basement\", \" First Floor\"]}, inplace=True) prior_checks.prior_predictive.plot.scatter(x=\"a_dim_0\", y=\"a\", color=\"k\", alpha=0.2, ax=ax) ax.set(xlabel=\"Level\", ylabel=\"Radon level (Log Scale)\"); <p>As there is no <code>coords</code> and <code>dims</code> integration to PyMC4's ModelTemplate, we need a bit extra manipulations to handle them. Here we need to assign_coords to dimensions of variable <code>a</code> to consider <code>Basement</code> and <code>First Floor</code>.</p> <p>Before seeing the data, these priors seem to allow for quite a wide range of the mean log radon level. Let's fire up Variational Inference machinery and fit the model -</p> In\u00a0[\u00a0]: Copied! <pre>pooled_advi = pm.fit(pooled_model(), num_steps=25_000)\n</pre> pooled_advi = pm.fit(pooled_model(), num_steps=25_000) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_elbo(loss):\n    plt.plot(loss)\n    plt.yscale(\"log\")\n    plt.xlabel(\"Number of iterations\")\n    plt.ylabel(\"Negative log(ELBO)\")\n\nplot_elbo(pooled_advi.losses)\n</pre> def plot_elbo(loss):     plt.plot(loss)     plt.yscale(\"log\")     plt.xlabel(\"Number of iterations\")     plt.ylabel(\"Negative log(ELBO)\")  plot_elbo(pooled_advi.losses) <p>Looks good, ELBO seems to have converged. As a sanity check, we will plot ELBO each time after fitting a new model to figure out its convergence.</p> <p>Now, we'll draw samples from the posterior distribution. And then, pass these samples to <code>sample_posterior_predictive</code> to estimate the uncertainty at Basement and First Floor radon levels.</p> In\u00a0[\u00a0]: Copied! <pre>pooled_advi_samples = pooled_advi.approximation.sample(2_000)\npooled_advi_samples\n</pre> pooled_advi_samples = pooled_advi.approximation.sample(2_000) pooled_advi_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                   (chain: 1, draw: 2000, pooled_model/a_dim_0: 2)\nCoordinates:\n  * chain                     (chain) int64 0\n  * draw                      (draw) int64 0 1 2 3 4 ... 1996 1997 1998 1999\n  * pooled_model/a_dim_0      (pooled_model/a_dim_0) int64 0 1\nData variables:\n    pooled_model/a            (chain, draw, pooled_model/a_dim_0) float32 1.4...\n    pooled_model/__log_sigma  (chain, draw) float32 -0.262588 ... -0.23297022\n    pooled_model/sigma        (chain, draw) float32 0.7690587 ... 0.79217714\nAttributes:\n    created_at:     2020-09-06T15:12:24.652480\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 2000</li><li>pooled_model/a_dim_0: 2</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999<pre>array([   0,    1,    2, ..., 1997, 1998, 1999])</pre></li><li>pooled_model/a_dim_0(pooled_model/a_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (3)<ul><li>pooled_model/a(chain, draw, pooled_model/a_dim_0)float321.4208013 ... -0.6438127<pre>array([[[ 1.4208013 , -0.50706375],\n        [ 1.4203647 , -0.52226704],\n        [ 1.4275815 , -0.52768236],\n        ...,\n        [ 1.3540957 , -0.6212556 ],\n        [ 1.3802642 , -0.6262389 ],\n        [ 1.3690256 , -0.6438127 ]]], dtype=float32)</pre></li><li>pooled_model/__log_sigma(chain, draw)float32-0.262588 ... -0.23297022<pre>array([[-0.262588  , -0.24097891, -0.22231792, ..., -0.28082496,\n        -0.22691318, -0.23297022]], dtype=float32)</pre></li><li>pooled_model/sigma(chain, draw)float320.7690587 0.7858582 ... 0.79217714<pre>array([[0.7690587 , 0.7858582 , 0.8006608 , ..., 0.7551605 , 0.79699   ,\n        0.79217714]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:24.652480arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:               (pooled_model/y_dim_0: 919)\nCoordinates:\n  * pooled_model/y_dim_0  (pooled_model/y_dim_0) int64 0 1 2 3 ... 916 917 918\nData variables:\n    pooled_model/y        (pooled_model/y_dim_0) float64 0.8329 0.8329 ... 1.099\nAttributes:\n    created_at:     2020-09-06T15:12:24.654109\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>pooled_model/y_dim_0: 919</li></ul></li><li>Coordinates: (1)<ul><li>pooled_model/y_dim_0(pooled_model/y_dim_0)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>pooled_model/y(pooled_model/y_dim_0)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:24.654109arviz_version :0.9.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>posterior_predictive = pm.sample_posterior_predictive(pooled_model(), pooled_advi_samples)\nremove_scope(posterior_predictive)\nposterior_predictive\n</pre> posterior_predictive = pm.sample_posterior_predictive(pooled_model(), pooled_advi_samples) remove_scope(posterior_predictive) posterior_predictive Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:      (a_dim_0: 2, chain: 1, draw: 2000)\nCoordinates:\n  * chain        (chain) int64 0\n  * draw         (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * a_dim_0      (a_dim_0) int64 0 1\nData variables:\n    a            (chain, draw, a_dim_0) float32 1.4208013 ... -0.6438127\n    __log_sigma  (chain, draw) float32 -0.262588 -0.24097891 ... -0.23297022\n    sigma        (chain, draw) float32 0.7690587 0.7858582 ... 0.79217714\nAttributes:\n    created_at:     2020-09-06T15:12:24.652480\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>a_dim_0: 2</li><li>chain: 1</li><li>draw: 2000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999<pre>array([   0,    1,    2, ..., 1997, 1998, 1999])</pre></li><li>a_dim_0(a_dim_0)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (3)<ul><li>a(chain, draw, a_dim_0)float321.4208013 ... -0.6438127<pre>array([[[ 1.4208013 , -0.50706375],\n        [ 1.4203647 , -0.52226704],\n        [ 1.4275815 , -0.52768236],\n        ...,\n        [ 1.3540957 , -0.6212556 ],\n        [ 1.3802642 , -0.6262389 ],\n        [ 1.3690256 , -0.6438127 ]]], dtype=float32)</pre></li><li>__log_sigma(chain, draw)float32-0.262588 ... -0.23297022<pre>array([[-0.262588  , -0.24097891, -0.22231792, ..., -0.28082496,\n        -0.22691318, -0.23297022]], dtype=float32)</pre></li><li>sigma(chain, draw)float320.7690587 0.7858582 ... 0.79217714<pre>array([[0.7690587 , 0.7858582 , 0.8006608 , ..., 0.7551605 , 0.79699   ,\n        0.79217714]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:24.652480arviz_version :0.9.0</li></ul> </ul> </li> <li> posterior_predictive <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (chain: 1, draw: 2000, obs_id: 919)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (chain, draw, obs_id) float32 0.8419045 0.9926412 ... 0.36609662\nAttributes:\n    created_at:     2020-09-06T15:12:25.247406\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>chain: 1</li><li>draw: 2000</li><li>obs_id: 919</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999<pre>array([   0,    1,    2, ..., 1997, 1998, 1999])</pre></li><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(chain, draw, obs_id)float320.8419045 0.9926412 ... 0.36609662<pre>array([[[ 0.8419045 ,  0.9926412 ,  2.3756475 , ...,  2.6291053 ,\n          0.52533627,  2.0592532 ],\n        [ 0.24704647,  0.86674404,  1.573486  , ...,  1.0467045 ,\n          0.03541672,  1.5265828 ],\n        [ 0.64094734,  2.8461373 ,  1.59538   , ...,  1.4419069 ,\n          2.3063586 ,  1.2692662 ],\n        ...,\n        [ 0.20850796,  2.1369853 ,  1.350267  , ...,  0.96725345,\n          2.351105  ,  2.197761  ],\n        [ 1.2758532 ,  1.1836511 , -0.5104476 , ...,  0.55777067,\n          1.732587  ,  0.874024  ],\n        [ 0.44130206,  0.70061743,  0.7445947 , ...,  1.6418209 ,\n          3.2866702 ,  0.36609662]]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:25.247406arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:12:24.654109\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:24.654109arviz_version :0.9.0</li></ul> </ul> </li> </ul> <p>We now want to calculate the highest density interval given by the posterior predictive on Radon levels. However, we are not interested in the HDI of each observation but in the HDI of each level (either Basement or First Floor). We first group posterior_predictive samples using <code>coords</code> and then pass the specific dimensions (\"chain\", \"draw\", \"obs_id\") to <code>az.hdi</code>.</p> In\u00a0[\u00a0]: Copied! <pre>floor = xr.DataArray(floor, dims=(\"obs_id\"))\nhdi_helper = lambda ds: az.hdi(ds, input_core_dims=[[\"chain\", \"draw\", \"obs_id\"]])\nhdi_ppc = posterior_predictive.posterior_predictive[\"y\"].groupby(floor).apply(hdi_helper)[\"y\"]\nhdi_ppc\n</pre> floor = xr.DataArray(floor, dims=(\"obs_id\")) hdi_helper = lambda ds: az.hdi(ds, input_core_dims=[[\"chain\", \"draw\", \"obs_id\"]]) hdi_ppc = posterior_predictive.posterior_predictive[\"y\"].groupby(floor).apply(hdi_helper)[\"y\"] hdi_ppc Out[\u00a0]: <pre>&lt;xarray.DataArray 'y' (group: 2, hdi: 2)&gt;\narray([[-0.12597895,  2.84736681],\n       [-0.7139045 ,  2.26243448]])\nCoordinates:\n  * hdi      (hdi) &lt;U6 'lower' 'higher'\n  * group    (group) int64 0 1</pre>xarray.DataArray'y'<ul><li>group: 2</li><li>hdi: 2</li></ul><ul><li>-0.126 2.847 -0.7139 2.262<pre>array([[-0.12597895,  2.84736681],\n       [-0.7139045 ,  2.26243448]])</pre></li><li>Coordinates: (2)<ul><li>hdi(hdi)&lt;U6'lower' 'higher'hdi_prob :0.94<pre>array(['lower', 'higher'], dtype='&lt;U6')</pre></li><li>group(group)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Attributes: (0)</li></ul> <p>In addition, ArviZ has also included the hdi_prob as an attribute of the hdi coordinate, click on its file icon to see it!</p> <p>We will now add one extra coordinate to the observed_data group: the Level labels (not indices). This will allow xarray to automatically generate the correct xlabel and xticklabels so we don\u2019t have to worry about labeling too much. In this particular case we will only do one plot, which makes the adding of a coordinate a bit of an overkill. In many cases however, we will have several plots and using this approach will automate labeling for all plots. Eventually, we will sort by Level coordinate to make sure Basement is the first value and goes at the left of the plot.</p> In\u00a0[\u00a0]: Copied! <pre>posterior_predictive.rename(name_dict={\"a_dim_0\": \"Level\"}, inplace=True)\nposterior_predictive.assign_coords({\"Level\": [\"Basement\", \"First Floor\"]}, inplace=True)\nlevel_labels = posterior_predictive.posterior.Level[floor]\nposterior_predictive.observed_data = posterior_predictive.observed_data.assign_coords(Level=level_labels).sortby(\"Level\")\n</pre> posterior_predictive.rename(name_dict={\"a_dim_0\": \"Level\"}, inplace=True) posterior_predictive.assign_coords({\"Level\": [\"Basement\", \"First Floor\"]}, inplace=True) level_labels = posterior_predictive.posterior.Level[floor] posterior_predictive.observed_data = posterior_predictive.observed_data.assign_coords(Level=level_labels).sortby(\"Level\") <p>Plot the point estimates of the slope and intercept for the complete pooling model.</p> In\u00a0[\u00a0]: Copied! <pre>xvals = xr.DataArray([0, 1], dims=\"Level\", coords={\"Level\": [\"Basement\", \"First Floor\"]})\nposterior_predictive.posterior[\"a\"] = posterior_predictive.posterior.a[:, :, 0] + posterior_predictive.posterior.a[:, :, 1] * xvals\npooled_means = posterior_predictive.posterior.mean(dim=(\"chain\", \"draw\"))\n\n_, ax = plt.subplots()\nposterior_predictive.observed_data.plot.scatter(x=\"Level\", y=\"y\", label=\"Observations\", alpha=0.4, ax=ax)\n\naz.plot_hdi(\n    [0, 1], hdi_data=hdi_ppc, fill_kwargs={\"alpha\": 0.2, \"label\": \"Exp. distrib. of Radon levels\"}, ax=ax\n)\n\naz.plot_hdi(\n    [0, 1], posterior_predictive.posterior.a, fill_kwargs={\"alpha\": 0.5, \"label\": \"Exp. mean HPD\"}, ax=ax\n)\nax.plot([0, 1], pooled_means.a, label=\"Exp. mean\")\n\nax.set_ylabel(\"Log radon level\")\nax.legend(ncol=2, fontsize=9, frameon=True);\n</pre> xvals = xr.DataArray([0, 1], dims=\"Level\", coords={\"Level\": [\"Basement\", \"First Floor\"]}) posterior_predictive.posterior[\"a\"] = posterior_predictive.posterior.a[:, :, 0] + posterior_predictive.posterior.a[:, :, 1] * xvals pooled_means = posterior_predictive.posterior.mean(dim=(\"chain\", \"draw\"))  _, ax = plt.subplots() posterior_predictive.observed_data.plot.scatter(x=\"Level\", y=\"y\", label=\"Observations\", alpha=0.4, ax=ax)  az.plot_hdi(     [0, 1], hdi_data=hdi_ppc, fill_kwargs={\"alpha\": 0.2, \"label\": \"Exp. distrib. of Radon levels\"}, ax=ax )  az.plot_hdi(     [0, 1], posterior_predictive.posterior.a, fill_kwargs={\"alpha\": 0.5, \"label\": \"Exp. mean HPD\"}, ax=ax ) ax.plot([0, 1], pooled_means.a, label=\"Exp. mean\")  ax.set_ylabel(\"Log radon level\") ax.legend(ncol=2, fontsize=9, frameon=True); <p>The 94% interval of the expected value is very narrow, and even narrower for basement measurements, meaning that the model is slightly more confident about these observations. The sampling distribution of individual radon levels is much wider. We can infer that floor level does account for some of the variation in radon levels. We can see however that the model underestimates the dispersion in radon levels across households \u2013 lots of them lie outside the light orange prediction envelope. Also, the error rates are high representing high bias. So this model is a good start but we can\u2019t stop there.</p> In\u00a0[\u00a0]: Copied! <pre>@pm.model\ndef unpooled_model():\n    a_county = yield pm.Normal('a_county', loc=0., scale=10., batch_stack=counties)\n    beta = yield pm.Normal('beta', loc=0, scale=10.)\n    loc = tf.gather(a_county, county_idx) + beta*floor\n    scale = yield pm.Exponential(\"sigma\", rate=1.)\n\n    y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values)\n</pre> @pm.model def unpooled_model():     a_county = yield pm.Normal('a_county', loc=0., scale=10., batch_stack=counties)     beta = yield pm.Normal('beta', loc=0, scale=10.)     loc = tf.gather(a_county, county_idx) + beta*floor     scale = yield pm.Exponential(\"sigma\", rate=1.)      y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values) In\u00a0[\u00a0]: Copied! <pre>unpooled_advi = pm.fit(unpooled_model(), num_steps=25_000)\nplot_elbo(unpooled_advi.losses)\n</pre> unpooled_advi = pm.fit(unpooled_model(), num_steps=25_000) plot_elbo(unpooled_advi.losses) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>unpooled_advi_samples = unpooled_advi.approximation.sample(2_000)\nremove_scope(unpooled_advi_samples)\nunpooled_advi_samples\n</pre> unpooled_advi_samples = unpooled_advi.approximation.sample(2_000) remove_scope(unpooled_advi_samples) unpooled_advi_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:         (a_county_dim_0: 85, chain: 1, draw: 2000)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * a_county_dim_0  (a_county_dim_0) int64 0 1 2 3 4 5 6 ... 79 80 81 82 83 84\nData variables:\n    a_county        (chain, draw, a_county_dim_0) float32 0.36999083 ... 1.03...\n    beta            (chain, draw) float32 -0.56419456 -0.76161397 ... -0.6635491\n    __log_sigma     (chain, draw) float32 -0.28598142 -0.28851208 ... -0.3346977\n    sigma           (chain, draw) float32 0.7512766 0.7493777 ... 0.71555436\nAttributes:\n    created_at:     2020-09-06T15:12:43.221131\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>a_county_dim_0: 85</li><li>chain: 1</li><li>draw: 2000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999<pre>array([   0,    1,    2, ..., 1997, 1998, 1999])</pre></li><li>a_county_dim_0(a_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li></ul></li><li>Data variables: (4)<ul><li>a_county(chain, draw, a_county_dim_0)float320.36999083 1.0238843 ... 1.0384322<pre>array([[[0.36999083, 1.0238843 , 2.0252652 , ..., 2.034409  ,\n         1.578296  , 0.9983911 ],\n        [0.64774776, 0.91157115, 1.2923429 , ..., 2.0109618 ,\n         1.6982571 , 0.8811697 ],\n        [1.8972617 , 0.9602209 , 1.4117332 , ..., 1.5687102 ,\n         1.779839  , 1.200284  ],\n        ...,\n        [0.7107576 , 0.82900226, 2.0056863 , ..., 1.8268906 ,\n         1.7657291 , 0.28820467],\n        [1.2736075 , 1.03682   , 1.1554356 , ..., 1.5688866 ,\n         1.4610525 , 1.1454637 ],\n        [0.3937166 , 0.88760495, 1.3843511 , ..., 1.773532  ,\n         1.7758514 , 1.0384322 ]]], dtype=float32)</pre></li><li>beta(chain, draw)float32-0.56419456 ... -0.6635491<pre>array([[-0.56419456, -0.76161397, -0.6025331 , ..., -0.7432619 ,\n        -0.65681094, -0.6635491 ]], dtype=float32)</pre></li><li>__log_sigma(chain, draw)float32-0.28598142 ... -0.3346977<pre>array([[-0.28598142, -0.28851208, -0.29483494, ..., -0.331497  ,\n        -0.2910944 , -0.3346977 ]], dtype=float32)</pre></li><li>sigma(chain, draw)float320.7512766 0.7493777 ... 0.71555436<pre>array([[0.7512766 , 0.7493777 , 0.7446545 , ..., 0.7178483 , 0.7474451 ,\n        0.71555436]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:43.221131arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:12:43.223073\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:12:43.223073arviz_version :0.9.0</li></ul> </ul> </li> </ul> <p>Let\u2019s plot each county's expected values with 94% confidence interval.</p> In\u00a0[\u00a0]: Copied! <pre>unpooled_advi_samples.assign_coords(coords={\"a_county_dim_0\": mn_counties}, inplace=True)\naz.plot_forest(\n    unpooled_advi_samples, var_names=\"a_county\", figsize=(6, 16), combined=True, textsize=8\n);\n</pre> unpooled_advi_samples.assign_coords(coords={\"a_county_dim_0\": mn_counties}, inplace=True) az.plot_forest(     unpooled_advi_samples, var_names=\"a_county\", figsize=(6, 16), combined=True, textsize=8 ); <p>Looking at the counties all together, the <code>unpooling</code> analysis overfits the data within each county. Also giving a view that individual counties look more different than they actually are.</p> <p>Since we are modelling data within each county, we can plot the ordered mean estimates to identify counties with high radon levels.</p> In\u00a0[\u00a0]: Copied! <pre>unpooled_means = unpooled_advi_samples.posterior.mean(dim=(\"chain\", \"draw\"))\nunpooled_hdi = az.hdi(unpooled_advi_samples)\n</pre> unpooled_means = unpooled_advi_samples.posterior.mean(dim=(\"chain\", \"draw\")) unpooled_hdi = az.hdi(unpooled_advi_samples) <p>We will now take advantage of label based indexing for Datasets with the sel method and of automagical sorting capabilities. We first sort using the values of a specific 1D variable <code>a</code>. Then, thanks to unpooled_means and unpooled_hdi both having the <code>a_county_dim_0</code> (representing each county) dimension, we can pass a 1D DataArray to sort the second dataset too.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(7, 5))\nxticks = np.arange(0, 86, 6)\nfontdict = {\"horizontalalignment\": \"right\", \"fontsize\": 10}\n\nunpooled_means_iter = unpooled_means.sortby(\"a_county\")\nunpooled_hdi_iter = unpooled_hdi.sortby(unpooled_means_iter[\"a_county\"])\nunpooled_means_iter.plot.scatter(x=f\"a_county_dim_0\", y=\"a_county\", ax=ax, alpha=0.8)\nax.vlines(\n    np.arange(counties),\n    unpooled_hdi_iter[\"a_county\"].sel(hdi=\"lower\"),\n    unpooled_hdi_iter[\"a_county\"].sel(hdi=\"higher\"),\n    color=\"orange\", alpha=0.6\n)\nax.set(\n    ylabel=\"Radon estimate\",\n    xlabel=\"Ordered County\",\n    ylim=(-1, 4.5),\n    xticks=xticks\n)\nax.set_xticklabels(unpooled_means_iter[f\"a_county_dim_0\"].values[xticks], fontdict=fontdict)\nax.tick_params(rotation=30)\n</pre> fig, ax = plt.subplots(figsize=(7, 5)) xticks = np.arange(0, 86, 6) fontdict = {\"horizontalalignment\": \"right\", \"fontsize\": 10}  unpooled_means_iter = unpooled_means.sortby(\"a_county\") unpooled_hdi_iter = unpooled_hdi.sortby(unpooled_means_iter[\"a_county\"]) unpooled_means_iter.plot.scatter(x=f\"a_county_dim_0\", y=\"a_county\", ax=ax, alpha=0.8) ax.vlines(     np.arange(counties),     unpooled_hdi_iter[\"a_county\"].sel(hdi=\"lower\"),     unpooled_hdi_iter[\"a_county\"].sel(hdi=\"higher\"),     color=\"orange\", alpha=0.6 ) ax.set(     ylabel=\"Radon estimate\",     xlabel=\"Ordered County\",     ylim=(-1, 4.5),     xticks=xticks ) ax.set_xticklabels(unpooled_means_iter[f\"a_county_dim_0\"].values[xticks], fontdict=fontdict) ax.tick_params(rotation=30) <p>Here are some visual comparisons between the pooled and unpooled estimates for a subset of counties representing a range of sample sizes.</p> In\u00a0[\u00a0]: Copied! <pre>SAMPLE_COUNTIES = (\n    \"LAC QUI PARLE\",\n    \"AITKIN\",\n    \"KOOCHICHING\",\n    \"DOUGLAS\",\n    \"CLAY\",\n    \"STEARNS\",\n    \"RAMSEY\",\n    \"ST LOUIS\",\n)\n\nunpooled_advi_samples.observed_data = unpooled_advi_samples.observed_data.assign_coords({\n    \"County\": (\"obs_id\", mn_counties[county_idx]),\n    \"Level\": (\"obs_id\", np.array([\"Basement\", \"Floor\"])[floor.values.astype(np.int32)])\n})\n\n\nfig, axes = plt.subplots(2, 4, figsize=(12, 6), sharey=True, sharex=True)\nxspace = np.linspace(0, 1, 100)\nfor ax, c in zip(axes.ravel(), SAMPLE_COUNTIES):\n    sample_county_mask = unpooled_advi_samples.observed_data.County.isin([c])\n\n    # plot obs:\n    unpooled_advi_samples.observed_data.where(\n        sample_county_mask, drop=True\n    ).sortby(\"Level\").plot.scatter(x=\"Level\", y=\"y\", ax=ax, alpha=.4, label=\"Log Radon\")\n\n    # plot both models:\n    ax.plot([0, 1], unpooled_means.a_county.sel(a_county_dim_0=c) + unpooled_means.beta*xvals, \"b\", label=\"Unpooled estimates\")\n    ax.plot([0, 1], pooled_means.a, \"r--\", label=\"Pooled estimates\")\n\n    ax.set_title(c); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n    ax.tick_params(labelsize=10)\n\naxes[0,0].set_ylabel(\"Log radon level\"); axes[1,0].set_ylabel(\"Log radon level\")\naxes[0,0].legend(fontsize=8, frameon=True); axes[1,0].legend(fontsize=8, frameon=True);\n</pre> SAMPLE_COUNTIES = (     \"LAC QUI PARLE\",     \"AITKIN\",     \"KOOCHICHING\",     \"DOUGLAS\",     \"CLAY\",     \"STEARNS\",     \"RAMSEY\",     \"ST LOUIS\", )  unpooled_advi_samples.observed_data = unpooled_advi_samples.observed_data.assign_coords({     \"County\": (\"obs_id\", mn_counties[county_idx]),     \"Level\": (\"obs_id\", np.array([\"Basement\", \"Floor\"])[floor.values.astype(np.int32)]) })   fig, axes = plt.subplots(2, 4, figsize=(12, 6), sharey=True, sharex=True) xspace = np.linspace(0, 1, 100) for ax, c in zip(axes.ravel(), SAMPLE_COUNTIES):     sample_county_mask = unpooled_advi_samples.observed_data.County.isin([c])      # plot obs:     unpooled_advi_samples.observed_data.where(         sample_county_mask, drop=True     ).sortby(\"Level\").plot.scatter(x=\"Level\", y=\"y\", ax=ax, alpha=.4, label=\"Log Radon\")      # plot both models:     ax.plot([0, 1], unpooled_means.a_county.sel(a_county_dim_0=c) + unpooled_means.beta*xvals, \"b\", label=\"Unpooled estimates\")     ax.plot([0, 1], pooled_means.a, \"r--\", label=\"Pooled estimates\")      ax.set_title(c); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")     ax.tick_params(labelsize=10)  axes[0,0].set_ylabel(\"Log radon level\"); axes[1,0].set_ylabel(\"Log radon level\") axes[0,0].legend(fontsize=8, frameon=True); axes[1,0].legend(fontsize=8, frameon=True); <p>Notice the slopes $\\beta$ differ slightly. The county <code>LAC QUI PARLE</code> has the highest average radon level from 85 counties as it is evident from the previous plot. But these estimates are calculated with just two observations. And there is a big shift in its intercept from complete pooling to no pooling. This is a classic issue with no-pooling models: when you estimate clusters independently from each other, can you trust estimates from small-sample-size counties?</p> <p>Neither of these models are satisfactory:</p> <ul> <li>If we are trying to identify high-radon counties, pooling is not useful as it ignores any variation in average radon levels between counties.</li> <li>We do not trust extreme unpooled estimates produced by models using few observations. This leads to maximal overfitting: only the within-county variations are taken into account and the overall population is not estimated.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>@pm.model\ndef partial_pooling():\n    # Priors\n    mu_a = yield pm.Normal('mu_a', loc=0., scale=10.)\n    sigma_a = yield pm.HalfCauchy('sigma_a', scale=1.)\n\n    # Intercepts\n    a_county = yield pm.Normal('a_county', loc=mu_a, scale=sigma_a, batch_stack=counties)\n\n    loc = tf.gather(a_county, county_idx)\n    scale = yield pm.Exponential(\"sigma\", rate=1.)\n\n    y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values)\n</pre> @pm.model def partial_pooling():     # Priors     mu_a = yield pm.Normal('mu_a', loc=0., scale=10.)     sigma_a = yield pm.HalfCauchy('sigma_a', scale=1.)      # Intercepts     a_county = yield pm.Normal('a_county', loc=mu_a, scale=sigma_a, batch_stack=counties)      loc = tf.gather(a_county, county_idx)     scale = yield pm.Exponential(\"sigma\", rate=1.)      y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values) In\u00a0[\u00a0]: Copied! <pre>partial_pooling_advi = pm.fit(partial_pooling(), num_steps=25_000)\nplot_elbo(partial_pooling_advi.losses)\n</pre> partial_pooling_advi = pm.fit(partial_pooling(), num_steps=25_000) plot_elbo(partial_pooling_advi.losses) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>partial_pooling_samples = partial_pooling_advi.approximation.sample(2_000)\nremove_scope(partial_pooling_samples)\npartial_pooling_samples\n</pre> partial_pooling_samples = partial_pooling_advi.approximation.sample(2_000) remove_scope(partial_pooling_samples) partial_pooling_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:         (a_county_dim_0: 85, chain: 1, draw: 2000)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * a_county_dim_0  (a_county_dim_0) int64 0 1 2 3 4 5 6 ... 79 80 81 82 83 84\nData variables:\n    mu_a            (chain, draw) float32 1.3184162 1.3072621 ... 1.3369763\n    a_county        (chain, draw, a_county_dim_0) float32 1.1006255 ... 1.406...\n    __log_sigma_a   (chain, draw) float32 -1.1094719 -1.1404127 ... -1.1693175\n    __log_sigma     (chain, draw) float32 -0.2506265 -0.25947058 ... -0.28390348\n    sigma_a         (chain, draw) float32 0.32973304 0.31968707 ... 0.31057885\n    sigma           (chain, draw) float32 0.77831304 0.77145994 ... 0.7528393\nAttributes:\n    created_at:     2020-09-06T15:13:08.869424\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>a_county_dim_0: 85</li><li>chain: 1</li><li>draw: 2000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999<pre>array([   0,    1,    2, ..., 1997, 1998, 1999])</pre></li><li>a_county_dim_0(a_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li></ul></li><li>Data variables: (6)<ul><li>mu_a(chain, draw)float321.3184162 1.3072621 ... 1.3369763<pre>array([[1.3184162, 1.3072621, 1.3651627, ..., 1.3594881, 1.3647654,\n        1.3369763]], dtype=float32)</pre></li><li>a_county(chain, draw, a_county_dim_0)float321.1006255 0.82927036 ... 1.4069959<pre>array([[[1.1006255 , 0.82927036, 1.0694137 , ..., 1.3657898 ,\n         1.660825  , 1.4485801 ],\n        [0.99680126, 0.7985607 , 0.7317189 , ..., 1.5842866 ,\n         1.6412865 , 1.2769667 ],\n        [1.1700957 , 1.1524378 , 1.4914379 , ..., 1.426831  ,\n         1.7535654 , 0.9966372 ],\n        ...,\n        [0.7133796 , 0.9491382 , 0.80141616, ..., 1.4381852 ,\n         1.3584955 , 1.4065135 ],\n        [1.0746276 , 0.94761926, 1.1560918 , ..., 1.6516389 ,\n         1.5542179 , 1.3733586 ],\n        [1.423568  , 1.0943595 , 1.0630486 , ..., 1.6526098 ,\n         1.5484184 , 1.4069959 ]]], dtype=float32)</pre></li><li>__log_sigma_a(chain, draw)float32-1.1094719 ... -1.1693175<pre>array([[-1.1094719, -1.1404127, -1.050507 , ..., -1.1475506, -1.2099934,\n        -1.1693175]], dtype=float32)</pre></li><li>__log_sigma(chain, draw)float32-0.2506265 ... -0.28390348<pre>array([[-0.2506265 , -0.25947058, -0.2589428 , ..., -0.26138023,\n        -0.2666051 , -0.28390348]], dtype=float32)</pre></li><li>sigma_a(chain, draw)float320.32973304 ... 0.31057885<pre>array([[0.32973304, 0.31968707, 0.3497604 , ..., 0.3174133 , 0.29819927,\n        0.31057885]], dtype=float32)</pre></li><li>sigma(chain, draw)float320.77831304 0.77145994 ... 0.7528393<pre>array([[0.77831304, 0.77145994, 0.77186716, ..., 0.76998806, 0.7659755 ,\n        0.7528393 ]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:13:08.869424arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:13:08.875350\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:13:08.875350arviz_version :0.9.0</li></ul> </ul> </li> </ul> <p>To compare partial-pooling and no-pooling estimates, let\u2019s run the unpooled model without the floor predictor:</p> In\u00a0[\u00a0]: Copied! <pre>@pm.model\ndef unpooled_without_floor():\n    a_county = yield pm.Normal('a_county', loc=0., scale=10., batch_stack=counties)\n\n    loc = tf.gather(a_county, county_idx)\n    scale = yield pm.Exponential(\"sigma\", rate=1.)\n\n    y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values)\n</pre> @pm.model def unpooled_without_floor():     a_county = yield pm.Normal('a_county', loc=0., scale=10., batch_stack=counties)      loc = tf.gather(a_county, county_idx)     scale = yield pm.Exponential(\"sigma\", rate=1.)      y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values) In\u00a0[\u00a0]: Copied! <pre>unpooled_without_floor_advi = pm.fit(unpooled_without_floor(), num_steps=25_000)\nplot_elbo(unpooled_without_floor_advi.losses)\n</pre> unpooled_without_floor_advi = pm.fit(unpooled_without_floor(), num_steps=25_000) plot_elbo(unpooled_without_floor_advi.losses) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>unpooled_without_floor_samples = unpooled_without_floor_advi.approximation.sample(2_000)\nremove_scope(unpooled_without_floor_samples)\nunpooled_without_floor_samples\n</pre> unpooled_without_floor_samples = unpooled_without_floor_advi.approximation.sample(2_000) remove_scope(unpooled_without_floor_samples) unpooled_without_floor_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:         (a_county_dim_0: 85, chain: 1, draw: 2000)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * a_county_dim_0  (a_county_dim_0) int64 0 1 2 3 4 5 6 ... 79 80 81 82 83 84\nData variables:\n    a_county        (chain, draw, a_county_dim_0) float32 1.0178491 ... 1.448...\n    __log_sigma     (chain, draw) float32 -0.13307804 -0.2458005 ... -0.24015264\n    sigma           (chain, draw) float32 0.8753968 0.78207827 ... 0.7865078\nAttributes:\n    created_at:     2020-09-06T15:13:25.786248\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>a_county_dim_0: 85</li><li>chain: 1</li><li>draw: 2000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999<pre>array([   0,    1,    2, ..., 1997, 1998, 1999])</pre></li><li>a_county_dim_0(a_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li></ul></li><li>Data variables: (3)<ul><li>a_county(chain, draw, a_county_dim_0)float321.0178491 1.0013496 ... 1.4488665<pre>array([[[1.0178491 , 1.0013496 , 0.49264288, ..., 1.4281553 ,\n         1.5073564 , 1.8631239 ],\n        [0.26082283, 0.91135824, 0.62953234, ..., 1.5972172 ,\n         1.5199922 , 1.0201625 ],\n        [0.58227336, 0.9683933 , 1.0847672 , ..., 1.347017  ,\n         1.7169664 , 1.1955161 ],\n        ...,\n        [0.7516582 , 0.81360966, 0.87281895, ..., 1.5931329 ,\n         1.724778  , 0.7069099 ],\n        [0.7011503 , 0.7636232 , 1.5743186 , ..., 1.432193  ,\n         1.4250641 , 0.9046785 ],\n        [0.95084935, 0.7369791 , 1.8440148 , ..., 1.5269847 ,\n         1.6424592 , 1.4488665 ]]], dtype=float32)</pre></li><li>__log_sigma(chain, draw)float32-0.13307804 ... -0.24015264<pre>array([[-0.13307804, -0.2458005 , -0.20709029, ..., -0.15590635,\n        -0.25564823, -0.24015264]], dtype=float32)</pre></li><li>sigma(chain, draw)float320.8753968 0.78207827 ... 0.7865078<pre>array([[0.8753968 , 0.78207827, 0.81294626, ..., 0.85563934, 0.7744143 ,\n        0.7865078 ]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:13:25.786248arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:13:25.787835\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:13:25.787835arviz_version :0.9.0</li></ul> </ul> </li> </ul> <p>Now let\u2019s compare both models\u2019 estimates for all 85 counties. We\u2019ll plot the estimates against each county\u2019s sample size, to let you see more clearly what hierarchical models bring to the table:</p> In\u00a0[\u00a0]: Copied! <pre>N_county = data.groupby(\"county\")[\"idnum\"].count().values\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\nfor ax, idata, func_name, level in zip(\n    axes,\n    (unpooled_without_floor_samples, partial_pooling_samples),\n    (unpooled_without_floor().name, partial_pooling().name),\n    (\"no pooling\", \"partial pooling\"),\n):\n    # add variable with x values to xarray dataset\n    idata.posterior = idata.posterior.assign_coords({\"N_county\": (\"a_county_dim_0\", N_county)})\n    # plot means\n    idata.posterior.mean(dim=(\"chain\", \"draw\")).plot.scatter(x=\"N_county\", y=\"a_county\", ax=ax, alpha=0.9);\n    ax.hlines(\n        partial_pooling_samples.posterior.a_county.mean(),\n        0.9,\n        max(N_county) + 1,\n        alpha=0.4,\n        ls=\"--\",\n        label=\"Est. population mean\",\n    )\n\n    # plot hdi\n    hdi = az.hdi(idata).a_county\n    ax.vlines(N_county, hdi.sel(hdi=\"lower\"), hdi.sel(hdi=\"higher\"), color=\"orange\", alpha=0.5)\n\n    ax.set(\n        title=f\"{level.title()} Estimates\",\n        xlabel=\"Nbr obs in county (log scale)\",\n        xscale=\"log\",\n        ylabel=\"Log radon Level\",\n    )\n    ax.legend(fontsize=10)\n</pre> N_county = data.groupby(\"county\")[\"idnum\"].count().values  fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True) for ax, idata, func_name, level in zip(     axes,     (unpooled_without_floor_samples, partial_pooling_samples),     (unpooled_without_floor().name, partial_pooling().name),     (\"no pooling\", \"partial pooling\"), ):     # add variable with x values to xarray dataset     idata.posterior = idata.posterior.assign_coords({\"N_county\": (\"a_county_dim_0\", N_county)})     # plot means     idata.posterior.mean(dim=(\"chain\", \"draw\")).plot.scatter(x=\"N_county\", y=\"a_county\", ax=ax, alpha=0.9);     ax.hlines(         partial_pooling_samples.posterior.a_county.mean(),         0.9,         max(N_county) + 1,         alpha=0.4,         ls=\"--\",         label=\"Est. population mean\",     )      # plot hdi     hdi = az.hdi(idata).a_county     ax.vlines(N_county, hdi.sel(hdi=\"lower\"), hdi.sel(hdi=\"higher\"), color=\"orange\", alpha=0.5)      ax.set(         title=f\"{level.title()} Estimates\",         xlabel=\"Nbr obs in county (log scale)\",         xscale=\"log\",         ylabel=\"Log radon Level\",     )     ax.legend(fontsize=10) <p>Notice the difference between the unpooled and partially-pooled estimates, particularly at smaller sample sizes: As expected, the former is both more extreme and more imprecise. Indeed, in the partially-pooled model, estimates in small-sample-size counties are informed by the population parameters \u2013 hence more precise estimates. Moreover, the smaller the sample size, the more regression towards the overall mean (the dashed gray line) \u2013 hence less extreme estimates. In other words, the model is skeptical of extreme deviations from the population mean in counties where data is sparse.</p> <p>Now let\u2019s try to integrate the floor predictor -</p> In\u00a0[\u00a0]: Copied! <pre>@pm.model\ndef varying_intercept():\n    mu_a = yield pm.Normal('mu_a', loc=0., scale=1e5)\n    sigma_a = yield pm.HalfCauchy('sigma_a', scale=5.)\n\n    a_county = yield pm.Normal('a_county', loc=mu_a, scale=sigma_a, batch_stack=counties)\n    b = yield pm.Normal('b', loc=0., scale=10.)\n\n    loc = tf.gather(a_county, county_idx) + b*floor\n    scale = yield pm.Exponential(\"sigma\", rate=1.)\n\n    y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values)\n</pre> @pm.model def varying_intercept():     mu_a = yield pm.Normal('mu_a', loc=0., scale=1e5)     sigma_a = yield pm.HalfCauchy('sigma_a', scale=5.)      a_county = yield pm.Normal('a_county', loc=mu_a, scale=sigma_a, batch_stack=counties)     b = yield pm.Normal('b', loc=0., scale=10.)      loc = tf.gather(a_county, county_idx) + b*floor     scale = yield pm.Exponential(\"sigma\", rate=1.)      y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values) In\u00a0[\u00a0]: Copied! <pre>varying_intercept_advi = pm.fit(varying_intercept(), num_steps=40_000)\nplot_elbo(varying_intercept_advi.losses)\n</pre> varying_intercept_advi = pm.fit(varying_intercept(), num_steps=40_000) plot_elbo(varying_intercept_advi.losses) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>varying_intercept_samples = varying_intercept_advi.approximation.sample(2_000)\nremove_scope(varying_intercept_samples)\nvarying_intercept_samples\n</pre> varying_intercept_samples = varying_intercept_advi.approximation.sample(2_000) remove_scope(varying_intercept_samples) varying_intercept_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:         (a_county_dim_0: 85, chain: 1, draw: 2000)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * a_county_dim_0  (a_county_dim_0) int64 0 1 2 3 4 5 6 ... 79 80 81 82 83 84\nData variables:\n    mu_a            (chain, draw) float32 1.4493645 1.5067779 ... 1.5112383\n    a_county        (chain, draw, a_county_dim_0) float32 1.782057 ... 1.7976772\n    b               (chain, draw) float32 -0.6368493 -0.635342 ... -0.6911656\n    __log_sigma_a   (chain, draw) float32 -1.1040249 -1.0546951 ... -1.1985127\n    __log_sigma     (chain, draw) float32 -0.32528934 -0.2625135 ... -0.30933022\n    sigma_a         (chain, draw) float32 0.331534 0.3482986 ... 0.30164254\n    sigma           (chain, draw) float32 0.7223183 0.769116 ... 0.73393834\nAttributes:\n    created_at:     2020-09-06T15:14:00.002746\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>a_county_dim_0: 85</li><li>chain: 1</li><li>draw: 2000</li></ul></li><li>Coordinates: (3)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999<pre>array([   0,    1,    2, ..., 1997, 1998, 1999])</pre></li><li>a_county_dim_0(a_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li></ul></li><li>Data variables: (7)<ul><li>mu_a(chain, draw)float321.4493645 1.5067779 ... 1.5112383<pre>array([[1.4493645, 1.5067779, 1.4653704, ..., 1.4295348, 1.5160272,\n        1.5112383]], dtype=float32)</pre></li><li>a_county(chain, draw, a_county_dim_0)float321.782057 1.1568834 ... 1.7976772<pre>array([[[1.782057  , 1.1568834 , 1.1844532 , ..., 1.7383634 ,\n         1.8035787 , 1.6306922 ],\n        [1.2556518 , 0.90442353, 1.6552949 , ..., 1.5384247 ,\n         1.7612361 , 1.743976  ],\n        [0.8760731 , 0.8613335 , 2.2567198 , ..., 1.5762341 ,\n         1.5510461 , 1.8853244 ],\n        ...,\n        [1.1741817 , 0.95642495, 1.7446842 , ..., 1.5429543 ,\n         1.4440159 , 1.1208017 ],\n        [1.3757881 , 0.87958705, 1.3585677 , ..., 1.6110898 ,\n         1.7152646 , 0.9002575 ],\n        [1.2784272 , 1.0408564 , 1.6897631 , ..., 1.472668  ,\n         1.6169112 , 1.7976772 ]]], dtype=float32)</pre></li><li>b(chain, draw)float32-0.6368493 -0.635342 ... -0.6911656<pre>array([[-0.6368493 , -0.635342  , -0.68866193, ..., -0.5476906 ,\n        -0.59620386, -0.6911656 ]], dtype=float32)</pre></li><li>__log_sigma_a(chain, draw)float32-1.1040249 ... -1.1985127<pre>array([[-1.1040249, -1.0546951, -1.2069846, ..., -1.1648154, -1.0723704,\n        -1.1985127]], dtype=float32)</pre></li><li>__log_sigma(chain, draw)float32-0.32528934 ... -0.30933022<pre>array([[-0.32528934, -0.2625135 , -0.34570464, ..., -0.3002294 ,\n        -0.27668455, -0.30933022]], dtype=float32)</pre></li><li>sigma_a(chain, draw)float320.331534 0.3482986 ... 0.30164254<pre>array([[0.331534  , 0.3482986 , 0.2990978 , ..., 0.31198025, 0.3421964 ,\n        0.30164254]], dtype=float32)</pre></li><li>sigma(chain, draw)float320.7223183 0.769116 ... 0.73393834<pre>array([[0.7223183 , 0.769116  , 0.7077215 , ..., 0.74064827, 0.7582936 ,\n        0.73393834]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:14:00.002746arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:14:00.010940\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:14:00.010940arviz_version :0.9.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>varying_intercept_samples.assign_coords(**{\"a_county_dim_0\": mn_counties}, inplace=True)\naz.plot_forest(varying_intercept_samples, var_names=[\"mu_a\", \"a_county\"], combined=True, textsize=9\n);\n</pre> varying_intercept_samples.assign_coords(**{\"a_county_dim_0\": mn_counties}, inplace=True) az.plot_forest(varying_intercept_samples, var_names=[\"mu_a\", \"a_county\"], combined=True, textsize=9 ); In\u00a0[\u00a0]: Copied! <pre>_, ax = plt.subplots(2, 2, figsize=(12, 8))\naz.plot_posterior(varying_intercept_samples, var_names=[\"mu_a\", \"sigma_a\", \"b\", \"sigma\"], ax=ax);\n</pre> _, ax = plt.subplots(2, 2, figsize=(12, 8)) az.plot_posterior(varying_intercept_samples, var_names=[\"mu_a\", \"sigma_a\", \"b\", \"sigma\"], ax=ax); <p>As we suspected, the estimate for the floor coefficient is reliably negative and centered around -0.67. This can be interpreted as houses without basements having about half ($\\exp(-0.66) = 0.52$) the radon levels of those with basements, after accounting for county. With this, the estimated average regression line for all the counties can be written as $y = 1.5\u22120.67x$.</p> In\u00a0[\u00a0]: Copied! <pre>post = varying_intercept_samples.posterior  # alias for readability\ntheta = (post.a_county + post.b * xvals).mean(dim=(\"chain\", \"draw\")).to_dataset(name=\"Mean log radon\")\n\n_, ax = plt.subplots()\ntheta.plot.scatter(x=\"Level\", y=\"Mean log radon\", alpha=0.2, color=\"k\", ax=ax)  # scatter\nax.plot(xvals, theta[\"Mean log radon\"].T,\"k-\", alpha=0.2);  # add lines too\nax.set_title(\"Log Radon Estimates(Varying Intercepts)\");\n</pre> post = varying_intercept_samples.posterior  # alias for readability theta = (post.a_county + post.b * xvals).mean(dim=(\"chain\", \"draw\")).to_dataset(name=\"Mean log radon\")  _, ax = plt.subplots() theta.plot.scatter(x=\"Level\", y=\"Mean log radon\", alpha=0.2, color=\"k\", ax=ax)  # scatter ax.plot(xvals, theta[\"Mean log radon\"].T,\"k-\", alpha=0.2);  # add lines too ax.set_title(\"Log Radon Estimates(Varying Intercepts)\"); <p>The graph above shows, for each county, the expected log radon level and the average effect of having no basement.</p> <p>Let's compare partial pooling estimates with pooled and unpooled models.</p> In\u00a0[\u00a0]: Copied! <pre>varying_intercept_samples.observed_data = varying_intercept_samples.observed_data.assign_coords({\n    \"County\": (\"obs_id\", mn_counties[county_idx]),\n    \"Level\": (\"obs_id\", np.array([\"Basement\", \"Floor\"])[floor.values.astype(np.int32)])\n})\n\nfig, axes = plt.subplots(2, 4, figsize=(12, 6), sharey=True, sharex=True)\nfor ax, c in zip(axes.ravel(), SAMPLE_COUNTIES):\n    sample_county_mask = varying_intercept_samples.observed_data.County.isin([c])\n\n    # plot obs:\n    unpooled_advi_samples.observed_data.where(\n        sample_county_mask, drop=True\n    ).sortby(\"Level\").plot.scatter(x=\"Level\", y=\"y\", ax=ax, alpha=.4)\n\n    # plot both models:\n    ax.plot([0, 1], pooled_means.a, \"r--\", label=\"Complete pooling\")\n    ax.plot([0, 1], unpooled_means.a_county.sel(a_county_dim_0=c) + unpooled_means.beta*xvals, \"k:\", label=\"No pooling\")\n    ax.plot([0, 1], theta[\"Mean log radon\"].sel(a_county_dim_0=c), \"b\", label=\"Partial pooling\")\n\n    ax.set_title(c); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n    \n\naxes[0,0].set_ylabel(\"Log radon level\"); axes[1, 0].set_ylabel(\"Log radon level\")\naxes[0,0].legend(fontsize=8, frameon=True), axes[1, 0].legend(fontsize=8, frameon=True)\nfig.tight_layout();\n</pre> varying_intercept_samples.observed_data = varying_intercept_samples.observed_data.assign_coords({     \"County\": (\"obs_id\", mn_counties[county_idx]),     \"Level\": (\"obs_id\", np.array([\"Basement\", \"Floor\"])[floor.values.astype(np.int32)]) })  fig, axes = plt.subplots(2, 4, figsize=(12, 6), sharey=True, sharex=True) for ax, c in zip(axes.ravel(), SAMPLE_COUNTIES):     sample_county_mask = varying_intercept_samples.observed_data.County.isin([c])      # plot obs:     unpooled_advi_samples.observed_data.where(         sample_county_mask, drop=True     ).sortby(\"Level\").plot.scatter(x=\"Level\", y=\"y\", ax=ax, alpha=.4)      # plot both models:     ax.plot([0, 1], pooled_means.a, \"r--\", label=\"Complete pooling\")     ax.plot([0, 1], unpooled_means.a_county.sel(a_county_dim_0=c) + unpooled_means.beta*xvals, \"k:\", label=\"No pooling\")     ax.plot([0, 1], theta[\"Mean log radon\"].sel(a_county_dim_0=c), \"b\", label=\"Partial pooling\")      ax.set_title(c); ax.set_xlabel(\"\"); ax.set_ylabel(\"\")       axes[0,0].set_ylabel(\"Log radon level\"); axes[1, 0].set_ylabel(\"Log radon level\") axes[0,0].legend(fontsize=8, frameon=True), axes[1, 0].legend(fontsize=8, frameon=True) fig.tight_layout(); <pre>&lt;ipython-input-37-0baef2f019b1&gt;:25: UserWarning: This figure was using constrained_layout==True, but that is incompatible with subplots_adjust and or tight_layout: setting constrained_layout==False. \n  fig.tight_layout();\n</pre> <p>Here we clearly see the notion that partial-pooling is a compromise between no pooling and complete pooling, as its mean estimates are usually between the other models\u2019 estimates. And interestingly, the bigger (smaller) the sample size in a given county, the closer the partial-pooling estimates are to the no-pooling (complete-pooling) estimates.</p> In\u00a0[\u00a0]: Copied! <pre>@pm.model\ndef varying_intercept_slope():\n    # Hyperpriors\n    mu_a = yield pm.Normal('mu_a', loc=0., scale=1.)\n    sigma_a = yield pm.HalfCauchy('sigma_a', scale=1.)\n    mu_b = yield pm.Normal('mu_b', loc=0., scale=1.)\n    sigma_b = yield pm.HalfCauchy('sigma_b', scale=1.)\n\n    # Intercept for each county, distributed around group mean mu_a\n    a_county = yield pm.Normal('a_county', loc=mu_a, scale=sigma_a, batch_stack=counties)\n    # Slope for each county, distributed around group mean mu_b\n    b_county = yield pm.Normal('b_county', loc=mu_b, scale=sigma_b, batch_stack=counties)\n\n    loc = tf.gather(a_county, county_idx) + tf.gather(b_county, county_idx) * data.floor.values\n    scale = yield pm.Exponential(\"sigma\", rate=1.)\n\n    y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values)\n</pre> @pm.model def varying_intercept_slope():     # Hyperpriors     mu_a = yield pm.Normal('mu_a', loc=0., scale=1.)     sigma_a = yield pm.HalfCauchy('sigma_a', scale=1.)     mu_b = yield pm.Normal('mu_b', loc=0., scale=1.)     sigma_b = yield pm.HalfCauchy('sigma_b', scale=1.)      # Intercept for each county, distributed around group mean mu_a     a_county = yield pm.Normal('a_county', loc=mu_a, scale=sigma_a, batch_stack=counties)     # Slope for each county, distributed around group mean mu_b     b_county = yield pm.Normal('b_county', loc=mu_b, scale=sigma_b, batch_stack=counties)      loc = tf.gather(a_county, county_idx) + tf.gather(b_county, county_idx) * data.floor.values     scale = yield pm.Exponential(\"sigma\", rate=1.)      y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values) In\u00a0[\u00a0]: Copied! <pre>varying_intercept_slope_advi = pm.fit(varying_intercept_slope(), num_steps=25_000)\nvarying_intercept_slope_samples = varying_intercept_slope_advi.approximation.sample(2_000)\nremove_scope(varying_intercept_slope_samples)\nvarying_intercept_slope_samples\n</pre> varying_intercept_slope_advi = pm.fit(varying_intercept_slope(), num_steps=25_000) varying_intercept_slope_samples = varying_intercept_slope_advi.approximation.sample(2_000) remove_scope(varying_intercept_slope_samples) varying_intercept_slope_samples <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:         (a_county_dim_0: 85, b_county_dim_0: 85, chain: 1, draw: 2000)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * a_county_dim_0  (a_county_dim_0) int64 0 1 2 3 4 5 6 ... 79 80 81 82 83 84\n  * b_county_dim_0  (b_county_dim_0) int64 0 1 2 3 4 5 6 ... 79 80 81 82 83 84\nData variables:\n    mu_a            (chain, draw) float32 1.4930406 1.4486337 ... 1.5145546\n    mu_b            (chain, draw) float32 -0.7048421 -0.64330596 ... -0.59183025\n    a_county        (chain, draw, a_county_dim_0) float32 1.1657196 ... 0.597...\n    b_county        (chain, draw, b_county_dim_0) float32 -0.370982 ... -1.03...\n    __log_sigma_a   (chain, draw) float32 -1.1933181 -1.1757988 ... -1.1121558\n    __log_sigma_b   (chain, draw) float32 -0.8809401 -0.96785593 ... -0.6744319\n    __log_sigma     (chain, draw) float32 -0.35025468 ... -0.29480654\n    sigma_a         (chain, draw) float32 0.30321348 0.3085724 ... 0.32884926\n    sigma_b         (chain, draw) float32 0.41439316 0.3798967 ... 0.5094457\n    sigma           (chain, draw) float32 0.70450866 0.70668447 ... 0.74467564\nAttributes:\n    created_at:     2020-09-06T15:14:36.909984\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>a_county_dim_0: 85</li><li>b_county_dim_0: 85</li><li>chain: 1</li><li>draw: 2000</li></ul></li><li>Coordinates: (4)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999<pre>array([   0,    1,    2, ..., 1997, 1998, 1999])</pre></li><li>a_county_dim_0(a_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li><li>b_county_dim_0(b_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li></ul></li><li>Data variables: (10)<ul><li>mu_a(chain, draw)float321.4930406 1.4486337 ... 1.5145546<pre>array([[1.4930406, 1.4486337, 1.4955258, ..., 1.4699484, 1.5402639,\n        1.5145546]], dtype=float32)</pre></li><li>mu_b(chain, draw)float32-0.7048421 ... -0.59183025<pre>array([[-0.7048421 , -0.64330596, -0.6644298 , ..., -0.6669646 ,\n        -0.60908604, -0.59183025]], dtype=float32)</pre></li><li>a_county(chain, draw, a_county_dim_0)float321.1657196 1.054467 ... 0.59795755<pre>array([[[1.1657196 , 1.054467  , 0.92647433, ..., 1.766197  ,\n         1.6828083 , 1.6666431 ],\n        [1.0680116 , 1.0140331 , 1.1937376 , ..., 1.3710928 ,\n         1.591624  , 1.6319671 ],\n        [1.1849082 , 0.97396046, 1.5205295 , ..., 1.8124071 ,\n         1.4027165 , 1.4950168 ],\n        ...,\n        [1.2612067 , 0.96331567, 1.1616149 , ..., 1.4058344 ,\n         1.1527064 , 1.3057501 ],\n        [1.183216  , 0.9911122 , 1.6703023 , ..., 1.5332017 ,\n         1.658561  , 1.4238042 ],\n        [1.2632247 , 0.99801606, 1.8306286 , ..., 1.7863529 ,\n         1.778602  , 0.59795755]]], dtype=float32)</pre></li><li>b_county(chain, draw, b_county_dim_0)float32-0.370982 -0.7640928 ... -1.035405<pre>array([[[-0.370982  , -0.7640928 , -0.93647075, ..., -1.3101674 ,\n         -0.8603058 , -0.76141113],\n        [-0.46477285, -1.0195789 , -0.6515858 , ..., -1.3889568 ,\n         -0.8997741 , -0.4294933 ],\n        [-0.4628898 , -1.5606394 , -0.4034984 , ..., -1.5112813 ,\n         -0.7789763 , -1.510519  ],\n        ...,\n        [-0.3307628 , -0.8429571 , -1.101429  , ..., -1.71626   ,\n         -0.5948556 ,  0.08953285],\n        [-0.30029684, -1.3296744 , -0.5047833 , ..., -1.3792053 ,\n         -0.00357264, -0.30857176],\n        [-0.7416409 , -0.6979863 , -0.67211825, ..., -1.7206702 ,\n         -0.67385936, -1.035405  ]]], dtype=float32)</pre></li><li>__log_sigma_a(chain, draw)float32-1.1933181 ... -1.1121558<pre>array([[-1.1933181, -1.1757988, -1.1129156, ..., -1.0994793, -0.9975351,\n        -1.1121558]], dtype=float32)</pre></li><li>__log_sigma_b(chain, draw)float32-0.8809401 ... -0.6744319<pre>array([[-0.8809401 , -0.96785593, -0.8011294 , ..., -1.0679553 ,\n        -0.85843027, -0.6744319 ]], dtype=float32)</pre></li><li>__log_sigma(chain, draw)float32-0.35025468 ... -0.29480654<pre>array([[-0.35025468, -0.34717098, -0.3214259 , ..., -0.344014  ,\n        -0.30487636, -0.29480654]], dtype=float32)</pre></li><li>sigma_a(chain, draw)float320.30321348 0.3085724 ... 0.32884926<pre>array([[0.30321348, 0.3085724 , 0.32859948, ..., 0.33304447, 0.36878735,\n        0.32884926]], dtype=float32)</pre></li><li>sigma_b(chain, draw)float320.41439316 0.3798967 ... 0.5094457<pre>array([[0.41439316, 0.3798967 , 0.44882178, ..., 0.3437106 , 0.42382687,\n        0.5094457 ]], dtype=float32)</pre></li><li>sigma(chain, draw)float320.70450866 ... 0.74467564<pre>array([[0.70450866, 0.70668447, 0.72511435, ..., 0.70891905, 0.7372145 ,\n        0.74467564]], dtype=float32)</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:14:36.909984arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:14:36.914459\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:14:36.914459arviz_version :0.9.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>az.plot_trace(varying_intercept_slope_samples, compact=True)\n</pre> az.plot_trace(varying_intercept_slope_samples, compact=True) Out[\u00a0]: <pre>&lt;AxesSubplot:title={'center':'sigma'}&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>post = varying_intercept_slope_samples.posterior  # alias for readability\navg_a_county = post.a_county.mean(dim=(\"chain\", \"draw\"))\navg_b_county = post.b_county.mean(dim=(\"chain\", \"draw\"))\ntheta = (avg_a_county.rename(a_county_dim_0=\"County\") + avg_b_county.rename(b_county_dim_0=\"County\")*xvals).to_dataset(name=\"Mean log radon\")\n\n_, ax = plt.subplots()\ntheta.plot.scatter(x=\"Level\", y=\"Mean log radon\", alpha=0.2, color=\"k\", ax=ax) #scatter\nax.plot(xvals, theta[\"Mean log radon\"].T,\"k-\", alpha=0.2); # add lines too\nax.set_title(\"MEAN LOG RADON BY COUNTY\");\n</pre> post = varying_intercept_slope_samples.posterior  # alias for readability avg_a_county = post.a_county.mean(dim=(\"chain\", \"draw\")) avg_b_county = post.b_county.mean(dim=(\"chain\", \"draw\")) theta = (avg_a_county.rename(a_county_dim_0=\"County\") + avg_b_county.rename(b_county_dim_0=\"County\")*xvals).to_dataset(name=\"Mean log radon\")  _, ax = plt.subplots() theta.plot.scatter(x=\"Level\", y=\"Mean log radon\", alpha=0.2, color=\"k\", ax=ax) #scatter ax.plot(xvals, theta[\"Mean log radon\"].T,\"k-\", alpha=0.2); # add lines too ax.set_title(\"MEAN LOG RADON BY COUNTY\"); <p>We can see that now both the intercept and the slope vary by county. Now, the plan of action is to model the covariation between intercepts and slopes using <code>MvNormal</code> distribution.</p> In\u00a0[\u00a0]: Copied! <pre>class CorrelationCholesky(pm.distributions.transforms.BackwardTransform):\n    name = \"cholesky\"\n\n    def __init__(self):\n        transform = tfb.CorrelationCholesky()\n        super().__init__(transform)\n</pre> class CorrelationCholesky(pm.distributions.transforms.BackwardTransform):     name = \"cholesky\"      def __init__(self):         transform = tfb.CorrelationCholesky()         super().__init__(transform) In\u00a0[\u00a0]: Copied! <pre>@pm.model\ndef covariation_intercept_slope():\n    sigma_slope = yield pm.Exponential(\"sigma_slope\", rate=tf.cast(1., tf.float64), batch_stack=2)\n    cov = yield pm.LKJCholesky('cov', dimension=2, concentration=tf.cast(2, tf.float64), transform=CorrelationCholesky())\n\n    # Hyperpriors\n    mu_a = yield pm.Normal('mu_a', loc=tf.cast(0., tf.float64), scale=tf.cast(5., tf.float64))\n    mu_b = yield pm.Normal('mu_b', loc=tf.cast(0., tf.float64), scale=tf.cast(5., tf.float64))\n    ab_county = yield pm.MvNormalCholesky(\n        'ab_county', tf.stack([mu_a, mu_b]), tf.linalg.LinearOperatorDiag(sigma_slope).matmul(cov), batch_stack=counties\n        )\n\n    loc = tf.gather(ab_county[:, 0], county_idx) + tf.gather(ab_county[:, 1], county_idx)*floor\n    scale = yield pm.Exponential(\"sigma\", rate=tf.cast(1., tf.float64))\n    y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values)\n</pre> @pm.model def covariation_intercept_slope():     sigma_slope = yield pm.Exponential(\"sigma_slope\", rate=tf.cast(1., tf.float64), batch_stack=2)     cov = yield pm.LKJCholesky('cov', dimension=2, concentration=tf.cast(2, tf.float64), transform=CorrelationCholesky())      # Hyperpriors     mu_a = yield pm.Normal('mu_a', loc=tf.cast(0., tf.float64), scale=tf.cast(5., tf.float64))     mu_b = yield pm.Normal('mu_b', loc=tf.cast(0., tf.float64), scale=tf.cast(5., tf.float64))     ab_county = yield pm.MvNormalCholesky(         'ab_county', tf.stack([mu_a, mu_b]), tf.linalg.LinearOperatorDiag(sigma_slope).matmul(cov), batch_stack=counties         )      loc = tf.gather(ab_county[:, 0], county_idx) + tf.gather(ab_county[:, 1], county_idx)*floor     scale = yield pm.Exponential(\"sigma\", rate=tf.cast(1., tf.float64))     y = yield pm.Normal('y', loc=loc, scale=scale, observed=data.log_radon.values)  <p>Let's also use higher <code>sample_size</code> to achieve better convergence.</p> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_slope_advi = pm.fit(covariation_intercept_slope(), num_steps=25_000, sample_size=10)\n</pre> covariation_intercept_slope_advi = pm.fit(covariation_intercept_slope(), num_steps=25_000, sample_size=10) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>plot_elbo(covariation_intercept_slope_advi.losses)\n</pre> plot_elbo(covariation_intercept_slope_advi.losses) <p>Since, the variable <code>cov</code> is a lower triangular matrix, we need to multiply it with its transpose to get back the correlation matrix.</p> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_slope_samples = covariation_intercept_slope_advi.approximation.sample(3_000)\nremove_scope(covariation_intercept_slope_samples)\ncovariation_intercept_slope_samples.posterior[\"cov\"].values = np.matmul(\n    covariation_intercept_slope_samples.posterior[\"cov\"].values,\n    np.transpose(covariation_intercept_slope_samples.posterior[\"cov\"].values, (0, 1, 3, 2))\n)\ncovariation_intercept_slope_samples\n</pre> covariation_intercept_slope_samples = covariation_intercept_slope_advi.approximation.sample(3_000) remove_scope(covariation_intercept_slope_samples) covariation_intercept_slope_samples.posterior[\"cov\"].values = np.matmul(     covariation_intercept_slope_samples.posterior[\"cov\"].values,     np.transpose(covariation_intercept_slope_samples.posterior[\"cov\"].values, (0, 1, 3, 2)) ) covariation_intercept_slope_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                  (__cholesky_cov_dim_0: 1, __log_sigma_slope_dim_0: 2, ab_county_dim_0: 85, ab_county_dim_1: 2, chain: 1, cov_dim_0: 2, cov_dim_1: 2, draw: 3000, sigma_slope_dim_0: 2)\nCoordinates:\n  * chain                    (chain) int64 0\n  * draw                     (draw) int64 0 1 2 3 4 ... 2995 2996 2997 2998 2999\n  * ab_county_dim_0          (ab_county_dim_0) int64 0 1 2 3 4 ... 81 82 83 84\n  * ab_county_dim_1          (ab_county_dim_1) int64 0 1\n  * __log_sigma_slope_dim_0  (__log_sigma_slope_dim_0) int64 0 1\n  * __cholesky_cov_dim_0     (__cholesky_cov_dim_0) int64 0\n  * sigma_slope_dim_0        (sigma_slope_dim_0) int64 0 1\n  * cov_dim_0                (cov_dim_0) int64 0 1\n  * cov_dim_1                (cov_dim_1) int64 0 1\nData variables:\n    mu_a                     (chain, draw) float64 1.503 1.477 ... 1.514 1.475\n    mu_b                     (chain, draw) float64 -0.6059 -0.6531 ... -0.6212\n    ab_county                (chain, draw, ab_county_dim_0, ab_county_dim_1) float64 ...\n    __log_sigma_slope        (chain, draw, __log_sigma_slope_dim_0) float64 -...\n    __cholesky_cov           (chain, draw, __cholesky_cov_dim_0) float64 0.12...\n    __log_sigma              (chain, draw) float64 -0.3541 -0.2946 ... -0.36\n    sigma_slope              (chain, draw, sigma_slope_dim_0) float64 0.3175 ...\n    cov                      (chain, draw, cov_dim_0, cov_dim_1) float64 1.0 ...\n    sigma                    (chain, draw) float64 0.7018 0.7449 ... 0.6977\nAttributes:\n    created_at:     2020-09-06T15:16:05.404673\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__cholesky_cov_dim_0: 1</li><li>__log_sigma_slope_dim_0: 2</li><li>ab_county_dim_0: 85</li><li>ab_county_dim_1: 2</li><li>chain: 1</li><li>cov_dim_0: 2</li><li>cov_dim_1: 2</li><li>draw: 3000</li><li>sigma_slope_dim_0: 2</li></ul></li><li>Coordinates: (9)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999<pre>array([   0,    1,    2, ..., 2997, 2998, 2999])</pre></li><li>ab_county_dim_0(ab_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li><li>ab_county_dim_1(ab_county_dim_1)int640 1<pre>array([0, 1])</pre></li><li>__log_sigma_slope_dim_0(__log_sigma_slope_dim_0)int640 1<pre>array([0, 1])</pre></li><li>__cholesky_cov_dim_0(__cholesky_cov_dim_0)int640<pre>array([0])</pre></li><li>sigma_slope_dim_0(sigma_slope_dim_0)int640 1<pre>array([0, 1])</pre></li><li>cov_dim_0(cov_dim_0)int640 1<pre>array([0, 1])</pre></li><li>cov_dim_1(cov_dim_1)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (9)<ul><li>mu_a(chain, draw)float641.503 1.477 1.507 ... 1.514 1.475<pre>array([[1.50297226, 1.47662966, 1.50682203, ..., 1.4779058 , 1.51425872,\n        1.47520876]])</pre></li><li>mu_b(chain, draw)float64-0.6059 -0.6531 ... -0.6745 -0.6212<pre>array([[-0.60593625, -0.65312248, -0.65703405, ..., -0.68033895,\n        -0.67447093, -0.62119897]])</pre></li><li>ab_county(chain, draw, ab_county_dim_0, ab_county_dim_1)float640.9875 -0.8597 ... 0.9846 -0.7332<pre>array([[[[ 0.98753613, -0.85973684],\n         [ 1.05021129, -0.7599651 ],\n         [ 1.56348613, -0.36307538],\n         ...,\n         [ 1.47539419, -0.93410477],\n         [ 1.65063888, -0.89027457],\n         [ 1.29922529, -0.80955917]],\n\n        [[ 1.16472251, -1.00372091],\n         [ 0.84512632, -0.59523111],\n         [ 1.72337099, -0.73815348],\n         ...,\n         [ 1.53062087, -0.79304397],\n         [ 1.61317101, -1.00879707],\n         [ 1.5719441 , -1.12102989]],\n\n        [[ 1.05842259, -0.62397016],\n         [ 1.02017518, -0.55132196],\n         [ 1.20530825, -0.3074609 ],\n         ...,\n...\n         ...,\n         [ 1.83136056, -1.23226503],\n         [ 1.66181191, -0.79601287],\n         [ 1.61483588, -0.23794943]],\n\n        [[ 0.9120984 , -0.33232955],\n         [ 1.06463707, -0.88007317],\n         [ 1.2713347 , -0.69039174],\n         ...,\n         [ 1.81054568, -0.91925895],\n         [ 1.67351613, -0.15251035],\n         [ 0.74764682, -0.82722249]],\n\n        [[ 1.46147561, -0.47514546],\n         [ 1.15462958, -0.8816729 ],\n         [ 1.83451009, -0.64952792],\n         ...,\n         [ 1.67841525, -1.21466439],\n         [ 1.37997389, -0.83329095],\n         [ 0.98462632, -0.73319583]]]])</pre></li><li>__log_sigma_slope(chain, draw, __log_sigma_slope_dim_0)float64-1.147 -1.101 ... -1.223 -1.285<pre>array([[[-1.14733712, -1.1008361 ],\n        [-1.11687183, -1.21949091],\n        [-1.03124146, -1.31066995],\n        ...,\n        [-1.12076748, -1.25842616],\n        [-1.03309974, -1.10501786],\n        [-1.2232819 , -1.28534661]]])</pre></li><li>__cholesky_cov(chain, draw, __cholesky_cov_dim_0)float640.129 0.1551 ... -0.0003877 0.1431<pre>array([[[ 0.12902331],\n        [ 0.15508206],\n        [ 0.15371682],\n        ...,\n        [ 0.07718439],\n        [-0.00038772],\n        [ 0.14305497]]])</pre></li><li>__log_sigma(chain, draw)float64-0.3541 -0.2946 ... -0.3602 -0.36<pre>array([[-0.35409417, -0.29457169, -0.32894763, ..., -0.36121729,\n        -0.36023511, -0.36002339]])</pre></li><li>sigma_slope(chain, draw, sigma_slope_dim_0)float640.3175 0.3326 ... 0.2943 0.2766<pre>array([[[0.31748106, 0.33259289],\n        [0.32730205, 0.2953805 ],\n        [0.35656403, 0.26963935],\n        ...,\n        [0.32602948, 0.2841008 ],\n        [0.35590205, 0.33120497],\n        [0.29426284, 0.27655471]]])</pre></li><li>cov(chain, draw, cov_dim_0, cov_dim_1)float641.0 0.128 0.128 ... 0.1416 1.0<pre>array([[[[ 1.00000000e+00,  1.27962607e-01],\n         [ 1.27962607e-01,  1.00000000e+00]],\n\n        [[ 1.00000000e+00,  1.53250139e-01],\n         [ 1.53250139e-01,  1.00000000e+00]],\n\n        [[ 1.00000000e+00,  1.51932302e-01],\n         [ 1.51932302e-01,  1.00000000e+00]],\n\n        ...,\n\n        [[ 1.00000000e+00,  7.69555066e-02],\n         [ 7.69555066e-02,  1.00000000e+00]],\n\n        [[ 1.00000000e+00, -3.87716149e-04],\n         [-3.87716149e-04,  1.00000000e+00]],\n\n        [[ 1.00000000e+00,  1.41613274e-01],\n         [ 1.41613274e-01,  1.00000000e+00]]]])</pre></li><li>sigma(chain, draw)float640.7018 0.7449 ... 0.6975 0.6977<pre>array([[0.70180887, 0.74485055, 0.71968071, ..., 0.69682757, 0.69751231,\n        0.69766001]])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:16:05.404673arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:16:05.413183\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:16:05.413183arviz_version :0.9.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>az.plot_forest(\n    [varying_intercept_slope_samples, covariation_intercept_slope_samples],\n    model_names=[\"No covariation\", \"With covariation\"],\n    var_names=[\"mu_a\", \"mu_b\", \"cov\"],\n    combined=True,\n    figsize=(7.5, 5),\n);\n</pre> az.plot_forest(     [varying_intercept_slope_samples, covariation_intercept_slope_samples],     model_names=[\"No covariation\", \"With covariation\"],     var_names=[\"mu_a\", \"mu_b\", \"cov\"],     combined=True,     figsize=(7.5, 5), ); In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_slope_samples.posterior[\"cov\"].mean(dim=[\"chain\", \"draw\"]).values\n</pre> covariation_intercept_slope_samples.posterior[\"cov\"].mean(dim=[\"chain\", \"draw\"]).values Out[\u00a0]: <pre>array([[1.        , 0.06146909],\n       [0.06146909, 1.        ]])</pre> <p>Observing from the forest plot and the mean values, the correlation matrix is close to the Identity matrix. So, it is not a good idea to model the correlation using Mean Field ADVI as it fits the model with Diagonal Gaussian distribution. In general, any VI approximation does not care about hierarchies in the model. All it considers is to optimizer ELBO in an unbounded space.</p> <p>We have FullRank ADVI as a rescue that poses dense covariance matrix over all the parameters but it requires more memory and computational resources. Let's figure out if FullRank ADVI can capture some correlations.</p> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_full_rank = pm.fit(covariation_intercept_slope(), sample_size=10,\n                                         num_steps=25_000, method=\"fullrank_advi\")\nplot_elbo(covariation_intercept_full_rank.losses)\n</pre> covariation_intercept_full_rank = pm.fit(covariation_intercept_slope(), sample_size=10,                                          num_steps=25_000, method=\"fullrank_advi\") plot_elbo(covariation_intercept_full_rank.losses) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_full_rank_samples = covariation_intercept_full_rank.approximation.sample(4_000)\nremove_scope(covariation_intercept_full_rank_samples)\ncovariation_intercept_full_rank_samples.posterior[\"cov\"].values = np.matmul(\n    covariation_intercept_full_rank_samples.posterior[\"cov\"].values,\n    np.transpose(covariation_intercept_full_rank_samples.posterior[\"cov\"].values, (0, 1, 3, 2))\n)\ncovariation_intercept_full_rank_samples\n</pre> covariation_intercept_full_rank_samples = covariation_intercept_full_rank.approximation.sample(4_000) remove_scope(covariation_intercept_full_rank_samples) covariation_intercept_full_rank_samples.posterior[\"cov\"].values = np.matmul(     covariation_intercept_full_rank_samples.posterior[\"cov\"].values,     np.transpose(covariation_intercept_full_rank_samples.posterior[\"cov\"].values, (0, 1, 3, 2)) ) covariation_intercept_full_rank_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                  (__cholesky_cov_dim_0: 1, __log_sigma_slope_dim_0: 2, ab_county_dim_0: 85, ab_county_dim_1: 2, chain: 1, cov_dim_0: 2, cov_dim_1: 2, draw: 4000, sigma_slope_dim_0: 2)\nCoordinates:\n  * chain                    (chain) int64 0\n  * draw                     (draw) int64 0 1 2 3 4 ... 3995 3996 3997 3998 3999\n  * ab_county_dim_0          (ab_county_dim_0) int64 0 1 2 3 4 ... 81 82 83 84\n  * ab_county_dim_1          (ab_county_dim_1) int64 0 1\n  * __log_sigma_slope_dim_0  (__log_sigma_slope_dim_0) int64 0 1\n  * __cholesky_cov_dim_0     (__cholesky_cov_dim_0) int64 0\n  * sigma_slope_dim_0        (sigma_slope_dim_0) int64 0 1\n  * cov_dim_0                (cov_dim_0) int64 0 1\n  * cov_dim_1                (cov_dim_1) int64 0 1\nData variables:\n    mu_a                     (chain, draw) float64 1.442 1.411 ... 1.505 1.545\n    mu_b                     (chain, draw) float64 -0.5519 -0.6372 ... -0.6588\n    ab_county                (chain, draw, ab_county_dim_0, ab_county_dim_1) float64 ...\n    __log_sigma_slope        (chain, draw, __log_sigma_slope_dim_0) float64 -...\n    __cholesky_cov           (chain, draw, __cholesky_cov_dim_0) float64 0.04...\n    __log_sigma              (chain, draw) float64 -0.3121 -0.2898 ... -0.3897\n    sigma_slope              (chain, draw, sigma_slope_dim_0) float64 0.3378 ...\n    cov                      (chain, draw, cov_dim_0, cov_dim_1) float64 1.0 ...\n    sigma                    (chain, draw) float64 0.7319 0.7484 ... 0.6773\nAttributes:\n    created_at:     2020-09-06T15:18:55.592877\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__cholesky_cov_dim_0: 1</li><li>__log_sigma_slope_dim_0: 2</li><li>ab_county_dim_0: 85</li><li>ab_county_dim_1: 2</li><li>chain: 1</li><li>cov_dim_0: 2</li><li>cov_dim_1: 2</li><li>draw: 4000</li><li>sigma_slope_dim_0: 2</li></ul></li><li>Coordinates: (9)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 3996 3997 3998 3999<pre>array([   0,    1,    2, ..., 3997, 3998, 3999])</pre></li><li>ab_county_dim_0(ab_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li><li>ab_county_dim_1(ab_county_dim_1)int640 1<pre>array([0, 1])</pre></li><li>__log_sigma_slope_dim_0(__log_sigma_slope_dim_0)int640 1<pre>array([0, 1])</pre></li><li>__cholesky_cov_dim_0(__cholesky_cov_dim_0)int640<pre>array([0])</pre></li><li>sigma_slope_dim_0(sigma_slope_dim_0)int640 1<pre>array([0, 1])</pre></li><li>cov_dim_0(cov_dim_0)int640 1<pre>array([0, 1])</pre></li><li>cov_dim_1(cov_dim_1)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (9)<ul><li>mu_a(chain, draw)float641.442 1.411 1.458 ... 1.505 1.545<pre>array([[1.44157145, 1.41055974, 1.45796375, ..., 1.52769818, 1.50495889,\n        1.54501587]])</pre></li><li>mu_b(chain, draw)float64-0.5519 -0.6372 ... -0.5429 -0.6588<pre>array([[-0.55185719, -0.63719361, -0.64991064, ..., -0.60575006,\n        -0.54286851, -0.65876789]])</pre></li><li>ab_county(chain, draw, ab_county_dim_0, ab_county_dim_1)float641.103 0.06178 ... 1.495 -0.6167<pre>array([[[[ 1.10251159,  0.06177962],\n         [ 1.01775006, -0.99013645],\n         [ 1.84532091, -0.84903852],\n         ...,\n         [ 1.97449864, -1.73696854],\n         [ 1.44512079, -0.43008124],\n         [ 1.19366699, -0.38549995]],\n\n        [[ 1.33938165,  0.00469697],\n         [ 0.9895481 , -1.1903016 ],\n         [ 1.2565043 , -0.77370252],\n         ...,\n         [ 1.87720212, -1.0297701 ],\n         [ 1.51530009, -1.25217816],\n         [ 1.63933693, -1.10597871]],\n\n        [[ 1.41299128, -0.17744921],\n         [ 1.06667489, -0.85125227],\n         [ 1.17068493, -0.29846862],\n         ...,\n...\n         ...,\n         [ 1.38249088, -1.40076622],\n         [ 1.33664203, -1.0479582 ],\n         [ 1.43800977, -0.04056126]],\n\n        [[ 1.40232621, -0.65257964],\n         [ 0.92013147, -0.43971042],\n         [ 1.83521221, -1.1151818 ],\n         ...,\n         [ 1.78657534, -1.38489233],\n         [ 1.49688109, -0.39415821],\n         [ 1.19360837, -0.62458023]],\n\n        [[ 1.19015241,  0.07429586],\n         [ 0.83547864, -0.04993358],\n         [ 1.34631223, -0.56156258],\n         ...,\n         [ 1.40184397, -1.46157653],\n         [ 1.37037403, -1.38986458],\n         [ 1.49538317, -0.61670657]]]])</pre></li><li>__log_sigma_slope(chain, draw, __log_sigma_slope_dim_0)float64-1.085 -0.9142 ... -1.115 -0.7669<pre>array([[[-1.08524326, -0.91422757],\n        [-0.79367875, -0.79953138],\n        [-1.04254996, -0.78783721],\n        ...,\n        [-1.18658476, -0.76931894],\n        [-1.30549297, -0.92132119],\n        [-1.11496116, -0.76692019]]])</pre></li><li>__cholesky_cov(chain, draw, __cholesky_cov_dim_0)float640.0476 -0.3562 ... -0.4336 -0.1848<pre>array([[[ 0.04759938],\n        [-0.35615479],\n        [-0.19981954],\n        ...,\n        [ 0.03669606],\n        [-0.43361397],\n        [-0.18476808]]])</pre></li><li>__log_sigma(chain, draw)float64-0.3121 -0.2898 ... -0.303 -0.3897<pre>array([[-0.312052  , -0.28976289, -0.2830244 , ..., -0.33111312,\n        -0.30304402, -0.38970624]])</pre></li><li>sigma_slope(chain, draw, sigma_slope_dim_0)float640.3378 0.4008 ... 0.3279 0.4644<pre>array([[[0.3378196 , 0.40082612],\n        [0.45217828, 0.44953958],\n        [0.35255454, 0.45482743],\n        ...,\n        [0.30526203, 0.46332851],\n        [0.27103889, 0.39799287],\n        [0.32792802, 0.46444126]]])</pre></li><li>cov(chain, draw, cov_dim_0, cov_dim_1)float641.0 0.04755 0.04755 ... -0.1817 1.0<pre>array([[[[ 1.        ,  0.04754555],\n         [ 0.04754555,  1.        ]],\n\n        [[ 1.        , -0.33551077],\n         [-0.33551077,  1.        ]],\n\n        [[ 1.        , -0.19594598],\n         [-0.19594598,  1.        ]],\n\n        ...,\n\n        [[ 1.        ,  0.03667138],\n         [ 0.03667138,  1.        ]],\n\n        [[ 1.        , -0.3978242 ],\n         [-0.3978242 ,  1.        ]],\n\n        [[ 1.        , -0.18169268],\n         [-0.18169268,  1.        ]]]])</pre></li><li>sigma(chain, draw)float640.7319 0.7484 ... 0.7386 0.6773<pre>array([[0.73194347, 0.74844101, 0.7535014 , ..., 0.71812393, 0.73856659,\n        0.6772558 ]])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:18:55.592877arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:18:55.601440\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:18:55.601440arviz_version :0.9.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>az.plot_forest(\n    [varying_intercept_slope_samples, covariation_intercept_slope_samples, covariation_intercept_full_rank_samples],\n    model_names=[\"No covariation\", \"With covariation Mean Field\", \"With covariation Full Rank\"],\n    var_names=[\"mu_a\", \"mu_b\", \"cov\"],\n    combined=True,\n    figsize=(8, 8),\n);\n</pre> az.plot_forest(     [varying_intercept_slope_samples, covariation_intercept_slope_samples, covariation_intercept_full_rank_samples],     model_names=[\"No covariation\", \"With covariation Mean Field\", \"With covariation Full Rank\"],     var_names=[\"mu_a\", \"mu_b\", \"cov\"],     combined=True,     figsize=(8, 8), ); <p>So the correlation between slopes and intercepts seems to be negative: when a_county increases, b_county tends to decrease. Also the estimates are close for both <code>mu</code> and <code>sigma</code> when using <code>fullrank_advi</code>. MeanField and FullRank ADVI are extreme ends of computing correlations. But now, PyMC4 comes with a LowRank ADVI approximation to let us decide how much correlation we wish to see.</p> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_slope_advi.approximation.order.size\n</pre> covariation_intercept_slope_advi.approximation.order.size Out[\u00a0]: <pre>176</pre> <p>The flattened shape of all parameters in our model is 176. So, we can specify a <code>rank</code> anyway between 1 and 176. Closer to zero, represents MeanField behaviour and closer to 176, resembles FullRank behaviour.</p> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_low_rank = pm.fit(covariation_intercept_slope(), num_steps=20_000, method=\"lowrank_advi\", \n                                        sample_size=10, approx_kwargs=dict(rank=50))\nplot_elbo(covariation_intercept_low_rank.losses)\n</pre> covariation_intercept_low_rank = pm.fit(covariation_intercept_slope(), num_steps=20_000, method=\"lowrank_advi\",                                          sample_size=10, approx_kwargs=dict(rank=50)) plot_elbo(covariation_intercept_low_rank.losses) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_low_rank_samples = covariation_intercept_low_rank.approximation.sample(4_000)\nremove_scope(covariation_intercept_low_rank_samples)\ncovariation_intercept_low_rank_samples.posterior[\"cov\"].values = np.matmul(\n    covariation_intercept_low_rank_samples.posterior[\"cov\"].values, \n    np.transpose(covariation_intercept_low_rank_samples.posterior[\"cov\"].values, (0, 1, 3, 2))\n)\ncovariation_intercept_low_rank_samples\n</pre> covariation_intercept_low_rank_samples = covariation_intercept_low_rank.approximation.sample(4_000) remove_scope(covariation_intercept_low_rank_samples) covariation_intercept_low_rank_samples.posterior[\"cov\"].values = np.matmul(     covariation_intercept_low_rank_samples.posterior[\"cov\"].values,      np.transpose(covariation_intercept_low_rank_samples.posterior[\"cov\"].values, (0, 1, 3, 2)) ) covariation_intercept_low_rank_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                  (__cholesky_cov_dim_0: 1, __log_sigma_slope_dim_0: 2, ab_county_dim_0: 85, ab_county_dim_1: 2, chain: 1, cov_dim_0: 2, cov_dim_1: 2, draw: 4000, sigma_slope_dim_0: 2)\nCoordinates:\n  * chain                    (chain) int64 0\n  * draw                     (draw) int64 0 1 2 3 4 ... 3995 3996 3997 3998 3999\n  * ab_county_dim_0          (ab_county_dim_0) int64 0 1 2 3 4 ... 81 82 83 84\n  * ab_county_dim_1          (ab_county_dim_1) int64 0 1\n  * __log_sigma_slope_dim_0  (__log_sigma_slope_dim_0) int64 0 1\n  * __cholesky_cov_dim_0     (__cholesky_cov_dim_0) int64 0\n  * sigma_slope_dim_0        (sigma_slope_dim_0) int64 0 1\n  * cov_dim_0                (cov_dim_0) int64 0 1\n  * cov_dim_1                (cov_dim_1) int64 0 1\nData variables:\n    mu_a                     (chain, draw) float64 1.509 1.498 ... 1.532 1.449\n    mu_b                     (chain, draw) float64 -0.6815 -0.5409 ... -0.593\n    ab_county                (chain, draw, ab_county_dim_0, ab_county_dim_1) float64 ...\n    __log_sigma_slope        (chain, draw, __log_sigma_slope_dim_0) float64 -...\n    __cholesky_cov           (chain, draw, __cholesky_cov_dim_0) float64 -0.0...\n    __log_sigma              (chain, draw) float64 -0.3119 -0.3696 ... -0.3382\n    sigma_slope              (chain, draw, sigma_slope_dim_0) float64 0.3565 ...\n    cov                      (chain, draw, cov_dim_0, cov_dim_1) float64 1.0 ...\n    sigma                    (chain, draw) float64 0.732 0.691 ... 0.7456 0.7131\nAttributes:\n    created_at:     2020-09-06T15:20:40.418225\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__cholesky_cov_dim_0: 1</li><li>__log_sigma_slope_dim_0: 2</li><li>ab_county_dim_0: 85</li><li>ab_county_dim_1: 2</li><li>chain: 1</li><li>cov_dim_0: 2</li><li>cov_dim_1: 2</li><li>draw: 4000</li><li>sigma_slope_dim_0: 2</li></ul></li><li>Coordinates: (9)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 3996 3997 3998 3999<pre>array([   0,    1,    2, ..., 3997, 3998, 3999])</pre></li><li>ab_county_dim_0(ab_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li><li>ab_county_dim_1(ab_county_dim_1)int640 1<pre>array([0, 1])</pre></li><li>__log_sigma_slope_dim_0(__log_sigma_slope_dim_0)int640 1<pre>array([0, 1])</pre></li><li>__cholesky_cov_dim_0(__cholesky_cov_dim_0)int640<pre>array([0])</pre></li><li>sigma_slope_dim_0(sigma_slope_dim_0)int640 1<pre>array([0, 1])</pre></li><li>cov_dim_0(cov_dim_0)int640 1<pre>array([0, 1])</pre></li><li>cov_dim_1(cov_dim_1)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (9)<ul><li>mu_a(chain, draw)float641.509 1.498 1.426 ... 1.532 1.449<pre>array([[1.50925392, 1.49795824, 1.42629924, ..., 1.5308051 , 1.53216212,\n        1.44900649]])</pre></li><li>mu_b(chain, draw)float64-0.6815 -0.5409 ... -0.6193 -0.593<pre>array([[-0.68151757, -0.54089748, -0.56782709, ..., -0.76452922,\n        -0.61933998, -0.59300725]])</pre></li><li>ab_county(chain, draw, ab_county_dim_0, ab_county_dim_1)float641.177 -0.8777 ... 0.9354 -0.7091<pre>array([[[[ 1.17699098, -0.87772655],\n         [ 0.8719603 , -0.71295297],\n         [ 1.82976076, -0.47848699],\n         ...,\n         [ 1.66878237, -0.63678348],\n         [ 1.78509161, -0.97226677],\n         [ 1.38019166, -0.94294452]],\n\n        [[ 0.98922584, -0.14417853],\n         [ 1.03280072, -0.9385154 ],\n         [ 1.34471995, -0.60151779],\n         ...,\n         [ 1.41518306, -1.00600861],\n         [ 1.92616453, -0.18611176],\n         [ 1.43425089, -0.62545378]],\n\n        [[ 1.43652147, -0.81952798],\n         [ 0.93561256, -0.62141368],\n         [ 1.7604567 , -0.65514191],\n         ...,\n...\n         ...,\n         [ 1.42641328, -0.92645284],\n         [ 1.74696447, -0.85951273],\n         [ 1.34936729, -1.16444385]],\n\n        [[ 0.74230133, -0.34197424],\n         [ 0.9600017 , -1.20626791],\n         [ 1.78019199, -0.74515731],\n         ...,\n         [ 1.8721507 , -1.05775905],\n         [ 1.74983331, -0.5792016 ],\n         [ 1.3991723 , -0.36454429]],\n\n        [[ 1.26957593, -0.35145346],\n         [ 0.88842673, -0.62990523],\n         [ 1.33597802, -0.49303204],\n         ...,\n         [ 1.70985002, -0.59150253],\n         [ 1.78929784, -0.63468222],\n         [ 0.93541207, -0.70910543]]]])</pre></li><li>__log_sigma_slope(chain, draw, __log_sigma_slope_dim_0)float64-1.031 -1.275 ... -1.413 -1.243<pre>array([[[-1.03128578, -1.27532842],\n        [-1.03294963, -1.07077614],\n        [-1.27912088, -1.10397565],\n        ...,\n        [-1.24034039, -1.03842296],\n        [-1.15618308, -0.9692149 ],\n        [-1.41318963, -1.2434011 ]]])</pre></li><li>__cholesky_cov(chain, draw, __cholesky_cov_dim_0)float64-0.04151 -0.0801 ... 0.03822<pre>array([[[-0.04150945],\n        [-0.08010301],\n        [-0.0301403 ],\n        ...,\n        [-0.06599168],\n        [-0.09905543],\n        [ 0.0382171 ]]])</pre></li><li>__log_sigma(chain, draw)float64-0.3119 -0.3696 ... -0.2936 -0.3382<pre>array([[-0.31192621, -0.36962706, -0.33602381, ..., -0.31870618,\n        -0.29359504, -0.33817581]])</pre></li><li>sigma_slope(chain, draw, sigma_slope_dim_0)float640.3565 0.2793 ... 0.2434 0.2884<pre>array([[[0.35654822, 0.27933921],\n        [0.35595547, 0.3427424 ],\n        [0.27828184, 0.33155033],\n        ...,\n        [0.28928573, 0.35401253],\n        [0.31468502, 0.37938077],\n        [0.2433658 , 0.28840167]]])</pre></li><li>cov(chain, draw, cov_dim_0, cov_dim_1)float641.0 -0.04147 ... 0.03819 1.0<pre>array([[[[ 1.        , -0.04147374],\n         [-0.04147374,  1.        ]],\n\n        [[ 1.        , -0.07984725],\n         [-0.07984725,  1.        ]],\n\n        [[ 1.        , -0.03012662],\n         [-0.03012662,  1.        ]],\n\n        ...,\n\n        [[ 1.        , -0.06584846],\n         [-0.06584846,  1.        ]],\n\n        [[ 1.        , -0.09857301],\n         [-0.09857301,  1.        ]],\n\n        [[ 1.        ,  0.03818922],\n         [ 0.03818922,  1.        ]]]])</pre></li><li>sigma(chain, draw)float640.732 0.691 ... 0.7456 0.7131<pre>array([[0.73203555, 0.69099198, 0.71460609, ..., 0.72708915, 0.74557836,\n        0.71306991]])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:20:40.418225arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:20:40.423839\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:20:40.423839arviz_version :0.9.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_low_rank_150 = pm.fit(covariation_intercept_slope(), num_steps=20_000, method=\"lowrank_advi\", \n                                        sample_size=10, approx_kwargs=dict(rank=150))\n</pre> covariation_intercept_low_rank_150 = pm.fit(covariation_intercept_slope(), num_steps=20_000, method=\"lowrank_advi\",                                          sample_size=10, approx_kwargs=dict(rank=150)) <pre>|&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|</pre> In\u00a0[\u00a0]: Copied! <pre>covariation_intercept_low_rank_150_samples = covariation_intercept_low_rank_150.approximation.sample(4_000)\nremove_scope(covariation_intercept_low_rank_150_samples)\ncovariation_intercept_low_rank_150_samples.posterior[\"cov\"].values = np.matmul(\n    covariation_intercept_low_rank_150_samples.posterior[\"cov\"].values, \n    np.transpose(covariation_intercept_low_rank_150_samples.posterior[\"cov\"].values, (0, 1, 3, 2))\n)\ncovariation_intercept_low_rank_150_samples\n</pre> covariation_intercept_low_rank_150_samples = covariation_intercept_low_rank_150.approximation.sample(4_000) remove_scope(covariation_intercept_low_rank_150_samples) covariation_intercept_low_rank_150_samples.posterior[\"cov\"].values = np.matmul(     covariation_intercept_low_rank_150_samples.posterior[\"cov\"].values,      np.transpose(covariation_intercept_low_rank_150_samples.posterior[\"cov\"].values, (0, 1, 3, 2)) ) covariation_intercept_low_rank_150_samples Out[\u00a0]: arviz.InferenceData <ul> <li> posterior <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:                  (__cholesky_cov_dim_0: 1, __log_sigma_slope_dim_0: 2, ab_county_dim_0: 85, ab_county_dim_1: 2, chain: 1, cov_dim_0: 2, cov_dim_1: 2, draw: 4000, sigma_slope_dim_0: 2)\nCoordinates:\n  * chain                    (chain) int64 0\n  * draw                     (draw) int64 0 1 2 3 4 ... 3995 3996 3997 3998 3999\n  * ab_county_dim_0          (ab_county_dim_0) int64 0 1 2 3 4 ... 81 82 83 84\n  * ab_county_dim_1          (ab_county_dim_1) int64 0 1\n  * __log_sigma_slope_dim_0  (__log_sigma_slope_dim_0) int64 0 1\n  * __cholesky_cov_dim_0     (__cholesky_cov_dim_0) int64 0\n  * sigma_slope_dim_0        (sigma_slope_dim_0) int64 0 1\n  * cov_dim_0                (cov_dim_0) int64 0 1\n  * cov_dim_1                (cov_dim_1) int64 0 1\nData variables:\n    mu_a                     (chain, draw) float64 1.39 1.411 ... 1.627 1.53\n    mu_b                     (chain, draw) float64 -0.5366 -0.5627 ... -0.5805\n    ab_county                (chain, draw, ab_county_dim_0, ab_county_dim_1) float64 ...\n    __log_sigma_slope        (chain, draw, __log_sigma_slope_dim_0) float64 -...\n    __cholesky_cov           (chain, draw, __cholesky_cov_dim_0) float64 -0.2...\n    __log_sigma              (chain, draw) float64 -0.3665 -0.3195 ... -0.3739\n    sigma_slope              (chain, draw, sigma_slope_dim_0) float64 0.3012 ...\n    cov                      (chain, draw, cov_dim_0, cov_dim_1) float64 1.0 ...\n    sigma                    (chain, draw) float64 0.6931 0.7265 ... 0.6881\nAttributes:\n    created_at:     2020-09-06T15:23:17.272501\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>__cholesky_cov_dim_0: 1</li><li>__log_sigma_slope_dim_0: 2</li><li>ab_county_dim_0: 85</li><li>ab_county_dim_1: 2</li><li>chain: 1</li><li>cov_dim_0: 2</li><li>cov_dim_1: 2</li><li>draw: 4000</li><li>sigma_slope_dim_0: 2</li></ul></li><li>Coordinates: (9)<ul><li>chain(chain)int640<pre>array([0])</pre></li><li>draw(draw)int640 1 2 3 4 ... 3996 3997 3998 3999<pre>array([   0,    1,    2, ..., 3997, 3998, 3999])</pre></li><li>ab_county_dim_0(ab_county_dim_0)int640 1 2 3 4 5 6 ... 79 80 81 82 83 84<pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84])</pre></li><li>ab_county_dim_1(ab_county_dim_1)int640 1<pre>array([0, 1])</pre></li><li>__log_sigma_slope_dim_0(__log_sigma_slope_dim_0)int640 1<pre>array([0, 1])</pre></li><li>__cholesky_cov_dim_0(__cholesky_cov_dim_0)int640<pre>array([0])</pre></li><li>sigma_slope_dim_0(sigma_slope_dim_0)int640 1<pre>array([0, 1])</pre></li><li>cov_dim_0(cov_dim_0)int640 1<pre>array([0, 1])</pre></li><li>cov_dim_1(cov_dim_1)int640 1<pre>array([0, 1])</pre></li></ul></li><li>Data variables: (9)<ul><li>mu_a(chain, draw)float641.39 1.411 1.571 ... 1.627 1.53<pre>array([[1.38981514, 1.41102252, 1.57056486, ..., 1.49308907, 1.62727705,\n        1.52984406]])</pre></li><li>mu_b(chain, draw)float64-0.5366 -0.5627 ... -0.6235 -0.5805<pre>array([[-0.5366487 , -0.56272015, -0.78026924, ..., -0.62740262,\n        -0.6235238 , -0.58051591]])</pre></li><li>ab_county(chain, draw, ab_county_dim_0, ab_county_dim_1)float641.425 -0.526 1.04 ... 1.495 -0.1514<pre>array([[[[ 1.42506697, -0.52604944],\n         [ 1.03981901, -0.68456314],\n         [ 1.25016548, -0.22305214],\n         ...,\n         [ 1.76422553, -0.89457632],\n         [ 1.57397667, -0.3100903 ],\n         [ 1.53707972, -1.03427611]],\n\n        [[ 1.21160443,  0.14193781],\n         [ 1.04232542, -0.55043179],\n         [ 1.36190382, -0.50466553],\n         ...,\n         [ 1.63148471, -1.2569052 ],\n         [ 1.34257446, -0.18240088],\n         [ 1.50646085, -0.31975188]],\n\n        [[ 1.3345931 , -0.69556278],\n         [ 1.0765899 , -0.81404235],\n         [ 1.40043841, -1.11853939],\n         ...,\n...\n         ...,\n         [ 2.00472532, -1.36699321],\n         [ 1.5679758 , -0.17008079],\n         [ 1.73254455, -0.26321725]],\n\n        [[ 1.08675031, -0.46753599],\n         [ 0.86289356, -0.19610458],\n         [ 2.15038148, -0.4468328 ],\n         ...,\n         [ 1.97438806, -0.9933176 ],\n         [ 1.69074963, -0.5830286 ],\n         [ 1.58846354, -0.22340221]],\n\n        [[ 0.43115089, -0.01547547],\n         [ 1.10117516, -0.27906998],\n         [ 1.74180379, -0.70284035],\n         ...,\n         [ 1.99408171, -1.17529547],\n         [ 1.53073065, -0.52420763],\n         [ 1.49453539, -0.15143056]]]])</pre></li><li>__log_sigma_slope(chain, draw, __log_sigma_slope_dim_0)float64-1.2 -1.078 ... -0.9892 -1.029<pre>array([[[-1.19987902, -1.07827793],\n        [-1.05150271, -0.99156852],\n        [-1.09105603, -1.08172808],\n        ...,\n        [-0.95469953, -1.08248219],\n        [-1.17356413, -1.18389744],\n        [-0.98921242, -1.02942615]]])</pre></li><li>__cholesky_cov(chain, draw, __cholesky_cov_dim_0)float64-0.2201 -0.01191 ... -0.1023<pre>array([[[-0.22008592],\n        [-0.01191335],\n        [-0.0259371 ],\n        ...,\n        [-0.13640177],\n        [-0.28041118],\n        [-0.1022795 ]]])</pre></li><li>__log_sigma(chain, draw)float64-0.3665 -0.3195 ... -0.3517 -0.3739<pre>array([[-0.3665242 , -0.31953318, -0.31282419, ..., -0.31213638,\n        -0.35173813, -0.37388115]])</pre></li><li>sigma_slope(chain, draw, sigma_slope_dim_0)float640.3012 0.3402 ... 0.3719 0.3572<pre>array([[[0.30123065, 0.34018084],\n        [0.34941229, 0.37099432],\n        [0.33586163, 0.33900918],\n        ...,\n        [0.38492779, 0.33875363],\n        [0.30926272, 0.30608347],\n        [0.37186945, 0.35721189]]])</pre></li><li>cov(chain, draw, cov_dim_0, cov_dim_1)float641.0 -0.2149 -0.2149 ... -0.1017 1.0<pre>array([[[[ 1.        , -0.21494182],\n         [-0.21494182,  1.        ]],\n\n        [[ 1.        , -0.0119125 ],\n         [-0.0119125 ,  1.        ]],\n\n        [[ 1.        , -0.02592838],\n         [-0.02592838,  1.        ]],\n\n        ...,\n\n        [[ 1.        , -0.1351503 ],\n         [-0.1351503 ,  1.        ]],\n\n        [[ 1.        , -0.26999703],\n         [-0.26999703,  1.        ]],\n\n        [[ 1.        , -0.10174868],\n         [-0.10174868,  1.        ]]]])</pre></li><li>sigma(chain, draw)float640.6931 0.7265 ... 0.7035 0.6881<pre>array([[0.69313936, 0.7264881 , 0.73137849, ..., 0.73188171, 0.70346431,\n        0.68805868]])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:23:17.272501arviz_version :0.9.0</li></ul> </ul> </li> <li> observed_data <ul> <pre>&lt;xarray.Dataset&gt;\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 0 1 2 3 4 5 6 7 ... 911 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 0.8329 0.8329 1.099 0.09531 ... 1.629 1.335 1.099\nAttributes:\n    created_at:     2020-09-06T15:23:17.277314\n    arviz_version:  0.9.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>obs_id: 919</li></ul></li><li>Coordinates: (1)<ul><li>obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918<pre>array([  0,   1,   2, ..., 916, 917, 918])</pre></li></ul></li><li>Data variables: (1)<ul><li>y(obs_id)float640.8329 0.8329 1.099 ... 1.335 1.099<pre>array([ 0.83290912,  0.83290912,  1.09861229,  0.09531018,  1.16315081,\n        0.95551145,  0.47000363,  0.09531018, -0.22314355,  0.26236426,\n        0.26236426,  0.33647224,  0.40546511, -0.69314718,  0.18232156,\n        1.5260563 ,  0.33647224,  0.78845736,  1.79175947,  1.22377543,\n        0.64185389,  1.70474809,  1.85629799,  0.69314718,  1.90210753,\n        1.16315081,  1.93152141,  1.96009478,  2.05412373,  1.66770682,\n        1.5260563 ,  1.5040774 ,  1.06471074,  2.10413415,  0.53062825,\n        1.45861502,  1.70474809,  1.41098697,  0.87546874,  1.09861229,\n        0.40546511,  1.22377543,  1.09861229,  0.64185389, -1.2039728 ,\n        0.91629073,  0.18232156,  0.83290912, -0.35667494,  0.58778666,\n        1.09861229,  0.83290912,  0.58778666,  0.40546511,  0.69314718,\n        0.64185389,  0.26236426,  1.48160454,  1.5260563 ,  1.85629799,\n        1.54756251,  1.75785792,  0.83290912, -0.69314718,  1.54756251,\n        1.5040774 ,  1.90210753,  1.02961942,  1.09861229,  1.09861229,\n        1.98787435,  1.62924054,  0.99325177,  1.62924054,  2.57261223,\n        1.98787435,  1.93152141,  2.55722731,  1.77495235,  2.2617631 ,\n        1.80828877,  1.36097655,  2.66722821,  0.64185389,  1.94591015,\n        1.56861592,  2.2617631 ,  0.95551145,  1.91692261,  1.41098697,\n        2.32238772,  0.83290912,  0.64185389,  1.25276297,  1.74046617,\n        1.48160454,  1.38629436,  0.33647224,  1.45861502, -0.10536052,\n...\n        1.80828877,  1.09861229,  1.91692261,  2.96527307,  1.41098697,\n        1.79175947,  2.20827441,  2.14006616,  0.18232156,  1.16315081,\n        2.4510051 ,  2.27212589,  1.09861229, -0.22314355,  1.19392247,\n        1.56861592,  1.58923521, -0.69314718,  2.24070969,  0.58778666,\n        0.        ,  2.3321439 ,  2.05412373,  0.83290912,  1.88706965,\n        2.50959926,  1.54756251,  1.84054963,  1.88706965,  1.06471074,\n        0.69314718,  0.26236426,  0.91629073,  0.09531018,  0.26236426,\n        0.53062825, -0.10536052,  0.58778666,  1.56861592,  0.58778666,\n        1.22377543, -0.10536052,  2.29253476,  1.68639895,  2.1517622 ,\n        0.69314718,  1.90210753,  1.36097655,  1.79175947,  1.60943791,\n        0.95551145,  2.37954613,  0.91629073,  0.78845736,  1.56861592,\n        1.33500107,  2.60268969,  1.09861229,  1.48160454,  1.36097655,\n        0.64185389,  0.47000363,  0.64185389,  0.33647224,  1.90210753,\n        3.02042489,  1.80828877,  2.63188884,  2.3321439 ,  1.75785792,\n        2.24070969,  1.25276297,  1.43508453,  2.45958884,  1.98787435,\n        1.56861592,  0.64185389, -0.22314355,  1.56861592,  2.3321439 ,\n        2.43361336,  2.04122033,  2.4765384 , -0.51082562,  1.91692261,\n        1.68639895,  1.16315081,  0.78845736,  2.00148   ,  1.64865863,\n        0.83290912,  0.87546874,  2.77258872,  2.2617631 ,  1.87180218,\n        1.5260563 ,  1.62924054,  1.33500107,  1.09861229])</pre></li></ul></li><li>Attributes: (2)created_at :2020-09-06T15:23:17.277314arviz_version :0.9.0</li></ul> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>az.plot_forest(\n    [\n        varying_intercept_slope_samples,\n        covariation_intercept_slope_samples,\n        covariation_intercept_low_rank_samples,\n        covariation_intercept_low_rank_150_samples,\n        covariation_intercept_full_rank_samples\n    ],\n    model_names=[\"No covariation\", \"Mean Field\", \"Low Rank(50)\", \"Low Rank(150)\", \"Full Rank\"],\n    var_names=[\"cov\"],\n    combined=True,\n    figsize=(6.5, 8),\n);\n</pre> az.plot_forest(     [         varying_intercept_slope_samples,         covariation_intercept_slope_samples,         covariation_intercept_low_rank_samples,         covariation_intercept_low_rank_150_samples,         covariation_intercept_full_rank_samples     ],     model_names=[\"No covariation\", \"Mean Field\", \"Low Rank(50)\", \"Low Rank(150)\", \"Full Rank\"],     var_names=[\"cov\"],     combined=True,     figsize=(6.5, 8), ); <p>We can see a negative coorelation as the mean of correlation matrix lies around -0.25. This means when the intercept increases, the slope decreases. Interestingly, by increasing the rank for LowRank ADVI, the correlation is getting close to that in case of FullRank ADVI one.</p> In\u00a0[\u00a0]: Copied! <pre>a_county_cov = covariation_intercept_full_rank_samples.posterior[\"ab_county\"].mean(dim=(\"chain\", \"draw\"))[:, 0]\nb_county_cov = covariation_intercept_full_rank_samples.posterior[\"ab_county\"].mean(dim=(\"chain\", \"draw\"))[:, 1]\n\n# plot both and connect with lines\nplt.scatter(avg_a_county, avg_b_county, label=\"No cov estimates\", alpha=0.6)\nplt.scatter(\n    a_county_cov,\n    b_county_cov,\n    facecolors=\"none\",\n    edgecolors=\"k\",\n    lw=1,\n    label=\"With cov estimates\",\n    alpha=0.8,\n)\nplt.plot([avg_a_county, a_county_cov], [avg_b_county, b_county_cov], \"k-\", alpha=0.5)\nplt.xlabel(\"Intercept\")\nplt.ylabel(\"Slope\")\nplt.legend();\n</pre> a_county_cov = covariation_intercept_full_rank_samples.posterior[\"ab_county\"].mean(dim=(\"chain\", \"draw\"))[:, 0] b_county_cov = covariation_intercept_full_rank_samples.posterior[\"ab_county\"].mean(dim=(\"chain\", \"draw\"))[:, 1]  # plot both and connect with lines plt.scatter(avg_a_county, avg_b_county, label=\"No cov estimates\", alpha=0.6) plt.scatter(     a_county_cov,     b_county_cov,     facecolors=\"none\",     edgecolors=\"k\",     lw=1,     label=\"With cov estimates\",     alpha=0.8, ) plt.plot([avg_a_county, a_county_cov], [avg_b_county, b_county_cov], \"k-\", alpha=0.5) plt.xlabel(\"Intercept\") plt.ylabel(\"Slope\") plt.legend(); <p>Interestingly, the differences between both models occur at extreme slope and intercept values. This is because the second model used the slightly negative correlation between intercepts and slopes to adjust their estimates and brings out more information from the data.</p> <ul> <li>Mean Field tends to scale better to larger datasets where correlation between parameters is not much concerned.</li> <li>Fitting data with FullRank can capture very good correlations on par with MCMC but comes with its own computational problems.</li> <li>LowRank ADVI can be seen as a mid-way of both extremes.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>%load_ext watermark\n%watermark -n -u -v -iv -w\n</pre> %load_ext watermark %watermark -n -u -v -iv -w <pre>pandas     1.1.1\narviz      0.9.0\ntensorflow 2.4.0-dev20200905\npymc4      4.0a2\nlogging    0.5.1.2\nxarray     0.16.0\nnumpy      1.18.5\nlast updated: Sun Sep 06 2020 \n\nCPython 3.8.5\nIPython 7.18.1\nwatermark 2.0.2\n</pre>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#multilevel-modelling-with-variational-inference","title":"Multilevel Modelling with Variational Inference\u00b6","text":"<p>There have been two reasons for writing this notebook -</p> <ol> <li>To have a port of Multilevel modelling from PyMC3 to PyMC4.</li> <li>To test the Variational Inference API added this summer.</li> </ol>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#radon-contamination-gelman-and-hill-2006","title":"Radon contamination (Gelman and Hill 2006)\u00b6","text":"<p>Radon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household. The EPA did a study of radon levels in 80,000 houses. There are two important predictors:</p> <ul> <li><p>Measurement in basement or first floor (radon higher in basements)</p> </li> <li><p>Measurement of Uranium level available at county level</p> </li> </ul> <p>We will focus on modeling radon levels in Minnesota. The hierarchy in this example is households within county.</p> <p>The model building has been inspired from TFP port of Multilevel modelling and the visualizations have been borrowed from PyMC3's Multilevel modelling.</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#conventional-approaches","title":"Conventional approaches\u00b6","text":"<p>Before comparing ADVI approximations on hierarchical models, lets model radon exposure by conventional approaches -</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#complete-pooling","title":"Complete pooling:\u00b6","text":"<p>Treat all counties the same, and estimate a single radon level. $$ y_i = \\alpha + \\beta x_i + \\epsilon_i $$ where $y_i$ is the logarithm of radon level in house $i$, $x_i$ is the floor of measurement (either basement or first floor) and $\\epsilon_i$ are the errors representing measurement error, temporal within-house variation, or variation among houses. The model directly translates to PyMC4 as -</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#no-pooling","title":"No pooling:\u00b6","text":"<p>Here we do not pool the estimates of the intercepts but completely pool the slope estimates assuming the variance is same within each county. $$ y_i = \\alpha_{j[i]} + \\beta x_i + \\epsilon_i $$ where $j$ = 1, ..., 85 representing each county.</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#multilevel-and-hierarchical-models","title":"Multilevel and hierarchical models\u00b6","text":"<p>When we pool our data, we imply that they are sampled from the same model. This ignores any variation among sampling units (other than sampling variance) -- we assume that counties are all the same:</p> <p></p> <p>When we analyze data unpooled, we imply that they are sampled independently from separate models. At the opposite extreme from the pooled case, this approach claims that differences between sampling units are too large to combine them -- we assume that counties have no similarity whatsoever:</p> <p></p> <p>In a hierarchical model, parameters are viewed as a sample from a population distribution of parameters. Thus, we view them as being neither entirely different or exactly the same. This is partial pooling:</p> <p></p> <p>We can use PyMC to easily specify multilevel models, and fit them using Variational Inference Approximations.</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#partial-pooling-model","title":"Partial pooling model\u00b6","text":"<p>The simplest partial pooling model for the household radon dataset is one which simply estimates radon levels, without any predictors at any level. A partial pooling model represents a compromise between the pooled and unpooled extremes, approximately a weighted average (based on sample size) of the unpooled county estimates and the pooled estimates.</p> <p>$$\\hat{\\alpha} \\approx \\frac{(n_j/\\sigma_y^2)\\bar{y}_j + (1/\\sigma_{\\alpha}^2)\\bar{y}}{(n_j/\\sigma_y^2) + (1/\\sigma_{\\alpha}^2)}$$ where $n_j$ is the number of houses for county $j$, $\\sigma_y^{2}$ is within county variance, and $\\sigma_a^{2}$ is the variance among the average log radon levels of the different counties.</p> <p>We expect the following when using partial pooling:</p> <ul> <li>Estimates for counties with smaller sample sizes will shrink towards the state-wide average.</li> <li>Estimates for counties with larger sample sizes will be closer to the unpooled county estimates and will influence the the state-wide average.</li> </ul>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#varying-intercept-model","title":"Varying intercept model\u00b6","text":"<p>As above, this model allows intercepts to vary across county, according to a random effect. We just add a fixed slope for the predictor (i.e all counties will have the same slope):</p> <p>$$y_i = \\alpha_{j[i]} + \\beta x_{i} + \\epsilon_i$$</p> <p>where</p> <p>$$\\epsilon_i \\sim N(0, \\sigma_y^2)$$</p> <p>and the intercept random effect:</p> <p>$$\\alpha_{j[i]} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$$</p> <p>As with the the no-pooling model, we set a separate intercept for each county, but rather than fitting separate regression models for each county, multilevel modeling shares strength among counties, allowing for more reasonable inference in counties with little data. Here is what that looks in code:</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#varying-intercept-and-slope-model","title":"Varying intercept and slope model\u00b6","text":"<p>Let's model the effect floor by considering the slope to vary by county as well $$ y_i = \\alpha_{j[i]} + \\beta_{j[i]} + \\epsilon_i $$</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#covariation-intercept-model","title":"Covariation intercept model\u00b6","text":"<p>$$y \\sim Normal(\\theta, \\sigma)$$</p> <p>$$\\theta = \\alpha_{COUNTY} + \\beta_{COUNTY} \\times floor$$</p> <p>$$\\begin{bmatrix} \\alpha_{COUNTY} \\\\ \\beta_{COUNTY} \\end{bmatrix} \\sim MvNormal(\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\Sigma)$$</p> <p>$$\\Sigma = \\begin{pmatrix} \\sigma_{\\alpha} &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta} \\end{pmatrix}      P      \\begin{pmatrix} \\sigma_{\\alpha} &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta} \\end{pmatrix}$$</p> <p>Every correlation matrix can be split into product of its Lower Traingular cholesky factors. $$ P = L*L^T $$ We will make use of <code>LKJCholesky</code> distribution from PyMC4 to generate lower triangular cholesky matrix $L$ and then to match its support from unconstrained space, we need to pass a bijector <code>tfb.CorrelationCholesky</code> as well. Variational Inference and even MCMC approximations work in unconstrained regions.</p> <p>Read more here in PyMC3 docs about the use of <code>LKJ priors</code> and this blog post by Adam Haber to know more about correlation matrices.</p>"},{"location":"bayesian/multilevel_modelling_with_variational_inference/#todo","title":"TODO\u00b6","text":"<p>Perform waic and loo tests to compare models. I need to learn about this.</p>"},{"location":"bayesian/partial_pooling_demonstration/","title":"A Gentle Introduction to Partial Pooled models","text":"In\u00a0[1]: Copied! <pre># Necessary imports\nimport pymc as pm\nimport pandas as pd\nimport numpy as np\nimport arviz as az\n\nfrom typing import List\nimport matplotlib.pyplot as plt\n\n%config InlineBackend.figure_format = 'retina'\nRANDOM_SEED = 42\naz.style.use('arviz-darkgrid')\n</pre> # Necessary imports import pymc as pm import pandas as pd import numpy as np import arviz as az  from typing import List import matplotlib.pyplot as plt  %config InlineBackend.figure_format = 'retina' RANDOM_SEED = 42 az.style.use('arviz-darkgrid') In\u00a0[2]: Copied! <pre># The total number of schools in the city\nnum_of_schools = 50\n\n# The number of data samples (students) for each school. \ndata_samples_per_school = np.random.randint(2, 100, size=num_of_schools)\n\n# The average performance of students in the city. Lets say the performance is measured out of 100\ntrue_global_mean = 63\n\n# The variation around the global mean, which represents the average performance of students across all schools, follows a normal distribution with a mean of 63 and a standard deviation of 2.\ntrue_global_variability = 2\n\n# The variability in the exam scores within each school\nwithin_school_variability = 10\n</pre> # The total number of schools in the city num_of_schools = 50  # The number of data samples (students) for each school.  data_samples_per_school = np.random.randint(2, 100, size=num_of_schools)  # The average performance of students in the city. Lets say the performance is measured out of 100 true_global_mean = 63  # The variation around the global mean, which represents the average performance of students across all schools, follows a normal distribution with a mean of 63 and a standard deviation of 2. true_global_variability = 2  # The variability in the exam scores within each school within_school_variability = 10 In\u00a0[3]: Copied! <pre>rng = np.random.default_rng(seed=RANDOM_SEED)\n\ndef generate_data(*, num_of_schools: int, data_samples_per_school: List[int], true_global_mean: int, true_global_variability: int, within_school_variability: int):\n\n    # Generate true mean performance for each school around the global mean\n    true_school_means = rng.normal(true_global_mean, true_global_variability, num_of_schools)\n\n    # Generate synthetic academic performance data for each school\n    data = []\n    for school, num_samples in enumerate(data_samples_per_school):\n\n        # Generate data samples for each school around their true mean\n        school_data = rng.normal(true_school_means[school], within_school_variability, num_samples)\n\n        # Append data with school index for each generated value\n        for value in school_data:\n            data.append({'School ID': school, 'score': value})\n        \n    data = pd.DataFrame(data)\n    data.index.name = \"Student ID\"\n    return true_school_means, data\n\n# Create data\ntrue_group_means, data = generate_data(\n    num_of_schools=num_of_schools,\n    data_samples_per_school=data_samples_per_school,\n    true_global_mean=true_global_mean,\n    true_global_variability=true_global_variability,\n    within_school_variability=within_school_variability\n)\n\n# Shuffle data\ndata = data.sample(frac=1)\n\ndata.head()\n</pre> rng = np.random.default_rng(seed=RANDOM_SEED)  def generate_data(*, num_of_schools: int, data_samples_per_school: List[int], true_global_mean: int, true_global_variability: int, within_school_variability: int):      # Generate true mean performance for each school around the global mean     true_school_means = rng.normal(true_global_mean, true_global_variability, num_of_schools)      # Generate synthetic academic performance data for each school     data = []     for school, num_samples in enumerate(data_samples_per_school):          # Generate data samples for each school around their true mean         school_data = rng.normal(true_school_means[school], within_school_variability, num_samples)          # Append data with school index for each generated value         for value in school_data:             data.append({'School ID': school, 'score': value})              data = pd.DataFrame(data)     data.index.name = \"Student ID\"     return true_school_means, data  # Create data true_group_means, data = generate_data(     num_of_schools=num_of_schools,     data_samples_per_school=data_samples_per_school,     true_global_mean=true_global_mean,     true_global_variability=true_global_variability,     within_school_variability=within_school_variability )  # Shuffle data data = data.sample(frac=1)  data.head() Out[3]: School ID score Student ID 1697 33 64.510645 314 6 62.005591 1786 35 67.453794 1730 35 73.349892 1224 23 56.454934 <p>Here is how the distribution of students in each school looks like:</p> In\u00a0[4]: Copied! <pre>group_counts = data['School ID'].value_counts()\n\n# Create a bar plot\nplt.figure(figsize=(10, 6))\nplt.bar(group_counts.index, group_counts.values)\n\n# Add labels and title\nplt.xlabel('School Index')\nplt.ylabel('Number of Students')\nplt.title('Number of Students in Each School')\n\nplt.show();\n</pre> group_counts = data['School ID'].value_counts()  # Create a bar plot plt.figure(figsize=(10, 6)) plt.bar(group_counts.index, group_counts.values)  # Add labels and title plt.xlabel('School Index') plt.ylabel('Number of Students') plt.title('Number of Students in Each School')  plt.show(); <p>It can be observed that there are a few schools which has a lot lesser no of students compared to other schools.</p> <p>Lets also plot the distribution of student's performance scores for first 4 schools, to help us get a glimpse of how the data looks like:</p> In\u00a0[5]: Copied! <pre># Set up 2x2 subplots for the first 4 schools\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8), sharey=True)\n\n# Plot performance scores for each school\nfor school_index, ax in enumerate(axes.flatten()):\n    school_data = data[data['School ID'] == school_index]\n    \n    ax.hist(school_data['score'], bins=20, color='skyblue', edgecolor='black')\n    ax.set_title(f'School {school_index + 1} Performance Distribution')\n    ax.set_xlabel('Performance Score')\n    ax.set_ylabel('Frequency')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\nplt.show();\n</pre> # Set up 2x2 subplots for the first 4 schools fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8), sharey=True)  # Plot performance scores for each school for school_index, ax in enumerate(axes.flatten()):     school_data = data[data['School ID'] == school_index]          ax.hist(school_data['score'], bins=20, color='skyblue', edgecolor='black')     ax.set_title(f'School {school_index + 1} Performance Distribution')     ax.set_xlabel('Performance Score')     ax.set_ylabel('Frequency')  # Adjust layout to prevent overlap plt.tight_layout()  plt.show();  <pre>/var/folders/ht/b4r2nlz93qv31tp71vbdb5hm0000gn/T/ipykernel_30691/1436856505.py:14: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n</pre> <p>There are two approaches to estimate performance of students:</p> In\u00a0[6]: Copied! <pre># Here is the implementation of unpooled model in pymc\n\nwith pm.Model() as unpooled_model:\n\n    # School specific information\n    # We'll assume a reasonable mean of 50 as a prior of city-wide average performance of students\n    school_means = pm.Normal(\"school_means\", mu=50, sigma=10, shape=num_of_schools)\n\n    school_idx = data['School ID'].values\n\n    # Model error\n    sigma_y = pm.Exponential('sigma_y', 1)\n\n    # Likelihood\n    y = pm.Normal(\"y\", mu=school_means[school_idx], sigma=sigma_y, observed=data['score'])\n\n# Plot the graphical structure of the unpooled model\npm.model_to_graphviz(unpooled_model)\n</pre> # Here is the implementation of unpooled model in pymc  with pm.Model() as unpooled_model:      # School specific information     # We'll assume a reasonable mean of 50 as a prior of city-wide average performance of students     school_means = pm.Normal(\"school_means\", mu=50, sigma=10, shape=num_of_schools)      school_idx = data['School ID'].values      # Model error     sigma_y = pm.Exponential('sigma_y', 1)      # Likelihood     y = pm.Normal(\"y\", mu=school_means[school_idx], sigma=sigma_y, observed=data['score'])  # Plot the graphical structure of the unpooled model pm.model_to_graphviz(unpooled_model) Out[6]: In\u00a0[7]: Copied! <pre># Using MCMC to estimate the model's parameters\nwith unpooled_model:\n    unpooled_trace = pm.sample(random_seed=RANDOM_SEED)\n</pre> # Using MCMC to estimate the model's parameters with unpooled_model:     unpooled_trace = pm.sample(random_seed=RANDOM_SEED) <pre>Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [school_means, sigma_y]\n</pre>        100.00% [8000/8000 00:07&lt;00:00 Sampling 4 chains, 0 divergences]      <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 73 seconds.\n</pre> <p>Equation for estimating group level mean in partially pooled model can be written as:</p> <p>$$ \\hat{\\alpha}_{j}^{\\text{partial-pooled}} \\approx \\frac{\\frac{n_j}{\\sigma_y^2} \\bar{y}_j + \\frac{1}{\\sigma_{\\alpha}^2} \\bar{y}_{\\text{all}}}{\\frac{n_j}{\\sigma_y^2} + \\frac{1}{\\sigma_{\\alpha}^2}} $$</p> <ul> <li>$\\hat{\\alpha}_{j}^{\\text{partial-pooled}}$: The estimated mean for group $j$ in the partial-pooled model.</li> <li>$n_j$: The number of observations in group $j$.</li> <li>$\\bar{y}_j$: The observed mean for group $j$.</li> <li>$\\sigma_y^2$: The variance within groups.</li> <li>$\\sigma_{\\alpha}^2$: The variance between groups.</li> <li>$\\bar{y}_{\\text{all}}$: The overall observed mean across all groups.</li> </ul> <p>In our example, a school is a group.</p> <p>Through the equation we can observe that estimates for groups with smaller sample sizes will shrink towards the city-wide average, while those for groups with larger sample sizes will be closer to the unpooled school estimates.</p> <p>Here is the derivation of the equation: This is the real reason for me writing this blog post, i.e. to share the derivation of group level mean estimates of partially pooled model</p> Derivation <p> <p>% Assuming a normal model for the data $y_j$ for group j with a normal prior for the mean $\\alpha_j$.</p> <p>% Let $y_j = N(\\alpha_j, \\sigma^2_y)$ and $\\alpha_j = N(\\mu, \\sigma^2_{\\alpha})$</p> <p>% The likelihood is given by the normal distribution: $$ p(y_j | \\alpha_j) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(y_j - \\alpha_j)^2}{2\\sigma^2_y}\\right) $$</p> <p>% The log likelihood is the logarithm of the above: $$ \\log p(y_j | \\alpha_j) = \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}\\right) -\\frac{(y_j - \\alpha_j)^2}{2\\sigma^2_y} $$</p> <p>% Simplifying the constant and assuming a sample size of $n_j$ for group j, i.e. to add log prob terms of all samples as they are i.i.d: $$ \\log p(y_j | \\alpha_j) = -\\frac{n_j}{2}\\log(2\\pi\\sigma^2_y) -\\frac{1}{2\\sigma^2_y}\\sum_{i=1}^{n_j}(y_{ji} - \\alpha_j)^2 $$</p> <p>% For the prior: $$ p(\\alpha_j) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_\\alpha}} \\exp\\left(-\\frac{(\\alpha_j - \\mu)^2}{2\\sigma^2_\\alpha}\\right) $$</p> <p>% The log prior is the logarithm of the above: $$ \\log p(\\alpha_j) = \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2_\\alpha}}\\right) -\\frac{(\\alpha_j - \\mu)^2}{2\\sigma^2_\\alpha} $$</p> <p>% Simplifying the constant: $$ \\log p(\\alpha_j) = -\\frac{1}{2}\\log(2\\pi\\sigma^2_\\alpha) -\\frac{1}{2\\sigma^2_\\alpha}(\\alpha_j - \\mu)^2 $$</p> <p>% Now, combining the log likelihood and the log prior for the log posterior: $$ \\log p(\\alpha_j | y_j) \\propto -\\frac{n_j}{2}\\log(2\\pi\\sigma^2_y) -\\frac{1}{2\\sigma^2_y}\\sum_{i=1}^{n_j}(y_{ji} - \\alpha_j)^2 -\\frac{1}{2}\\log(2\\pi\\sigma^2_\\alpha) -\\frac{1}{2\\sigma^2_\\alpha}(\\alpha_j - \\mu)^2 $$</p> <p>% Taking the derivative of the log posterior with respect to $\\alpha_j$, setting it to zero, and solving for $\\alpha_j$ will yield the MAP estimate.</p> <p>% Continuing from the log posterior, we have: $$ \\log p(\\alpha_j | y_j) \\propto -\\frac{1}{2\\sigma^2_y}\\sum_{i=1}^{n_j}(y_{ji} - \\alpha_j)^2 -\\frac{1}{2\\sigma^2_\\alpha}(\\alpha_j - \\mu)^2 $$</p> <p>% The derivative of the log posterior with respect to alpha_j is: $$ \\frac{d}{d\\alpha_j}\\log p(\\alpha_j | y_j) = \\frac{1}{\\sigma^2_y}\\sum_{i=1}^{n_j}(y_{ji} - \\alpha_j) - \\frac{1}{\\sigma^2_\\alpha}(\\alpha_j - \\mu) $$</p> <p>% Setting the derivative to zero for the MAP estimate: $$ \\frac{1}{\\sigma^2_y}\\sum_{i=1}^{n_j}(y_{ji} - \\alpha_j) - \\frac{1}{\\sigma^2_\\alpha}(\\alpha_j - \\mu) = 0 $$</p> <p>% Solving for alpha_j gives us: $$ \\frac{1}{\\sigma^2_y}\\sum_{i=1}^{n_j}y_{ji} - \\frac{n_j}{\\sigma^2_y}\\alpha_j - \\frac{1}{\\sigma^2_\\alpha}\\alpha_j + \\frac{\\mu}{\\sigma^2_\\alpha} = 0 $$</p> <p>% Collect terms involving alpha_j: $$ \\left(\\frac{n_j}{\\sigma^2_y} + \\frac{1}{\\sigma^2_\\alpha}\\right)\\alpha_j = \\frac{1}{\\sigma^2_y}\\sum_{i=1}^{n_j}y_{ji} + \\frac{\\mu}{\\sigma^2_\\alpha} $$</p> <p>% Since $\\sum_{i=1}^{n_j}y_{ji}$ is just $n_j$ times the sample mean $\\bar{y}_j$: $$ \\left(\\frac{n_j}{\\sigma^2_y} + \\frac{1}{\\sigma^2_\\alpha}\\right)\\alpha_j = \\frac{n_j}{\\sigma^2_y}\\bar{y}_j + \\frac{\\mu}{\\sigma^2_\\alpha} $$</p> <p>% Divide through by the coefficient of $\\alpha_j$ to solve for it: $$ \\alpha_j = \\frac{\\frac{n_j}{\\sigma^2_y}\\bar{y}_j + \\frac{\\mu}{\\sigma^2_\\alpha}}{\\frac{n_j}{\\sigma^2_y} + \\frac{1}{\\sigma^2_\\alpha}} $$</p> <p>% If we let $\\mu = \\bar{y}_{\\text{all}}$, we get the original equation provided: $$ \\hat{\\alpha}_{j}^{\\text{partial-pooled}} \\approx \\frac{\\frac{n_j}{\\sigma^2_y}\\bar{y}_j + \\frac{1}{\\sigma^2_\\alpha}\\bar{y}_{\\text{all}}}{\\frac{n_j}{\\sigma^2_y} + \\frac{1}{\\sigma^2_\\alpha}} $$</p> <pre><code>&lt;/p&gt;</code></pre> </p> In\u00a0[8]: Copied! <pre># Lets code it up in PyMC:\n\nwith pm.Model() as partially_pooled_model:\n    # Global parameters\n    global_mean = pm.Normal(\"global_mean\", 50, sigma=10)\n    global_sigma = pm.Exponential(\"global_sigma\", 1)\n\n    # School specific information\n    school_means = pm.Normal(\"school_means\", global_mean, global_sigma, shape=num_of_schools)\n\n    school_idx = data['School ID'].values\n\n    # Model error\n    sigma_y = pm.Exponential(\"sigma_y\", 1)\n\n    # Likelihood\n    y = pm.Normal(\"y\", school_means[school_idx], sigma_y, observed=data['score'])\n\n# Plot the graphical structure of the partial pooled model\npm.model_to_graphviz(partially_pooled_model)\n</pre> # Lets code it up in PyMC:  with pm.Model() as partially_pooled_model:     # Global parameters     global_mean = pm.Normal(\"global_mean\", 50, sigma=10)     global_sigma = pm.Exponential(\"global_sigma\", 1)      # School specific information     school_means = pm.Normal(\"school_means\", global_mean, global_sigma, shape=num_of_schools)      school_idx = data['School ID'].values      # Model error     sigma_y = pm.Exponential(\"sigma_y\", 1)      # Likelihood     y = pm.Normal(\"y\", school_means[school_idx], sigma_y, observed=data['score'])  # Plot the graphical structure of the partial pooled model pm.model_to_graphviz(partially_pooled_model) Out[8]: In\u00a0[9]: Copied! <pre>with partially_pooled_model:\n    partially_pooled_trace = pm.sample(random_seed=RANDOM_SEED)\n</pre> with partially_pooled_model:     partially_pooled_trace = pm.sample(random_seed=RANDOM_SEED) <pre>Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [global_mean, global_sigma, school_means, sigma_y]\n</pre>        100.00% [8000/8000 00:09&lt;00:00 Sampling 4 chains, 0 divergences]      <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 75 seconds.\n</pre> In\u00a0[10]: Copied! <pre>num_observations = data_samples_per_school\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\nfor ax, trace, level in zip(\n    axes,\n    (unpooled_trace, partially_pooled_trace),\n    (\"no pooling\", \"partial pooling\"),\n):\n    trace.posterior = trace.posterior.assign_coords({\"num_observations\": (\"school_means_dim_0\", num_observations)})\n\n    # plot means\n    trace.posterior.mean(dim=(\"chain\", \"draw\")).plot.scatter(\n        x=\"num_observations\", y=\"school_means\", ax=ax, alpha=0.9, label=\"Group's mean estimate\"\n    )\n    ax.hlines(\n        partially_pooled_trace.posterior.school_means.mean(),\n        min(num_observations) - 1,\n        max(num_observations) + 1,\n        alpha=0.4,\n        ls=\"--\",\n        label=\"Est. population mean\",\n    )\n\n    # plot hdi\n    hdi = az.hdi(trace).school_means\n    ax.vlines(num_observations, hdi.sel(hdi=\"lower\"), hdi.sel(hdi=\"higher\"), color=\"orange\", alpha=0.5, label=\"Group's HDI interval\")\n\n    ax.set(\n        title=f\"{level.title()} Estimates\",\n        xlabel=\"Number of observations\",\n        ylabel=\"Group Mean Estimates\",\n    )\n    ax.legend(fontsize=10, loc='lower right')\n</pre> num_observations = data_samples_per_school fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True) for ax, trace, level in zip(     axes,     (unpooled_trace, partially_pooled_trace),     (\"no pooling\", \"partial pooling\"), ):     trace.posterior = trace.posterior.assign_coords({\"num_observations\": (\"school_means_dim_0\", num_observations)})      # plot means     trace.posterior.mean(dim=(\"chain\", \"draw\")).plot.scatter(         x=\"num_observations\", y=\"school_means\", ax=ax, alpha=0.9, label=\"Group's mean estimate\"     )     ax.hlines(         partially_pooled_trace.posterior.school_means.mean(),         min(num_observations) - 1,         max(num_observations) + 1,         alpha=0.4,         ls=\"--\",         label=\"Est. population mean\",     )      # plot hdi     hdi = az.hdi(trace).school_means     ax.vlines(num_observations, hdi.sel(hdi=\"lower\"), hdi.sel(hdi=\"higher\"), color=\"orange\", alpha=0.5, label=\"Group's HDI interval\")      ax.set(         title=f\"{level.title()} Estimates\",         xlabel=\"Number of observations\",         ylabel=\"Group Mean Estimates\",     )     ax.legend(fontsize=10, loc='lower right') <p>Notice the difference between the unpooled and partially-pooled estimates, particularly at smaller sample sizes: As expected, the former are both more extreme and more imprecise. In the partially-pooled model, estimates in small-sample-size counties are informed by the population parameters \u2013 hence more precise estimates. Moreover, the smaller the sample size, the more regression towards the overall mean (the dashed gray line) \u2013 hence less extreme estimates. In other words, the model is skeptical of extreme deviations from the population mean in counties where data is sparse.</p>"},{"location":"bayesian/partial_pooling_demonstration/#a-gentle-introduction-to-partial-pooled-models","title":"A Gentle Introduction to Partial Pooled models\u00b6","text":"<p>In this blog, we'll learn about the partial pooled models and where they are useful. In addition, we'll use PyMC framework to implement a simple partial pooled model.</p>"},{"location":"bayesian/partial_pooling_demonstration/#a-practical-example","title":"A practical example\u00b6","text":"<p>Lets take a practical example to understand partial pooled models and related variants.</p> <p>Imagine you are a data scientist tasked with analyzing the academic performance of students across various schools in a city. Each school caters to a unique demographic, with varying resources and teaching methods. Your goal is to estimate the average exam scores of students in different schools. However, you face a challenge \u2013 some schools have a large number of students with comprehensive data, while other schools are smaller and have limited data available.</p> <p>Now lets create some representative data for this task. I'll refer school as a group in some contexts below.</p>"},{"location":"bayesian/partial_pooling_demonstration/#unpooled-model","title":"Unpooled Model\u00b6","text":"<p>One way is to treat each school as an entirely independent entity. A separate model is fitted for each school, estimating the average exam scores within that specific school. This approach is known as the \"no-pooling\" or \"unpooling\" approach, as it refrains from sharing information across schools.</p> <p>Such a model can be visually represented as:</p> <p></p>"},{"location":"bayesian/partial_pooling_demonstration/#partial-pooled-model","title":"Partial pooled model\u00b6","text":"<p>Other approach to tackle the problem is to define a shared global model to capture the overall patterns in the entire dataset. Each school's model is then allowed to deviate from the global model based on its own data. The extent of this deviation is determined by the amount of data available for each school. This approach is known as the \"partial pooling\" approach.</p> <p>Such a model can be visually represented as:</p> <p></p>"},{"location":"bayesian/partial_pooling_demonstration/#comparison","title":"Comparison\u00b6","text":"<p>Lets compare the average estimates of student's performance in different schools computed in both approaches:</p>"},{"location":"bayesian/partial_pooling_demonstration/#key-takeaways","title":"Key takeaways\u00b6","text":"<ul> <li>The \"partial pooling\" approach seeks a balance between treating each group entirely independently and pooling all data together. It introduces a hierarchical structure to the modeling, allowing information sharing between groups.</li> <li>This approach is particularly useful when dealing with groups of different sizes, as it allows smaller groups to borrow strength from the overall trends observed in the larger dataset.</li> </ul>"},{"location":"bayesian/partial_pooling_demonstration/#references","title":"References\u00b6","text":"<ul> <li>Used ChatGPT for generating data, and equations.</li> <li>Multilevel modelling guide in PyMC</li> <li>Andrew Gelman and Jennifer Hill. Data analysis using regression and multilevel/hierarchical models. Cambridge university press, 2006.</li> </ul>"},{"location":"django/deploy-django/","title":"Deploy Django","text":"<p>In this blog, we will learn how to deploy Django Application with uWSGI and nginx on CentOS\u20197. Django is the most popular python based backend framework with the aim of rapid web development. Before installing anything, I recommend to read my previous post where I have discussed about initial server setup with ssh keys.</p>"},{"location":"django/deploy-django/#installing-the-prerequisites","title":"Installing the prerequisites","text":""},{"location":"django/deploy-django/#installing-python36","title":"Installing python3.6","text":"<p>First install the latest packages from EPEL and RPM. EPEL(Extra Packages for Enterprise Linux) is an open source repository that contains the latest packages for Red Hat Linux distributions. RPM is also an open source package management system from Red Hat. After all this, lets install python3 \u2013</p> <pre><code>sudo yum install -y epel-release\nsudo yum install -y https://centos7.iuscommunity.org/ius-release.rpm\nsudo yum update\nsudo yum install -y python36u python36u-libs python36u-devel python36u-pip python-devel\n</code></pre>"},{"location":"django/deploy-django/#upgrading-pip-and-installing-virtualenv","title":"Upgrading pip and installing virtualenv","text":"<p>Pip is the most popular python package installer. Virtual environments are used for separating the different versions of any package for different projects.</p> <pre><code>sudo pip3.6 install --upgrade pip\nsudo pip install virtualenv virtualenvwrapper\n</code></pre>"},{"location":"django/deploy-django/#configuring-the-shell","title":"Configuring the shell","text":"<p>We will use Env directory to hold all our virtual environments. This can be configured in <code>.bashrc</code> file.</p> <pre><code>echo \"export WORKON_HOME=~/Env\" &gt;&gt; ~/.bashrc\necho \"source /usr/bin/virtualenvwrapper.sh\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Now, open <code>/usr/bin/virtualenvwrapper.sh</code> with either vim or nano. Find the line \u2013</p> <pre><code>VIRTUALENVWRAPPER_PYTHON=\"$(command \\which python)\"\n</code></pre> <p>and replace python with python3.6 as \u2013</p> <pre><code>VIRTUALENVWRAPPER_PYTHON=\"$(command \\which python3.6)\"\n</code></pre> <p>Now, lets reflect these changes \u2013</p> <pre><code>source ~/.bashrc\n</code></pre>"},{"location":"django/deploy-django/#configuring-django-project","title":"Configuring Django project","text":""},{"location":"django/deploy-django/#creating-virtual-environments","title":"Creating virtual environments","text":"<pre><code>mkvirtualenv env_1\n</code></pre> <p>The environment env_1 gets automatically activated. The same can be verified by \u2013</p> <pre><code>which pip\n# Output: ~/Env/env_1/bin/pip\n</code></pre>"},{"location":"django/deploy-django/#copying-django-project-from-local-to-remote","title":"Copying Django project from local to remote","text":"<p>Since, we have activated our virtual env, now we will copy our django project to remote server using scp. If you have uploaded it on github, just install git via yum and then git clone the project. But before doing this, lets grab all the requirements of the project. If your project already contains a requirements file, then you can skip this part. \u201ccd\u201d into your project\u2019s directory and after activating virtual env in your local machine, use the following command to list out requirements in requirements.txt file in your local terminal-</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>Now, to copy the project, use the following command in your local terminal. Do remember to put your ip and user_name configured in previous post. Write the complete path of your django project from root in local machine. This will copy the project in the home directory of the server.</p> <pre><code>scp -r /path/to/project user_name@your_ip_here:~\n</code></pre> <p>Now, connect to your server and activate the virtual env.</p> <pre><code>ssh user_name@your_ip_here\nworkon env_1\n</code></pre> <p>Let\u2019s install all the requirements for the project. Use the path to requirements.txt file.</p> <pre><code>pip install -r /path/to/requirements.txt\n</code></pre>"},{"location":"django/deploy-django/#installing-mysql-from-rpm","title":"Installing MySQL from rpm","text":"<pre><code>sudo rpm -ivh https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm\n</code></pre> <p>Check if Mysql repo has been enabled \u2013</p> <pre><code>sudo yum repolist all | grep mysql | grep enabled\n# Output : enabled\n</code></pre> <p>Install and enable mysql \u2013</p> <pre><code>sudo yum -y install mysql-community-server\nsudo systemctl start mysqld\nsudo systemctl enable mysqld\nsudo systemctl status mysqld\n</code></pre> <p>Copy the mysql temporary root password from the command given below and paste this while secure installation of mysql. Change the root password and hit Enter for default actions.</p> <pre><code>cat /var/log/mysqld.log | grep -i 'temporary password'\nmysql_secure_installation\n</code></pre> <p>We have successfully installed mysql and now, we need a database to run our project. First, open the mysql interface and enter the root password \u2013</p> <pre><code>mysql -u root -p\n</code></pre> <p>Now, in mysql, create a database \u2013</p> <pre><code>mysql&gt; CREATE DATABASE first_db;\nmysql&gt; SHOW DATABASES;\nmysql&gt; exit\n</code></pre> <p>After this, we need to install a client to communicate with mysql \u2013</p> <pre><code>sudo yum install -y mysql-connector-python.x86_64 mysql-community-devel.x86_64 mysql-cluster-community-client.x86_64 mysql-shell.x86_64 mysql-router.x86_64 gcc\npip install mysqlclient # inside the virtual environment\n</code></pre>"},{"location":"django/deploy-django/#changing-settingspy-file","title":"Changing settings.py file","text":"<p>With everything installed, let\u2019s change some settings for the project \u2013</p> <pre><code>sudo nano ~/project_name/project_name/settings.py\n</code></pre> <p>Add the following line to the last of the file. As we will be using nginx to deploy the application, this line tells django to place our static files in \u2018static\u2019 directory. This helps nginx to easily serve these static files.</p> <pre><code>STATIC_ROOT = os.path.join(BASE_DIR, \"static/\")\n</code></pre> <p>Do not forget to change the default database configurations to \u2013</p> <pre><code>DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.mysql',\n        'NAME': 'first_db',\n        'USER': 'root',\n        'PASSWORD': 'your-root-password',\n        'HOST': 'localhost',\n        'PORT': '',\n    }\n}\n</code></pre> <p>And also add your ip in the allowed hosts \u2013</p> <pre><code>ALLOWED_HOSTS = ['your_ip_here']\n</code></pre>"},{"location":"django/deploy-django/#opening-port-8000","title":"Opening port 8000","text":"<pre><code>sudo firewall-cmd --permanent --add-service=http\nsudo firewall-cmd --permanent --add-port=8000/tcp\nsudo firewall-cmd --complete-reload\nsudo firewall-cmd --list-all\n</code></pre>"},{"location":"django/deploy-django/#running-the-application","title":"Running the application","text":"<p>First, flush out the initial migrations and delete the sqlite database. \u201ccd\u201d into your project\u2019s directory and use the following commands \u2013</p> <pre><code>find . -path \"*/migrations/*.py\" -not -name \"__init__.py\" -delete\nfind . -path \"*/migrations/*.pyc\"  -delete\nrm -f db.sqlite3\n</code></pre> <p>Run the migrations to sync up with database \u2013</p> <pre><code>python manage.py collectstatic\npython manage.py makemigrations\npython manage.py migrate\n</code></pre> <p>So, finally we can run the server and see the application accessible globally \u2013</p> <pre><code>python manage.py runserver 0.0.0.0:8000\n</code></pre> <p>Go to the web browser and enter your_ip:8000 to access the django application.</p>"},{"location":"django/deploy-django/#setting-up-uwsgi-and-nginx","title":"Setting up uWSGI and nginx","text":""},{"location":"django/deploy-django/#configuring-uwsgi-globally","title":"Configuring uWSGI globally","text":"<p>Store all the configuration files to /etc/uwsgi/sites. You should use your project name for all configurations \u2013</p> <pre><code>sudo pip install uwsgi\nsudo mkdir -p /etc/uwsgi/sites\ncd /etc/uwsgi/sites\nsudo nano project_name.ini\n</code></pre> <p>Add the following lines to the .ini file. Do remember to use your project and user name.</p> <pre><code>[uwsgi]\nproject = project_name\nusername = user_name\nbase = /home/%(username)\n\nchdir = %(base)/%(project)\nhome = %(base)/Env/env_1\nmodule = %(project).wsgi:application\n\nmaster = true\nprocesses = 5\n\nuid = %(username)\nsocket = /run/uwsgi/%(project).sock\nchown-socket = %(username):nginx\nchmod-socket = 660\nvacuum = true\n</code></pre> <p>Ctrl+x to exit and press y to save the changes. Base and home contain the full path for the home directory and virtual environment respectively. We have created a master process for loading our app server. Here, we have used Unix Socket. This socket uses uWSGI protocol which helps nginx to reverse proxy.</p> <pre><code>sudo nano /etc/systemd/system/uwsgi.service\n</code></pre> <p>Add the following lines. Do remember to use your user name in ExecStartPre of Service section.</p> <pre><code>[Unit]\nDescription=uWSGI Emperor service\n\n[Service]\nExecStartPre=/usr/bin/bash -c 'mkdir -p /run/uwsgi; chown user_name:nginx /run/uwsgi'\nExecStart=/usr/bin/uwsgi --emperor /etc/uwsgi/sites\nRestart=always\nKillSignal=SIGQUIT\nType=notify\nNotifyAccess=all\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>[Unit] section describes our service. [Service] section manages various applications. [Install] section ties up multi-user system state.</p>"},{"location":"django/deploy-django/#configuring-nginx","title":"Configuring Nginx","text":"<p>Installing Nginx -</p> <pre><code>sudo yum -y install nginx\nsudo nano /etc/nginx/nginx.conf\n</code></pre> <p>Add the following lines. Do remember to use your user name and project name in root and uwsgi_pass.</p> <pre><code>server {\n    listen 8000;\n    server_name localhost;\n\n    location = favicon.ico { access_log off; log_not_found off; }\n    location /static/ {\n        root /home/user_name/project_name;\n    }\n\n    location / {\n        include uwsgi_params;\n        uwsgi_pass unix:/run/uwsgi/project_name.sock;\n    }\n}\n</code></pre> <p>Above, we have set up a server block, with an open port to listen from. We have also specified the static file location and passed all the traffic to unix socket. Make sure the syntax of nginx file is correct and change permissions of the user.</p> <pre><code>sudo nginx -t\nsudo usermod -a -G user_name nginx\nchmod 710 /home/user_name\n</code></pre> <p>Start and enable the nginx and uwsgi.</p> <pre><code>sudo systemctl start nginx\nsudo systemctl start uwsgi\nsudo systemctl enable nginx\nsudo systemctl enable uwsgi\n</code></pre> <p>Now you can directly access the django application from the ip with an open port. Thanks for reading!</p>"},{"location":"django/initial-server-setup/","title":"Initial Server Setup","text":"<p>In this blog, we will be going through initial server setup on Centos7. This is generally recommended before diving straight into production. Centos is an open source Linux distribution under RHEL(Red Hat Enterprise Linux). The reason why Centos is preferred over Ubuntu is because of its stability. Its updates can take about more than 7-8 years to come.</p>"},{"location":"django/initial-server-setup/#configuring-centos-7","title":"Configuring Centos 7","text":"<p>After purchasing a server, login into it using the ssh command in the terminal. Remember to use your public ip and the password given by the administrator.</p> <pre><code>ssh root@your_ip_here\n</code></pre> <p>If you are getting a warning like \u201cLC_CTYPE: cannot change locale (UTF-8): No such file or directory\u201d, then enter the following command in your local terminal by logging out. The LC variables determine the language of encoding the characters. So, we need to export this variable.</p> <pre><code>logout\nexport LC_ALL=\"en_US.UTF-8\"\nssh root@your_ip_here\n</code></pre>"},{"location":"django/initial-server-setup/#updating-centos","title":"Updating centos","text":"<pre><code>sudo yum update\nsudo yum upgrade\n</code></pre>"},{"location":"django/initial-server-setup/#setting-up-hostname","title":"Setting up hostname","text":"<pre><code>hostnamectl set-hostname centos-server\nhostname # to check the hostname\n</code></pre>"},{"location":"django/initial-server-setup/#setting-up-hostname-in-the-hosts-file","title":"Setting up hostname in the hosts file","text":"<pre><code>sudo nano /etc/hosts\n</code></pre> <p>Add your ip followed by tab and then type centos-server which is the hostname. Then hit CTRL+x to exit and enter to save the changes.</p>"},{"location":"django/initial-server-setup/#adding-a-new-user","title":"Adding a new user","text":"<p>Root has the most privileges in the OS. It can be destructive to operate the server under root user. To limit the scope, we will be creating a new user. In future, if any need arises, we will change the permissions for this user.</p> <pre><code>adduser user_name\npasswd user_name #setting up password for new user\ngpasswd -a user_name wheel #adding sudo privileges\nlogout\n</code></pre>"},{"location":"django/initial-server-setup/#securing-the-server","title":"Securing the server","text":"<p>There are bots all around trying to find vulnerabilities in the servers. Till now, we have used password based authentication which is highly exploitable. These bots try brute force attacks to enter our server. So to fix this, we will disable password based authentication and setup ssh keys. These ssh keys will be stored in the local machine and the server. After this whenever we try to ssh into our server, it will analyse the keys and give access. So, setting up ssh keys for authentication in the local machine in home directory. Hit enter for default actions \u2013</p> <pre><code>ssh-keygen -t rsa #generating keys\nssh-copy-id user_name@your_ip_here #coping the keys to the server\nlogout\n</code></pre> <p>Now you can ssh login without password for the new user \u2013</p> <pre><code>ssh user_name@your_ip_here\n</code></pre> <p>Change configuration in <code>/etc/ssh/sshd_config</code>, thereby making our server more secure \u2013</p> <pre><code>PermitRootLogin no\nPasswordAuthentication no\n</code></pre> <pre><code>sudo nano /etc/ssh/sshd_config\nsudo systemctl restart sshd\n</code></pre> <p>Up next \u2013 Deploy Django Applications on Centos Thanks for reading!</p>"},{"location":"graph-theory/graph-theory-101/","title":"Graph Theory 101","text":""},{"location":"graph-theory/graph-theory-101/#what-is-graph-theory","title":"What is Graph Theory?","text":"<p>Graph Theory is an important branch of mathematics. Graphs are mathematical structures which is used for modelling pairwise relationships between objects. In graph theory terms, these objects are called vertices and the relationship two objects is given by edges between them. Graphs are primarily of two types \u2013 Directed Graphs or Undirected Graphs. In directed graphs, edges are directed from one vertex to another. While in undirected graphs, edges connect two vertex so that relation is bidirectional.</p> <p></p> <p>Directed Graph</p> <p></p> <p>Undirected Graph</p> <p>In above images, circles represent objects while the lines between them represent edges. Edges in directed graph are denoted by arrows while in undirected graph they are simply drawn as lines. In first image, edge \u201ca\u201d relates object \u201c1\u201d to object \u201c2\u201d but not vice versa. While in second image there is no direction associated with edges.</p> <p>Traversing a graph means moving from one vertex to another using the edges. You can go from one vertex to another only if there is an edge between them. In directed graph we can move only in one direction using an edge. But in undirected graph we can move in both directions.</p>"},{"location":"graph-theory/graph-theory-101/#an-interesting-story","title":"An interesting story","text":"<p>The city of K\u00f6nigsberg in Prussia (now Kaliningrad, Russia) was situated on both sides of the Pregel River, and included two large islands \u2013 Kneiphof and Lomse \u2013 which were connected to each other, or to the two mainland portions of the city, by seven bridges. When Carl Gottileb Ehler \u2013 a mathematician became mayor of the nearby town, he kept thinking about a single problem: Is there any route that allows you to cross all the seven bridges without crossing any of them more than once?</p> <p>Here, is the picture of K\u00f6nigsberg. The green shapes denote the bridges.</p> <p></p> <p>If you are trying to solve the problem then you are out of luck because such a route does not exist. Next we will see how Euler proved that this problem is impossible to solve.</p>"},{"location":"graph-theory/graph-theory-101/#graph-theory-is-discovered","title":"Graph Theory is discovered","text":"<p>While trying to explain why this problem is impossible to solve, Euler invented a field of mathematics which he called Geometry of Positions. Now, know as Graph Theory. But for understanding his solution we would need to reformulate the question in terms of graph theory. Let\u2019s do that first.</p> <p>We can mark the four lands as our objects \u2013 A, B, C, D and the bridges connecting them as edges. Since we can use bridges to move in either direction, we use undirected graph for this representation.</p> <p></p> <p>Drawing in more familiar way, here is the resulting graph.</p> <p></p> <p>So, we can reformulate our problem as follows \u2013</p> <p>Is there any way for traversing the graph such that we use each and every edge exactly once?</p>"},{"location":"graph-theory/graph-theory-101/#indegree-and-outdegree","title":"Indegree and Outdegree","text":"<p>By definition, Indegree of a vertex is the number of edges that are coming towards that vertex and similarly, Outdegree is number of edges going out of a vertex. In case of Undirected Graphs, we do not distinguish between indegree and outdegree and simply refer to number of edges connected to a vertex as its Degree.</p> <p>So, in our graph of K\u00f6nigsberg Bridges, degree of A is 5 while that of B, C and D is 3.</p>"},{"location":"graph-theory/graph-theory-101/#finally-the-solution","title":"Finally the solution","text":"<p>We can argue that for such a traversal to exist degree of all the vertices except for maybe 2 vertices must be even. That is excluding any two vertices, all the other vertices must have a even degree for the solution to exist.</p>"},{"location":"graph-theory/graph-theory-101/#why","title":"Why?","text":"<p>Because if we enter a vertex then we must also leave it but we cannot use the same edge that we used to enter it, so we must use a different edge. By this argument we can see that for every vertex we visit, there are two edges that we use. The vertex from which we start traversing and the vertex where we end it are exceptions. Because we start traversing from one vertex without entering it, and we can end our traversal in another vertex without leaving it. Therefore this two vertex need not have an even degree.</p> <p>In our K\u00f6nigsberg Bridges graph, all the vertices have odd degree, so we cannot traverse the graph in a way that we cross every edge exactly once.</p> <p>In Euler\u2019s honour, we today call such a traversal of graph as Eulerian path. A different but related problem adds extra condition to this traversal that the starting and ending vertex must be the same. This traversal is called a Eulerian circuit. For Eulerian circuit to exist in the graph, all the vertices must have even degree. (Because this time we must enter back to the vertex from where we started the traversal).</p>"},{"location":"graph-theory/graph-theory-101/#conclusion","title":"Conclusion","text":"<p>Graph Theory is a very useful branch of mathematics and has numerous applications in different fields of science. Especially, in computer science graph theory is used extensively in many places. Social Networks use it to represent connections among users. In maps, it is used to represent cities as vertices and roads between them as edges. Also standard graph algorithms like DFS, BFS, Topological Sort are used in various programming tasks.</p>"},{"location":"graph-theory/graph-theory-101/#references","title":"References","text":"<ul> <li>Seven Bridges of K\u00f6nigsberg \u2013 Wikipedia</li> <li>Graph Theory \u2013 Wikipedia</li> <li>How the K\u00f6nigsberg bridge problem changed mathematics \u2013 Dan Van der Vieren</li> <li>Mathematics | Graph Theory Basics \u2013 Set 1 \u2013 GeeksforGeeks</li> </ul> <p>This article has been written by my dear friend Ram Nad.</p>"},{"location":"gsoc/final-week-into-gsoc/","title":"Final Week into GSoC","text":"<p>Hello all. So, this blog is meant to summarize the work done in the last week of GSoC. I will write a separate post covering the entire work done in three months. Stay tuned.</p> <p>I have been working on adding features to ADVI. Here is the PR #310.</p> <ul> <li> Add progress bar support.</li> <li> Test progress bar in different OS.</li> <li> Add ParameterConvergence criteria to test convergence.</li> <li> Add LowRank Approximation.</li> <li> Add LowRank ADVI tests.</li> <li> Update quickstart notebook.</li> </ul> <p>The only thing left is including a sample notebook playing around with hierarchical models and VI. I have already written half of it. Hope this PR will get merged soon.</p> <p>See you soon.</p> <p>Thanks Sayam Kumar</p>"},{"location":"gsoc/gsoc-2020-with-numfocus/","title":"GSoC'20 with NumFOCUS","text":"<p>I am super excited to say that I have been selected as a Google Summer of Code student by NumFOCUS for PyMC4. I would like to thank my mentors Thomas Wiecki and Maxim Kochurov and the entire NumFOCUS community for giving this opportunity.</p> <p>My project is about adding <code>Variational Inference Interface</code> to PyMC4. Variational Inference scales better over larger datasets as compared to the traditional MCMC algorithms. First, I had plans to implement OPVI<sup>1</sup> framework as done in PyMC3 this summer. But as corrected by my mentor Maxim Kochurov, it would have taken extra time and more debugging because of the difficulty to deal with <code>symbolic graph manipulations</code> in Tensorflow. Now, the whole plan is to implement two Variational Inference Algorithms - Mean Field ADVI<sup>2</sup> and Full Rank ADVI<sup>2</sup> in PyMC4. Mean Field ADVI posits a Spherical Gaussian family and Full Rank ADVI posits a Multivariate Gaussian family to minimize KL divergence. I will write a blog post upto next week explaining both these algorithms.</p> <p>All in all, I look forward to a great summer.</p> <ol> <li> <p>Operator Variational Inference Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei (2016)\u00a0\u21a9</p> </li> <li> <p>Automatic Differentiation Variational Inference Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M. Blei (2016).\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"gsoc/variational-inference/","title":"Variational Inference","text":"In\u00a0[1]: Copied! <pre># Imports\n%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport tensorflow as tf\nfrom scipy.stats import expon, uniform\nimport arviz as az\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport tensorflow_probability as tfp\nfrom pprint import pprint\n\nplt.style.use(\"arviz-darkgrid\")\n\nfrom tensorflow_probability.python.mcmc.transformed_kernel import (\n    make_transform_fn, make_transformed_log_prob)\n\ntfb = tfp.bijectors\ntfd = tfp.distributions\ndtype = tf.float32\n</pre> # Imports %matplotlib inline import numpy as np import scipy as sp import pandas as pd import tensorflow as tf from scipy.stats import expon, uniform import arviz as az import pymc3 as pm import matplotlib.pyplot as plt import tensorflow_probability as tfp from pprint import pprint  plt.style.use(\"arviz-darkgrid\")  from tensorflow_probability.python.mcmc.transformed_kernel import (     make_transform_fn, make_transformed_log_prob)  tfb = tfp.bijectors tfd = tfp.distributions dtype = tf.float32 In\u00a0[2]: Copied! <pre># Plot functions\ndef plot_transformation(theta, zeta, p_theta, p_zeta):\n    fig, (const, trans) = plt.subplots(nrows=2, ncols=1, figsize=(6.5, 12))\n    const.plot(theta, p_theta, color='blue', lw=2)\n    const.set_xlabel(r\"$\\theta$\")\n    const.set_ylabel(r\"$P(\\theta)$\")\n    const.set_title(\"Constrained Space\")\n\n    trans.plot(zeta, p_zeta, color='blue', lw=2)\n    trans.set_xlabel(r\"$\\zeta$\")\n    trans.set_ylabel(r\"$P(\\zeta)$\")\n    trans.set_title(\"Transfomed Space\");\n</pre> # Plot functions def plot_transformation(theta, zeta, p_theta, p_zeta):     fig, (const, trans) = plt.subplots(nrows=2, ncols=1, figsize=(6.5, 12))     const.plot(theta, p_theta, color='blue', lw=2)     const.set_xlabel(r\"$\\theta$\")     const.set_ylabel(r\"$P(\\theta)$\")     const.set_title(\"Constrained Space\")      trans.plot(zeta, p_zeta, color='blue', lw=2)     trans.set_xlabel(r\"$\\zeta$\")     trans.set_ylabel(r\"$P(\\zeta)$\")     trans.set_title(\"Transfomed Space\");  In\u00a0[3]: Copied! <pre>theta = np.linspace(0, 5, 100)\nzeta = np.linspace(-5, 5, 100)\n\ndist = expon()\np_theta = dist.pdf(theta)\np_zeta = dist.pdf(np.exp(zeta)) * np.exp(zeta)\n\nplot_transformation(theta, zeta, p_theta, p_zeta)\n</pre> theta = np.linspace(0, 5, 100) zeta = np.linspace(-5, 5, 100)  dist = expon() p_theta = dist.pdf(theta) p_zeta = dist.pdf(np.exp(zeta)) * np.exp(zeta)  plot_transformation(theta, zeta, p_theta, p_zeta) In\u00a0[4]: Copied! <pre>theta = np.linspace(0, 1, 100)\nzeta = np.linspace(-5, 5, 100)\n\ndist = uniform()\np_theta = dist.pdf(theta)\nsigmoid = sp.special.expit\np_zeta = dist.pdf(sigmoid(zeta)) * sigmoid(zeta) * (1-sigmoid(zeta))\n\nplot_transformation(theta, zeta, p_theta, p_zeta)\n</pre> theta = np.linspace(0, 1, 100) zeta = np.linspace(-5, 5, 100)  dist = uniform() p_theta = dist.pdf(theta) sigmoid = sp.special.expit p_zeta = dist.pdf(sigmoid(zeta)) * sigmoid(zeta) * (1-sigmoid(zeta))  plot_transformation(theta, zeta, p_theta, p_zeta) In\u00a0[5]: Copied! <pre># Generating data\nmu = 12\nsigma = 2.2\ndata = np.random.normal(mu, sigma, size=200)\n</pre> # Generating data mu = 12 sigma = 2.2 data = np.random.normal(mu, sigma, size=200) In\u00a0[6]: Copied! <pre># Defining the model\nmodel = tfd.JointDistributionSequential([\n    # sigma_prior\n    tfd.Exponential(1, name='sigma'),\n\n    # mu_prior\n    tfd.Normal(loc=0, scale=10, name='mu'),\n\n    # likelihood\n    lambda mu, sigma: tfd.Normal(loc=mu, scale=sigma)\n])\n</pre> # Defining the model model = tfd.JointDistributionSequential([     # sigma_prior     tfd.Exponential(1, name='sigma'),      # mu_prior     tfd.Normal(loc=0, scale=10, name='mu'),      # likelihood     lambda mu, sigma: tfd.Normal(loc=mu, scale=sigma) ]) In\u00a0[7]: Copied! <pre>print(model.resolve_graph())\n</pre> print(model.resolve_graph()) <pre>(('sigma', ()), ('mu', ()), ('x', ('mu', 'sigma')))\n</pre> In\u00a0[8]: Copied! <pre># Let's generate joint log probability\njoint_log_prob = lambda *x: model.log_prob(x + (data,))\n</pre> # Let's generate joint log probability joint_log_prob = lambda *x: model.log_prob(x + (data,)) In\u00a0[9]: Copied! <pre># Build Mean Field ADVI\ndef build_mf_advi():\n    parameters = model.sample(1)\n    parameters.pop()\n    dists = []\n    for i, parameter in enumerate(parameters):\n        shape = parameter[0].shape\n        loc = tf.Variable(\n            tf.random.normal(shape, dtype=dtype),\n            name=f'meanfield_{i}_loc',\n            dtype=dtype\n        )\n        scale = tfp.util.TransformedVariable(\n            tf.fill(shape, value=tf.constant(0.02, dtype=dtype)),\n            tfb.Softplus(), # For positive values of scale\n            name=f'meanfield_{i}_scale'\n        )\n\n        approx_parameter = tfd.Normal(loc=loc, scale=scale)\n        dists.append(approx_parameter)\n    return tfd.JointDistributionSequential(dists)\n\nmeanfield_advi = build_mf_advi()\n</pre> # Build Mean Field ADVI def build_mf_advi():     parameters = model.sample(1)     parameters.pop()     dists = []     for i, parameter in enumerate(parameters):         shape = parameter[0].shape         loc = tf.Variable(             tf.random.normal(shape, dtype=dtype),             name=f'meanfield_{i}_loc',             dtype=dtype         )         scale = tfp.util.TransformedVariable(             tf.fill(shape, value=tf.constant(0.02, dtype=dtype)),             tfb.Softplus(), # For positive values of scale             name=f'meanfield_{i}_scale'         )          approx_parameter = tfd.Normal(loc=loc, scale=scale)         dists.append(approx_parameter)     return tfd.JointDistributionSequential(dists)  meanfield_advi = build_mf_advi() <p>TFP handles transformations differently as it transforms unconstrained space to match the support of distributions.</p> In\u00a0[10]: Copied! <pre>unconstraining_bijectors = [\n  tfb.Exp(),\n  tfb.Identity()\n]\n\nposterior = make_transformed_log_prob(\n    joint_log_prob,\n    unconstraining_bijectors,\n    direction='forward',\n    enable_bijector_caching=False\n)\n</pre> unconstraining_bijectors = [   tfb.Exp(),   tfb.Identity() ]  posterior = make_transformed_log_prob(     joint_log_prob,     unconstraining_bijectors,     direction='forward',     enable_bijector_caching=False ) In\u00a0[11]: Copied! <pre>opt = tf.optimizers.Adam(learning_rate=.1)\n\n@tf.function(autograph=False)\ndef run_approximation():\n    elbo_loss = tfp.vi.fit_surrogate_posterior(\n        posterior,\n        surrogate_posterior=meanfield_advi,\n        optimizer=opt,\n        sample_size=200,\n        num_steps=10000)\n    return elbo_loss\n\nelbo_loss = run_approximation()\n</pre> opt = tf.optimizers.Adam(learning_rate=.1)  @tf.function(autograph=False) def run_approximation():     elbo_loss = tfp.vi.fit_surrogate_posterior(         posterior,         surrogate_posterior=meanfield_advi,         optimizer=opt,         sample_size=200,         num_steps=10000)     return elbo_loss  elbo_loss = run_approximation() <pre>WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow_probability/python/math/minimize.py:74: calling &lt;lambda&gt; (from tensorflow_probability.python.vi.optimization) with loss is deprecated and will be removed after 2020-07-01.\nInstructions for updating:\nThe signature for `trace_fn`s passed to `minimize` has changed. Trace functions now take a single `traceable_quantities` argument, which is a `tfp.math.MinimizeTraceableQuantities` namedtuple containing `traceable_quantities.loss`, `traceable_quantities.gradients`, etc. Please update your `trace_fn` definition.\n</pre> In\u00a0[12]: Copied! <pre>plt.plot(elbo_loss, color='blue')\nplt.yscale(\"log\")\nplt.xlabel(\"No of iterations\")\nplt.ylabel(\"Negative ELBO\")\nplt.show()\n</pre> plt.plot(elbo_loss, color='blue') plt.yscale(\"log\") plt.xlabel(\"No of iterations\") plt.ylabel(\"Negative ELBO\") plt.show() In\u00a0[13]: Copied! <pre>graph_info = model.resolve_graph()\napprox_param = dict()\nfree_param = meanfield_advi.trainable_variables\nfor i, (rvname, param) in enumerate(graph_info[:-1]):\n    approx_param[rvname] = {\"mu\": free_param[i*2].numpy(),\n                            \"sd\": free_param[i*2+1].numpy()}\n</pre> graph_info = model.resolve_graph() approx_param = dict() free_param = meanfield_advi.trainable_variables for i, (rvname, param) in enumerate(graph_info[:-1]):     approx_param[rvname] = {\"mu\": free_param[i*2].numpy(),                             \"sd\": free_param[i*2+1].numpy()} In\u00a0[14]: Copied! <pre>print(approx_param)\n</pre> print(approx_param) <pre>{'sigma': {'mu': 0.7740287, 'sd': -0.7494337}, 'mu': {'mu': 11.233825, 'sd': 1.7977774}}\n</pre> <p>We got pretty good estimates of sigma and mu. We need to transform sigma via exp and I believe it will be something close to 2.2</p>"},{"location":"gsoc/variational-inference/#variational-inference","title":"Variational Inference\u00b6","text":"<p>Variational Inference is a powerful algorithm for fitting Bayesian networks. In this blog, you will learn about maths and intuition behind Variational Inference, Mean Field approximation and its implementation in Tensorflow Probability.</p>"},{"location":"gsoc/variational-inference/#intro-to-bayesian-networks","title":"Intro to Bayesian Networks\u00b6","text":""},{"location":"gsoc/variational-inference/#random-variables","title":"Random Variables\u00b6","text":"<p>Random Variables are simply variables whose values are uncertain. Eg -</p> <ol> <li><p>In case of flipping a coin $n$ times, a random variable $X$ can be number of heads shown up.</p> </li> <li><p>In COVID-19 pandemic situation, random variable can be number of patients found positive with virus daily.</p> </li> </ol>"},{"location":"gsoc/variational-inference/#probability-distributions","title":"Probability Distributions\u00b6","text":"<p>Probability Distributions governs the amount of uncertainty of random variables. They have a math function with which they assign probabilities to different values taken by random variables. The associated math function is called probability density function (pdf). For simplicity, let's denote any random variable as $X$ and its corresponding pdf as $P\\left (X\\right )$. Eg - Following figure shows the probability distribution for number of heads when an unbiased coin is flipped 5 times. </p>"},{"location":"gsoc/variational-inference/#bayesian-networks","title":"Bayesian Networks\u00b6","text":"<p>Bayesian Networks are graph based representations to acccount for randomness while modelling our data. The nodes of the graph are random variables and the connections between nodes denote the direct influence from parent to child.</p>"},{"location":"gsoc/variational-inference/#bayesian-network-example","title":"Bayesian Network Example\u00b6","text":"<p>Let's say a student is taking a class during school. The <code>difficulty</code> of the class and the <code>intelligence</code> of the student together directly influence student's <code>grades</code>. And the <code>grades</code> affects his/her acceptance to the university. Also, the <code>intelligence</code> factor influences student's <code>SAT</code> score. Keep this example in mind.</p> <p>More formally, Bayesian Networks represent joint probability distribution over all the nodes of graph - $P\\left (X_1, X_2, X_3, ..., X_n\\right )$ or $P\\left (\\bigcap_{i=1}^{n}X_i\\right )$ where $X_i$ is a random variable. Also Bayesian Networks follow local Markov property by which every node in the graph is independent on its non-descendants given its parents. In this way, the joint probability distribution can be decomposed as -</p> <p>$$ P\\left (X_1, X_2, X_3, ..., X_n\\right ) = \\prod_{i=1}^{n} P\\left (X_i | Par\\left (X_i\\right )\\right ) $$</p> Extra: Proof of decomposition <p>First, let's recall conditional probability,     $$P\\left (A|B\\right ) = \\frac{P\\left (A, B\\right )}{P\\left (B\\right )}$$     The above equation is so derived because of reduction of sample space of $A$ when $B$ has already occured.     Now, adjusting terms -     $$P\\left (A, B\\right ) = P\\left (A|B\\right )*P\\left (B\\right )$$     This equation is called chain rule of probability. Let's generalize this rule for Bayesian Networks. The ordering of names of nodes is such that parent(s) of nodes lie above them (Breadth First Ordering).     $$P\\left (X_1, X_2, X_3, ..., X_n\\right ) = P\\left (X_n, X_{n-1}, X_{n-2}, ..., X_1\\right )\\\\     = P\\left (X_n|X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) * P \\left (X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) \\left (Chain Rule\\right )\\\\       = P\\left (X_n|X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) * P \\left (X_{n-1}|X_{n-2}, X_{n-3}, X_{n-4}, ..., X_1\\right ) * P \\left (X_{n-2}, X_{n-3}, X_{n-4}, ..., X_1\\right )$$     Applying chain rule repeatedly, we get the following equation -     $$P\\left (\\bigcap_{i=1}^{n}X_i\\right ) = \\prod_{i=1}^{n} P\\left (X_i | P\\left (\\bigcap_{j=1}^{i-1}X_j\\right )\\right )$$     Keep the above equation in mind. Let's bring back Markov property. To bring some intuition behind Markov property, let's reuse Bayesian Network Example. If we say, the student scored very good  grades, then it is highly likely the student gets  acceptance letter  to university. No matter how  difficult the class was, how much  intelligent  the student was, and no matter what his/her  SAT score was. The key thing to note here is by  observing the node's parent, the influence by  non-descendants towards the node gets eliminated. Now, the equation becomes -     $$P\\left (\\bigcap_{i=1}^{n}X_i\\right ) = \\prod_{i=1}^{n} P\\left (X_i | Par\\left (X_i\\right )\\right )$$     Bingo, with the above equation, we have proved  Factorization Theorem  in Probability.     </p> <p>The decomposition of running Bayesian Network Example can be written as -</p> <p>$$ P\\left (Difficulty, Intelligence, Grade, SAT, Acceptance Letter\\right ) = P\\left (Difficulty\\right )*P\\left (Intelligence\\right )*\\left (Grade|Difficulty, Intelligence\\right )*P\\left (SAT|Intelligence\\right )*P\\left (Acceptance Letter|Grade\\right ) $$</p>"},{"location":"gsoc/variational-inference/#why-care-about-bayesian-networks","title":"Why care about Bayesian Networks\u00b6","text":"<p>Bayesian Networks allow us to determine the distribution of parameters given the data (Posterior Distribution). The whole idea is to model the underlying data generative process and estimate unobservable quantities. Regarding this, Bayes formula can be written as -</p> <p>$$ P\\left (\\theta | D\\right ) = \\frac{P\\left (D|\\theta\\right ) * P\\left (\\theta\\right )}{P\\left (D\\right )} $$</p> <p>$\\theta$ = Parameters of the model</p> <p>$P\\left (\\theta\\right )$ = Prior Distribution over the parameters</p> <p>$P\\left (D|\\theta\\right )$ = Likelihood of the data</p> <p>$P\\left (\\theta|D\\right )$ = Posterior Distribution</p> <p>$P\\left (D\\right )$ = Probability of Data. This term is calculated by marginalising out the effect of parameters.</p> <p>$$ P\\left (D\\right ) = \\int P\\left (D, \\theta\\right ) d\\left (\\theta\\right )\\\\ P\\left (D\\right ) = \\int P\\left (D|\\theta\\right ) P\\left (\\theta\\right ) d\\left (\\theta\\right ) $$</p> <p>So, the Bayes formula becomes -</p> <p>$$ P\\left (\\theta | D\\right ) = \\frac{P\\left (D|\\theta\\right ) * P\\left (\\theta\\right )}{\\int P\\left (D|\\theta\\right ) P\\left (\\theta\\right ) d\\left (\\theta\\right )} $$</p> <p>The devil is in the denominator. The integration over all the parameters is intractable. So we resort to sampling and optimization techniques.</p>"},{"location":"gsoc/variational-inference/#intro-to-variational-inference","title":"Intro to Variational Inference\u00b6","text":""},{"location":"gsoc/variational-inference/#information","title":"Information\u00b6","text":"<p>Variational Inference has its origin in Information Theory. So first, let's understand the basic terms - Information and Entropy . Simply, Information quantifies how much useful the data is. It is related to Probability Distributions as -</p> <p>$$ I = -\\log \\left (P\\left (X\\right )\\right ) $$</p> <p>The negative sign in the formula has high intuitive meaning. In words, it signifies whenever the probability of certain events is high, the related information is less and vica versa. For example -</p> <ol> <li>Consider the statement - It never snows in deserts. The probability of this statement being true is significantly high because we already know that it is hardly possible to snow in deserts. So, the related information is very small.</li> <li>Now consider - There was a snowfall in Sahara Desert in late December 2019. Wow, thats a great news because some unlikely event occured (probability was less). In turn, the information is high.</li> </ol>"},{"location":"gsoc/variational-inference/#entropy","title":"Entropy\u00b6","text":"<p>Entropy quantifies how much average Information is present in occurence of events. It is denoted by $H$. It is named Differential Entropy in case of Real Continuous Domain.</p> <p>$$ H =  E_{P\\left (X\\right )} \\left [-\\log\\left (P\\left (X\\right )\\right )\\right ]\\\\ H = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx $$</p>"},{"location":"gsoc/variational-inference/#entropy-of-normal-distribution","title":"Entropy of Normal Distribution\u00b6","text":"<p>As an exercise, let's calculate entropy of Normal Distribution. Let's denote $\\mu$ as mean nd $\\sigma$ as standard deviation of Normal Distribution. Remember the results, we will need them further.</p> <p>$$ X \\sim Normal\\left (\\mu, \\sigma^2\\right )\\\\ P_X\\left (x\\right ) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\\\ H = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx $$</p> <p>Only expanding $\\log\\left (P_X\\left (x\\right )\\right )$ -</p> <p>$$ H = -\\int_X P_X\\left (x\\right ) \\log\\left (\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\right ) dx\\\\ H = -\\frac{1}{2}\\int_X P_X\\left (x\\right ) \\log\\left (\\frac{1}{2 \\pi {\\sigma}^2}\\right )dx  - \\int_X P_X\\left (x\\right ) \\log\\left (e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\right ) dx\\\\ H = \\frac{1}{2}\\log \\left ( 2 \\pi {\\sigma}^2 \\right)\\int_X P_X\\left (x\\right ) dx  + \\frac{1}{2{\\sigma}^2} \\int_X \\left ( x-\\mu \\right)^2 P_X\\left (x\\right ) dx $$</p> <p>Identifying terms -</p> <p>$$ \\int_X P_X\\left (x\\right ) dx = 1\\\\ \\int_X \\left ( x-\\mu \\right)^2 P_X\\left (x\\right ) dx = \\sigma^2 $$</p> <p>Substituting back, the entropy becomes -</p> <p>$$ H = \\frac{1}{2}\\log \\left ( 2 \\pi {\\sigma}^2 \\right) + \\frac{1}{2\\sigma^2} \\sigma^2\\\\ H = \\frac{1}{2}\\left ( \\log \\left ( 2 \\pi {\\sigma}^2 \\right) + 1 \\right ) $$</p>"},{"location":"gsoc/variational-inference/#kl-divergence","title":"KL divergence\u00b6","text":"<p>This mathematical tool serves as the backbone of Variational Inference. Kullback\u2013Leibler (KL) divergence measures the mutual information between two probability distributions. Let's say, we have two probability distributions $P$ and $Q$, then KL divergence quantifies how much similar these distributions are. Mathematically, it is just the difference between entropies of probabilities distributions. In terms of notation, $KL(Q||P)$ represents KL divergence with respect to $Q$ against $P$.</p> <p>$$ KL(Q||P) = H_P - H_Q\\\\ = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx + \\int_X Q_X\\left (x\\right ) \\log\\left (Q_X\\left (x\\right )\\right ) dx $$</p> <p>Changing $-\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx$ to $-\\int_X Q_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx$ as the KL divergence is with respect to $Q$.</p> <p>$$ = -\\int_X Q_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx + \\int_X Q_X\\left (x\\right ) \\log\\left (Q_X\\left (x\\right )\\right ) dx\\\\ = \\int_X Q_X\\left (x \\right) \\log \\left( \\frac{Q_X\\left (x \\right)}{P_X\\left (x \\right)} \\right) dx $$</p> <p>Remember? We were stuck upon Bayesian Equation because of denominator term but now, we can estimate the posterior distribution $p(\\theta|D)$ by another distribution $q(\\theta)$ over all the parameters of the model.</p> <p>$$ KL(q(\\theta)||p(\\theta|D)) = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta|D)} \\right) d\\theta\\\\ $$</p> <p>Note</p> <p>         If two distributions are similar, then their entropies are similar, implies the KL divergence with respect to two distributions will be smaller. And vica versa. In Variational Inference, the whole idea is to minimize KL divergence so that our approximating distribution $q(\\theta)$ can be made similar to $p(\\theta|D)$.     </p> Extra: What are latent variables? <p>     If you go about exploring any paper talking about Variational Inference, then most certainly, the papers mention about latent variables instead of parameters. The parameters are fixed quantities for the model whereas latent variables are  unobserved quantities of the model conditioned on parameters. Also, we model parameters by probability distributions. For simplicity, let's consider the running terminology of  parameters  only.     </p>"},{"location":"gsoc/variational-inference/#evidence-lower-bound","title":"Evidence Lower Bound\u00b6","text":"<p>There is again an issue with KL divergence formula as it still involves posterior term i.e. $p(\\theta|D)$. Let's get rid of it -</p> <p>$$ KL(q(\\theta)||p(\\theta|D)) = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta|D)} \\right) d\\theta\\\\ KL = \\int q(\\theta) \\log \\left( \\frac{q(\\theta) p(D)}{p(\\theta, D)} \\right) d\\theta\\\\ KL = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta, D)} \\right) d\\theta + \\int q(\\theta) \\log \\left(p(D) \\right) d\\theta\\\\ KL + \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta = \\log \\left(p(D) \\right) \\int q(\\theta) d\\theta\\\\ $$</p> <p>Identifying terms -</p> <p>$$ \\int q(\\theta) d\\theta = 1 $$</p> <p>So, substituting back, our running equation becomes -</p> <p>$$ KL + \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta = \\log \\left(p(D) \\right) $$</p> <p>The term $\\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta$ is called Evidence Lower Bound (ELBO). The right side of the equation $\\log \\left(p(D) \\right)$ is constant.</p> <p>Observe</p> <p> Minimizing the KL divergence is equivalent to maximizing the ELBO. Also, the ELBO does not depend on posterior distribution.     </p> <p>Also,</p> <p>$$ ELBO = \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta\\\\ ELBO = E_{q(\\theta)}\\left [\\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) \\right]\\\\ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + E_{q(\\theta)} \\left [-\\log(q(\\theta)) \\right] $$</p> <p>The term $E_{q(\\theta)} \\left [-\\log(q(\\theta)) \\right]$ is entropy of $q(\\theta)$. Our running equation becomes -</p> <p>$$ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + H_{q(\\theta)} $$</p>"},{"location":"gsoc/variational-inference/#mean-field-advi","title":"Mean Field ADVI\u00b6","text":"<p>So far, the whole crux of the story is - To approximate the posterior, maximize the ELBO term. ADVI = Automatic Differentiation Variational Inference. I think the term Automatic Differentiation deals with maximizing the ELBO (or minimizing the negative ELBO) using any autograd differentiation library. Coming to Mean Field ADVI (MF ADVI), we simply assume that the parameters of approximating distribution $q(\\theta)$ are independent and posit Normal distributions over all parameters in transformed space to maximize ELBO.</p>"},{"location":"gsoc/variational-inference/#transformed-space","title":"Transformed Space\u00b6","text":"<p>To freely optimize ELBO, without caring about matching the support of model parameters, we transform the support of parameters to Real Coordinate Space. In other words, we optimize ELBO in transformed/unconstrained/unbounded space which automatically maps to minimization of KL divergence in original space. In terms of notation, let's denote a transformation over parameters $\\theta$ as $T$ and the transformed parameters as $\\zeta$. Mathematically, $\\zeta=T(\\theta)$. Also, since we are approximating by Normal Distributions, $q(\\zeta)$ can be written as -</p> <p>$$ q(\\zeta) = \\prod_{i=1}^{k} N(\\zeta_k; \\mu_k, \\sigma^2_k) $$</p> <p>Now, the transformed joint probability distribution of the model becomes -</p> <p>$$ p\\left (D, \\zeta \\right) = p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta)  \\right |\\\\ $$</p> Extra: Proof of transformation equation <p>To simplify notations, let's use $Y=T(X)$ instead of $\\zeta=T(\\theta)$. After reaching the results, we will put the values back. Also, let's denote cummulative distribution function (cdf) as $F$. There are two cases which respect to properties of function $T$.Case 1 - When $T$ is an increasing function $$F_Y(y) = P(Y &lt;= y) = P(T(X) &lt;= y)\\\\     = P\\left(X &lt;= T^{-1}(y) \\right) = F_X\\left(T^{-1}(y) \\right)\\\\     F_Y(y) = F_X\\left(T^{-1}(y) \\right)$$Let's differentiate with respect to $y$ both sides - $$\\frac{\\mathrm{d} (F_Y(y))}{\\mathrm{d} y} = \\frac{\\mathrm{d} (F_X\\left(T^{-1}(y) \\right))}{\\mathrm{d} y}\\\\     P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}$$Case 2 - When $T$ is a descreasing function $$F_Y(y) = P(Y &lt;= y) = P(T(X) &lt;= y) = P\\left(X &gt;= T^{-1}(y) \\right)\\\\     = 1-P\\left(X &lt; T^{-1}(y) \\right) = 1-P\\left(X &lt;= T^{-1}(y) \\right) = 1-F_X\\left(T^{-1}(y) \\right)\\\\     F_Y(y) = 1-F_X\\left(T^{-1}(y) \\right)$$Let's differentiate with respect to $y$ both sides - $$\\frac{\\mathrm{d} (F_Y(y))}{\\mathrm{d} y} = \\frac{\\mathrm{d} (1-F_X\\left(T^{-1}(y) \\right))}{\\mathrm{d} y}\\\\     P_Y(y) = (-1) P_X\\left(T^{-1}(y) \\right) (-1) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}\\\\     P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}$$Combining both results - $$P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\left | \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y} \\right |$$Now comes the role of Jacobians to deal with multivariate parameters $X$ and $Y$. $$J_{T^{-1}}(Y) = \\begin{vmatrix}     \\frac{\\partial (T_1^{-1})}{\\partial y_1} &amp; ... &amp; \\frac{\\partial (T_1^{-1})}{\\partial y_k}\\\\     . &amp;  &amp; .\\\\     . &amp;  &amp; .\\\\     \\frac{\\partial (T_k^{-1})}{\\partial y_1} &amp; ... &amp;\\frac{\\partial (T_k^{-1})}{\\partial y_k}     \\end{vmatrix}$$Concluding - $$P(Y) = P(T^{-1}(Y)) |det J_{T^{-1}}(Y)|\\\\P(Y) = P(X) |det J_{T^{-1}}(Y)|     $$Substitute $X$ as $\\theta$ and $Y$ as $\\zeta$, we will get - $$P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\$$     </p>"},{"location":"gsoc/variational-inference/#elbo-in-transformed-space","title":"ELBO in transformed Space\u00b6","text":"<p>Let's bring back the equation formed at ELBO. Expressing ELBO in terms of $\\zeta$ -</p> <p>$$ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + H_{q(\\theta)}\\\\ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta)  \\right | \\right) \\right] + H_{q(\\zeta)} $$</p> <p>Since, we are optimizing ELBO by factorized Normal Distributions, let's bring back the results of Entropy of Normal Distribution. Our running equation becomes -</p> <p>$$ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta)  \\right | \\right) \\right] + H_{q(\\zeta)}\\\\ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta)  \\right | \\right) \\right] + \\frac{1}{2}\\left ( \\log \\left ( 2 \\pi {\\sigma}^2 \\right) + 1 \\right ) $$</p> <p>Success</p> <p>         The above ELBO equation is the final one which needs to be optimized.     </p>"},{"location":"gsoc/variational-inference/#lets-code","title":"Let's Code\u00b6","text":""},{"location":"gsoc/variational-inference/#transformed-space-example-1","title":"Transformed Space Example-1\u00b6","text":"<p>Transformation of Standard Exponential Distribution</p> <p>$$ P_X(x) = e^{-x} $$</p> <p>The support of Exponential Distribution is $x&gt;=0$. Let's use log transformation  to map the support to real number line. Mathematically, $\\zeta=\\log(\\theta)$. Now, let's bring back our transformed joint probability distribution equation -</p> <p>$$ P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\ P(\\zeta) = P(e^{\\zeta}) * e^{\\zeta} $$</p> <p>Converting this directly into Python code -</p>"},{"location":"gsoc/variational-inference/#transformed-space-example-2","title":"Transformed Space Example-2\u00b6","text":"<p>Transformation of Uniform Distribution (with support $0&lt;=x&lt;=1$)</p> <p>$$ P_X(x) = 1 $$</p> <p>Let's use logit or inverse sigmoid transformation to map the support to real number line. Mathematically, $\\zeta=logit(\\theta)$.</p> <p>$$ P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\ P(\\zeta) = P(sig(\\zeta)) * sig(\\zeta) * (1-sig(\\zeta)) $$</p> <p>where $sig$ is the sigmoid function.</p> <p>Converting this directly into Python code -</p>"},{"location":"gsoc/variational-inference/#mean-field-advi-example","title":"Mean Field ADVI Example\u00b6","text":"<p>Infer $\\mu$ and $\\sigma$ for Normal distribution.</p>"},{"location":"gsoc/variational-inference/#drawbacks-of-this-blog-post","title":"Drawbacks of this blog post\u00b6","text":"<ol> <li>I have not used consistent notation for probability density functions (pdfs). Because I like equations handled this way.</li> <li>Coming up with more good examples using minibatches.</li> <li>The ADVI papers also mention Elliptical standardization and Adaptive step size for optimizers. I have not understood those sections well and thus, haven't tried to implement them.</li> </ol>"},{"location":"gsoc/variational-inference/#references","title":"References\u00b6","text":"<ul> <li>Chapter 1 and 2: Probabilistic Graphical Model Book</li> <li>Blog Post: An Introduction to Probability and Computational Bayesian Statistics by Ericmjl</li> <li>Section 10.1: Variational Inference Pattern Recognition and Machine Learning Book</li> <li>Section 2.5: Transformations Statistical Theory and Inference Book</li> <li>YouTube: Variational Inference in Python by Austin Rochford</li> <li>PyMC4: Basic Usage Notebook</li> <li>TFP: Joint Modelling Notebook</li> <li>Papers:<ul> <li>Automatic Differentiation Variational Inference. Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and Blei, D. M. (2016).</li> <li>Automatic Variational Inference in Stan. Kucukelbir, A., Ranganath, R., Gelman, A., &amp; Blei, D. (2015).</li> </ul> </li> </ul>"},{"location":"gsoc/variational-inference/#special-thanks","title":"Special Thanks\u00b6","text":"<ul> <li>Website: codecogs.com to help me generate LaTeX equations.</li> <li>Comments: #1 and #2 by Luciano Paz that cleared my all doubts regarding transformations.</li> </ul>"},{"location":"gsoc/week-1-into-gsoc/","title":"Week 1 into GSoC","text":"<p>This blog contains my 1st week's progress (June 1 - June 7) into GSoC. After I have got a fair intuition of how\u00a0tfp.vi\u00a0module works, I started implementing Mean Field ADVI in PyMC4. With the guidance of my mentor Maxim Kochurov, we have split the implementation of VI approximations into 3 major parts -</p> <ol> <li>Vector LogProb function</li> <li>Approximate Distribution from TFP</li> <li>Integrate with TFP, Optimizers, etc</li> </ol> <p>I started implementing ideas into my fork. Within 4-5 days, I have come up with a reasonable design incorporating the above 3 guidelines. And here is the PR #280 and now, I am working on the suggestions to improve API design.</p> <p>On the way of my exploration of\u00a0tfp.vi\u00a0module, I see that we can use either tfp.distributions.JointDistributionSequential or tfp.distributions.MultivariateNormalDiag to implement Mean Field ADVI. The PR #280 is based on JointDistributionSequential. I will incorporate use of MultivariateNormalDiag after having a flattened view of parameters.</p> <p>I observe that I am a week ahead of my proposed GSoC timeline. This means I have more time to explore PyMC4.</p> <p>I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community.</p> <p>Thank you for reading!</p> <p>With , Sayam</p>"},{"location":"gsoc/week-2-into-gsoc/","title":"Week 2 into GSoC","text":"<p>Following up the work done in week 1, I started exploring surrounding packages of PyMC4 i.e. TFP, ArviZ and PyMC3 to gain better insights how things need to be integrated. On the way, I opened a few issues and PRs fixing docstrings.</p> <ul> <li>ArviZ Issue #1232 regarding rendering of notebooks.</li> <li>PyMC4 Issue #283 regarding Transformations. I am still wondering whether the transformations are missing or automatically handled.</li> <li>TFP PR #965 that fixes <code>tfb.Chain</code> docstrings.</li> </ul>"},{"location":"gsoc/week-2-into-gsoc/#work-done-this-week","title":"Work done this week","text":"<p>Figuring out the solutions over the review of the PR #280, I have -</p> <ul> <li>Added <code>sample</code> method for sampling from posterior distribution.</li> <li>Fixed return type from <code>fit</code> function to include <code>Approximation</code> as well.</li> <li>Updated variational quickstart notebook incorporating both of the changes above.</li> </ul> <p>It took me a bit longer to look out for good variable names and to make quick-start notebook similar to PyMC3's Variational API notebook. But the time was worth it.</p>"},{"location":"gsoc/week-2-into-gsoc/#experiments","title":"Experiments","text":"<p>Most of the time this week, I spent experimenting on ideas building on top of week 1's work. I polished out my experiments and created two gists.</p>"},{"location":"gsoc/week-2-into-gsoc/#gist-1-source","title":"Gist 1 - Source","text":"<p>Comparison between MeanField ADVI in TFP, PyMC3 and PyMC4. It was fun doing this.</p>"},{"location":"gsoc/week-2-into-gsoc/#gist-2-source","title":"Gist 2 - Source","text":"<p>Experimenting with ArviZ. I started learning from how trace from <code>tfp.mcmc.sample_chain</code> gets converted to ArviZ InferenceData. Following the same, I soon ran into shape issues while integrating it into <code>sample</code> method. I plan to resolve it in week 3.</p>"},{"location":"gsoc/week-2-into-gsoc/#tasks-for-week-3","title":"Tasks for week 3","text":"<ul> <li>Tests about convergence checks</li> <li>Add convergence criteria example in quick start notebook</li> <li>Include samples for untransformed variables as well</li> <li>Handle shape issues with ArviZ while sampling</li> <li>More user friendly optimizers</li> </ul> <p>I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community.</p> <p>Thank you for reading!</p> <p>With , Sayam</p>"},{"location":"gsoc/week-3-into-gsoc/","title":"Week 3 into GSoC","text":"<p>Today (June 21, 2020) marks the ending of week 3 of GSoC coding period. Let me summarize what I did for this week. Following up the work done in week 2, I went on exploring <code>ArviZ</code> InferenceData and <code>tfp.vi</code> module to learn how to deal with shapes, convergence checks and optimizers.</p> <ul> <li>Adding a new axis to samples resolved the shape issues. By this, <code>number of chains</code> parameter is set to 1 and all the priors are handled perfectly.</li> <li>The convergence checks did not seem to work properly. (As pointed out by my mentor, we need to adjust window size).</li> <li>The default tensorflow optimizers also did not lead to convergence really well. Trying out the default values taken from PyMC3, I got really good results. This motivated me to write <code>updates</code> module for Variational Inference Interface.</li> <li>I applied inverse of bijectors to transformed parameters to match support in bounded space. But this approach is wrong. I need to handle this using deterministics callback which I will do in coming week.</li> <li>I had also written tests during this interval.</li> <li>From last week, I did not get quite good results while experimenting with Mean Field ADVI in TFP, PyMC3 and PyMC4. As suggested by my mentor, setting a common random seed and same optimizer leads to very good results . (gist)</li> </ul>"},{"location":"gsoc/week-3-into-gsoc/#experiments-source","title":"Experiments - Source","text":"<p>Whatever experiments I perform, I polish them out and share through GitHub gists. I do not why but I started loving to share code through GitHub gists rather than Colab or GitHub repo.</p> <p>Here is the notebook -</p>"},{"location":"gsoc/week-3-into-gsoc/#tasks-for-week-4","title":"Tasks for week 4","text":"<p>Phase 1 Evaluations are coming up. So, I need to sync work with my proposed timeline and spend time summarizing all the results in a single notebook. My tasks for week 4 -</p> <ul> <li>Write tests for conjugate normal models with known mean/variance.</li> <li>Configure <code>atol</code> argument for <code>np.testing.assert_allclose</code>. (I misunderstood how this parameter works)</li> <li>Complete docs for optimizers by adding <code>**kwargs</code> option and writing corresponding maths equations.</li> <li>Properly configure convergence checks and add an example to quickstart notebook.</li> <li>Configure autobatching. (I need to understand how this works for mcmc)</li> <li>Integrate Deterministics callbacks.</li> <li>Complete Full Rank/Low Rank Approximation. (This will take time)</li> <li>Configure Minibatches.</li> <li>Update quick_start notebook with respect to all changes above.</li> </ul>"},{"location":"gsoc/week-3-into-gsoc/#extra","title":"Extra","text":"<p>If I will be able to complete above mentioned tasks in time, I would love to -</p> <ul> <li>Have another implementation of Mean Field ADVI using <code>tfd.MultivariateNormalDiag</code>.</li> <li>Play around with Mean Field ADVI on Eight Schools notebook.</li> <li>Resolve a warning from <code>tfp.vi</code> module regarding repetitive use of <code>tf.function</code> in a loop. Maybe using <code>tf.while_loop</code> will solve this but I am not sure.</li> <li>Experiment with MeanField/ FullRank/ LowRank ADVI in TFP, PyMC3, PyMC4 inspired from ColCarroll's notebook. I am already getting excited to play around with this after having all APIs set correctly.</li> </ul>"},{"location":"gsoc/week-3-into-gsoc/#for-evaluations","title":"For Evaluations","text":"<p>I look forward to learn how Variational AutoEncoders are implemented in PyMC3 and try implementing that in PyMC4.</p> <p>I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community.</p> <p>Thank you for reading!</p> <p>With , Sayam</p>"},{"location":"gsoc/week-4-into-gsoc/","title":"Week 4 into GSoC","text":"<p>So, the phase 1 coding period ends and let me summarize what I did for the last week. Let's bring back the tasks proposed at the end of week 3 and figure out how many I have completed -</p> <ul> <li> Write tests for conjugate normal models with known mean/variance.</li> <li> Configure <code>atol</code> argument for <code>np.testing.assert_allclose</code>.</li> <li> Complete docs for optimizers by adding <code>**kwargs</code> option and writing corresponding maths equations.</li> <li> Properly configure convergence checks and add an example to quickstart notebooks.</li> <li> Configure autobatching.</li> <li> Integrate Deterministics callbacks. (But I did it wrong)</li> <li> Complete remaining Approximations.<ul> <li> Full Rank Approximation.</li> <li> Low Rank Approximation.</li> </ul> </li> <li> Configure Minibatches.</li> <li> Update quick_start notebook with respect to all changes above.</li> </ul> <p>I was unable to complete a few tasks because I got stuck for many days figuring out how to correctly handle shapes in Full Rank ADVI. Finally I was able to come up with a new <code>_build_logfn</code> to handle shapes (on the guidelines provided by my mentor).</p>"},{"location":"gsoc/week-4-into-gsoc/#experiments","title":"Experiments","text":""},{"location":"gsoc/week-4-into-gsoc/#gist-1-source","title":"Gist 1 - Source","text":"<p>I started the fourth week with a plan to include deterministics callbacks. Here are my experiments doing the same with PyMC4. All the determinitics are included in trace function. But when I opted for the same strategy to include deterministics while sampling, I got many shapes errors. The reason <code>determinitics_callback</code> failed because it assumes <code>sample size = 1</code>. We need to change this API.</p>"},{"location":"gsoc/week-4-into-gsoc/#gist-2-source","title":"Gist 2 - Source","text":"<p>My experiments involving how to configure Full Rank ADVI in TFP.</p>"},{"location":"gsoc/week-4-into-gsoc/#gist-3-source","title":"Gist 3 - Source","text":"<p>Comparisons drawn between PyMC3 and PyMC4 for 2-d Gaussians. TODO - complete comparisons for Mixture Distributions as well.</p>"},{"location":"gsoc/week-4-into-gsoc/#tasks-for-the-remaining-gsoc-period","title":"Tasks for the remaining GSoC period","text":"<p>From the API point of view, only 5 tasks are left for the GSoC -</p> <ul> <li>Configure autobatching.</li> <li>Configure Minibatches.</li> <li>Add an option of progressbar.</li> <li>Include deterministics samples.</li> <li>Add convergence checks.</li> </ul> <p>From the view of adding examples, all the notebooks from PyMC3 need to be ported to PyMC4.</p> <p>For week 5, I look forward to add progressbar and convergence checks to PyMC4.</p>"},{"location":"gsoc/week-4-into-gsoc/#comparing-with-timeline","title":"Comparing with timeline","text":"<p>I have already completed all the tasks proposed for phase 1 evaluations. Also I have added Mean Field and Full Rank Approximations which were proposed for phase 2 and phase 3 of GSoC coding period respectively.</p> <p>I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community.</p> <p>Thank you for reading!</p> <p>With , Sayam</p>"},{"location":"gsoc/week-5-into-gsoc/","title":"Week 5 into GSoC","text":"<p>Today (July 12, 2020) marks mid-way between phase 1 and phase 2 of GSoC coding period. Some good news, I have cleared Phase 1 evaluations. My mentor Maxim Kochurov is amazing. Not only we explore different ideas related to the project but also we have great conversations not related to GSoC.</p> <p>Most of the time spent this week was into resolving issues with Full Rank ADVI PR #289. Here is the long story short -</p> <ul> <li>Mean Field ADVI PR uses JointDistributionSequential and on the same paths, I added an interface of Full Rank ADVI.</li> <li>That approach was wrong because I misunderstood model flattening as variable flattening. And this leads to all sort of shape issues.</li> <li>After learning about model flattening, I integrated it into the PR.</li> <li>ADVI worked but losses contains lots of nans. As suspected, it was due to missing transformations.</li> <li>It took time to figure out how to setup transformations but later selected manually doing it.</li> <li>I was unable to make transformations generalized enough. Conditional statements and <code>tf.cond</code> does not seem to work either because at hand, we do not have values to transform inside bijectors. Later I plan to write a proposal mentioning the issue to PyMC-devs.</li> <li>Then after transformations, the major issue was of cholesky decomposition failure. And this comment is a life saviour one.</li> <li>It took time to experiment with each option with park_bias_model and the last one worked. As stated, cholesky decomposition is unstable to <code>tf.float32</code>. So casting all the parameters to <code>tf.float64</code> resolved the issue. I also want to thank Tirth Patel for helping me on the way to resolve dtype issues.</li> <li>Finally, I am all set to see the Full Rank ADVI PR getting merged.</li> </ul>"},{"location":"gsoc/week-5-into-gsoc/#experiments","title":"Experiments","text":""},{"location":"gsoc/week-5-into-gsoc/#gist-1-source","title":"Gist 1 - Source","text":"<p>Flattening and Full Rank ADVI in PyMC4</p>"},{"location":"gsoc/week-5-into-gsoc/#gist-2-source","title":"Gist 2 - Source","text":"<p>Testing issues with transformations</p>"},{"location":"gsoc/week-5-into-gsoc/#gist-3-source","title":"Gist 3 - Source","text":"<p>Failed at an attempt to generalize transformations</p>"},{"location":"gsoc/week-5-into-gsoc/#experiment-4","title":"Experiment 4","text":"<p>Good news regarding the progress bars but I cannot do <code>tf.print(\".\"*trace.step)</code></p> <pre><code>num_steps = 10_000\ndef trace_fn(trace):\n    tf.cond(\n        tf.math.mod(trace.step, 100) == 0,\n        lambda: tf.print(trace.step, \"/\", num_steps, \"Loss:\", trace.loss),\n        lambda: tf.print(\"\", end=\"\\r\")\n    )\n    return trace.loss\n\n# Pass this trace_fn to pm.fit()\n</code></pre> <p>I am planning to add convergence checks and progress bars for next week. Stay tuned.</p> <p>I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community.</p> <p>Thank you for reading!</p> <p>With , Sayam</p>"},{"location":"gsoc/week-6-7-into-gsoc/","title":"Week 6 and 7 into GSoC","text":"<p>All these days I have experimenting with Full Rank ADVI testing on rugby, radon notebooks and park_bias_model. Now, I am planning to draw general conclusions in what scenarios Variational Inference leads to Cholesky Decomposition errors when the model contains Gaussian Processes. Its mainly due to dtypes and high learning rates.</p>"},{"location":"gsoc/week-6-7-into-gsoc/#gist-source","title":"Gist - Source","text":"<p>Also, I have written a short proposal to explore ways of adding transforms to PyMC4.</p> <p>Thank you for reading!</p> <p>With , Sayam</p>"},{"location":"gsoc/week-9-into-gsoc/","title":"Week 9 into GSoC","text":"<p>The second evaluation for GSoC is cleared. I am happy. Also, the PR #289 for Full Rank ADVI is merged in. This week's time is spent adding small but useful features to PyMC4. The PR #310 includes addition of progress bar. I spent most of the time digging deep into TFP codebase learning how vi module is integrated. Some more features to be included are -</p> <ul> <li>Parameter Convergence checks</li> <li>Low Rank Approximation</li> <li>Add a notebook playing around with mcmc and VI approximations over hierarchial models.</li> </ul> <p>I plan to complete all these features before 15 August so that I can spend time writing documentation for PyMC4. I am loving my time with PyMC community.</p> <p>Thank you for reading!</p> <p>With , Sayam</p>"},{"location":"gsoc/work-summary/","title":"Google Summer of Code'20 Highlights with NumFOCUS","text":"<p>This post is meant to summarize the work done over the GSoC coding period. Let's get started real quick.</p>"},{"location":"gsoc/work-summary/#about-the-project","title":"About the project","text":"<p>My GSoC proposal was about adding a Variational Inference interface to PyMC4. Apart from MCMC algorithms, VI proposes an approximating distribution to fit the posterior. The whole plan was to implement two Variational Inference algorithms - Mean Field ADVI and Full Rank ADVI.</p>"},{"location":"gsoc/work-summary/#resolving-key-challenges","title":"Resolving Key challenges","text":"Key Challenges Solutions proposed How its resolved <code>theano.clone</code> equivalent for TF2 Model execution with replaced inputs Normal distribution's sample method is executed over flattened view of parameters Flattened view of parameters Use <code>tf.reshape()</code> Used <code>tf.concat()</code> with <code>tf.reshape()</code> Optimizers for ELBO Use tf.keras.optimizers Optimizers either from TFv1 or TFv2 with defaults from pymc3.updates can be used Initialization of MeanField and Full Rank ADVI Manually set bijectors Relied on <code>tfp.TransformedVariable</code> Progress bar Use <code>tqdm</code> or <code>\u200btf.keras.utils.Progbar</code> A small hack over <code>tf.print</code> Minibatch processing of data Capture slice in memory This is the only incomplete feature. Maybe <code>tf.Dataset</code> API has to explored more or implement our own <code>tfp.vi.fit_surrogate_posterior</code> function."},{"location":"gsoc/work-summary/#community-bounding-period","title":"Community Bounding Period","text":"<ul> <li>This was a super interesting period. I got to know about many PyMC core developers through slack.</li> <li>I spent the entire time learning about the basics of Bayesian statistics, prior, posterior predictive checks, and the theory of Variational Inference.</li> <li>I had also written a blog post during this interval about the nuts and bolts of VI and the implementation of Mean Field ADVI as well in Tensorflow Probability. Here is the blog post - Demystify Variational Inference.</li> <li>The most difficult part of learning VI was to understand the transformations because PyMC3 and TFP handle transformations differently.</li> </ul>"},{"location":"gsoc/work-summary/#month-1","title":"Month 1","text":"<p>The coding period started from June 1 and my intention for this period was to add a very basic and general Variational Inference interface to PyMC4. Here is the PR #280 and workflow of the basic interface was -</p> <ul> <li>Get the vectorized <code>log prob</code> of the model.</li> <li>For each parameter of the model, have a Normal Distribution with the same shape and then build a posterior using <code>tfd.JointDistributionSequential</code>.</li> <li>Add optimizers with defaults from PyMC3 and perform VI using <code>tfp.fit_surrogate_posterior</code>.</li> <li>Sample from <code>tfd.JointDistributionSequential</code> and there is no need of equivalent of <code>theano.clone</code>.</li> <li>Transform the samples by quering the <code>SamplingState</code> but <code>Deterministics</code> have to be added as well.</li> <li>Resolve shape issues with ArviZ. In short, making <code>chains=1</code>.</li> </ul> <p>I got the basic interface merged by late June and now, it was time to work upon Full Rank ADVI. I managed to open a PR #289 with Full Rank ADVI interface by the end of June.</p>"},{"location":"gsoc/work-summary/#month-2","title":"Month 2","text":"<p>This was the most dramatic month of GSoC coding period. Because Full Rank ADVI proposed in PR #289 resulted in errors most of the time. Here is the gist of workflow that was followed to get some useful insights about the errors -</p> <ul> <li>Instead of solving the shape issues independently and posing a <code>MvNormal</code> distribution for each parameter, build the posterior using flattened view of parameters.</li> <li>There were lots of NaNs in the ELBO, because of improper handling of transformations. As a result, <code>Interval</code>, <code>LowerBounded</code> and <code>UpperBounded</code> transformations were added as well.</li> <li>Then came the issue of <code>Cholesky Decomposition errors</code> while working with Gaussian Processes and Variational Inference. Here are my few insights after rigorous testing with different inputs -<ul> <li>Use dtype <code>tf.float64</code> with FullRank ADVI to maintain positive definiteness of covariance matrix.</li> <li>Avoid aggressive optimization of ELBO. Maintain learning rates around <code>1e-3</code>.</li> <li>Stabilize the diagonal of covariance matrix by adding a small jitter.</li> <li>Double check for NaNs in the data.</li> </ul> </li> <li>Here the results after trying reparametrization and different jitter amounts while doing VI. </li> </ul> <p>I got this PR merged by the end of July. And now, it was time to work on adding some features to ADVI.</p>"},{"location":"gsoc/work-summary/#month-3","title":"Month 3","text":"<p>After adding missing transformations in PR #289, my mentor asked me to write a proposal so as the Bounded Distributions are inherited instead of we applying transformations manually to each distribution. I explored each possibility to make a generalized version of transformations as it is done in PyMC3 using <code>tf.cond</code>. Since, we do not have values before model execution, it was difficult to use <code>tf.cond</code>. Here is the proposal's source.</p> <p>After getting an interface to use MeanField and FullRank ADVI, some features that are included in the PR #310 -</p> <ul> <li>Add a progress bar. (This is small hack over <code>tf.print</code>)</li> <li>Test progress bar in different OS.</li> <li>Add <code>ParameterConvergence</code> criteria to test convergence.</li> <li>Add LowRank Approximation.</li> </ul> <p>I am still working on adding examples on hierarchical models and I hope to get it merged soon.</p>"},{"location":"gsoc/work-summary/#contributions","title":"Contributions","text":"<p>The Pull Requests I have opened and got merged during GSoC. I have explained each one above but here I try to summarize.</p> <ul> <li>Add Variational Inference Interface: #280</li> <li>Add Full Rank Approximation: #289</li> <li>Add features to ADVI: #310 (WIP)</li> <li>Remove transformations for Discrete distributions: #314</li> </ul>"},{"location":"gsoc/work-summary/#gists-created","title":"Gists created","text":"<p>Whatever experiments I perform to aid my learnings, I polish them out and share through GitHub gists. I do not why but I started loving to share code through GitHub gists rather than Colab or GitHub repo. Here are all the experiments I performed with ADVI during this summer.</p> <ul> <li>Comparison of MeanField ADVI in TFP, PyMC3, PyMC4: Source</li> <li>Demonstration of shape issues while working with InferenceData: Source</li> <li>Playing around Convergence and Optimizers: Source</li> <li>Tracking all parameters including deterministics: Source</li> <li>Implementation of FullRank ADVI in TFP: Source</li> <li>Comparison of MeanField and FullRank ADVI over correlated Gaussians: Source</li> <li>Model flattening and Full Rank ADVI in PyMC4: Source</li> <li>Missing transformations in PyMC4: Source</li> <li>Testing transformations in PyMC4: Source</li> <li>Distribution Enhancement Proposal: Source</li> <li>Hacking <code>tf.print</code> for progress bar: Source</li> <li>Parameter Convergence Checks in TFP: Source</li> </ul>"},{"location":"gsoc/work-summary/#future-goals","title":"Future Goals","text":"<p>Some future tasks I would like to work upon -</p> <ul> <li>Configure Mini Batch processing of data.</li> <li>Add Normalizing Flows to variational inference interface.</li> <li>Add support of Variational AutoEncoders to PyMC4.</li> </ul>"},{"location":"gsoc/work-summary/#conclusion","title":"Conclusion","text":"<p>It was an incredible experience contributing to open source. I have improved my Python skills. I want to thank my mentors @ferrine and @twiecki for being extremely supportive throughout this entire journey. I am loving my time with the PyMC community. Next, I also want to thank @numfocus community for sharing this opportunity via Google Summer of Code.</p> <p>Thank you for being a part of this fantastic summer.</p> <p>With , Sayam Kumar</p>"},{"location":"guides/git/","title":"Git","text":"<ul> <li>Git is a distributed version control system.</li> <li>Svn is a central control system.</li> <li>In central control system, all the files are stored in the server's database. In distributed vcs, everybody has a local repository.</li> <li>There are three areas -<ul> <li><code>Working Directory</code> - Where we put up all the files and codes in the local repository. We need to add them.</li> <li><code>Staging Area</code> - Where we commit our files</li> <li><code>Repo</code> - Contains all the files</li> </ul> </li> </ul>"},{"location":"guides/git/#basic-commands","title":"Basic Commands","text":"<pre><code>git --version\ngit config --global user.name \"&lt;user-name&gt;\"\ngit config --global user.email \"&lt;user-email-ID&gt;\"\ngit config --global --list\ngit config --local --list\ngit help &lt;verb&gt;\ngit &lt;verb&gt; --help\ngit init  # For initialising a repository with git\nrm -rf .git  # To stop tracking\ntouch .gitignore  # To put all the unwanted files which we do not want to git\ngit remote -v  # Info about remote repo\n</code></pre>"},{"location":"guides/git/#save-changes","title":"Save changes","text":"<pre><code>git status\ngit add -A  # Adds files to staging area\ngit commit -m \"Message\"\ngit pull origin master  # If remote is set\ngit push origin master\n</code></pre>"},{"location":"guides/git/#create-a-new-git-repo-with-existing-code","title":"Create a new git repo with existing code","text":"<pre><code>git init  # In the folder. Also create a new repo in GitHub\ngit add -A\ngit commit -m \"message\"\ngit remote add origin link_to_repo\ngit push -u origin master\n</code></pre>"},{"location":"guides/git/#branches","title":"Branches","text":"<p>Common Overflow - create a branch for desire feature</p> <pre><code>git branch  # To check the current branch\ngit branch my_branch  # To initialise a branch\ngit checkout my_branch  # To change the branch\ngit checkout -b my_branch  # Single command for above two operations\n</code></pre> <p>After adding and commiting, push branch to remote</p> <pre><code>git push -u origin my_branch  # To create a separate branch in the repo with the modified changes\ngit branch -a  # Check all branches at local as well as remote repo\n</code></pre>"},{"location":"guides/git/#merge-a-branch","title":"Merge a branch","text":"<pre><code>git checkout master\ngit pull origin master\ngit branch --merged\ngit merge my_branch\ngit push origin master\n</code></pre>"},{"location":"guides/git/#delete-a-branch","title":"Delete a branch","text":"<pre><code>git branch --merged  # To check if everything is merged correctly\ngit branch -d my_branch  # Deleted locally\ngit branch -D my_branch  # Delete branch forcefully, if not merged\ngit branch -a  # To check\ngit push origin --delete my_branch\n</code></pre>"},{"location":"guides/git/#fixing-mistakes-and-bad-commits","title":"Fixing Mistakes and Bad Commits","text":""},{"location":"guides/git/#rename-a-message-after-a-commit","title":"Rename a message after a commit","text":"<p>But this changes the hash of the commit as the hash depends of message. This also changes the git history so it is not a good option.</p> <pre><code>git commit --amend -m \"Added add function\"\n</code></pre>"},{"location":"guides/git/#add-a-file-to-previous-commit","title":"Add a file to previous commit","text":"<p>First add that file to staging area. This again changes the git history.</p> <pre><code>touch new.py\ngit add new.py\ngit commit --amend  # Amend the file to last commit. Vim is opened asking for any changes in commit message. OR\ngit commit --amend --no-edit  # To keep the last commit message same\ngit log --stat  # To see changes in a commit\n</code></pre>"},{"location":"guides/git/#move-a-commit-to-different-branch","title":"Move a commit to different branch","text":"<ol> <li> <p>Copy the <code>hash</code> of commit you want to move -</p> <pre><code>git checkout &lt;branch-name&gt;\ngit cherry-pick &lt;hash-of-commit&gt;  # The changes are reflected but the commit hash is changed\n</code></pre> </li> <li> <p>Remove that <code>commit</code> from previous branch -</p> <pre><code># Remove from branch\ngit checkout &lt;branch-name&gt;\n\n# Three different types of reset -\ngit reset --soft &lt;previous-commit-hash-of-branch&gt;  # It will keep the changes in staging area.\ngit reset &lt;previous-commit-hash-of-branch&gt;  # Mixed reset - It will keep the changes in the working directory.\ngit reset --hard &lt;previous-commit-hash-of-branch&gt;  # Remove all the changes from tracked files. All new files added will be there in working directory. Untracked files are left\n</code></pre> <p>Warning- Use of <code>git reset --hard</code>, will reset to <code>previous commit</code> only when files are staged or commited. If they are in working directory, git reset --hard will reset only tracked files.</p> </li> <li> <p>To delete untracked files/directories forcefully -</p> <pre><code>git clean -df\n</code></pre> </li> <li> <p>Retrieve critical files that were lost, and you want them back -</p> <pre><code>git reflog  # Copy the hash\ngit checkout &lt;hash-copied-from-reflog&gt;  # Now the head is detached. You need to create a branch if you want those changes\ngit branch backup  # Create a backup branch\n</code></pre> </li> </ol>"},{"location":"guides/git/#git-revert","title":"Git revert","text":"<p>Creates new commit on top of earlier commits to revert the changes.</p> <pre><code>git revert &lt;hash-of-commit&gt;\n</code></pre>"},{"location":"guides/git/#using-stash-command","title":"Using stash command","text":"<p>Git stash helps when you have some uncommitted changes. You want to revert or switch between branches. Stash will temporarily save your changes.</p> <ol> <li> <p>Save you changes before moving around.</p> <pre><code>git stash save \"Message for your changes\"\ngit stash save -u \"Message for your changes\"  # For including all untracked as well\ngit stash save --all \"Message for your changes\"  # For including all untracked and ignored files as well\ngit stash list  # To list out all our stashes\n</code></pre> </li> <li> <p>Explore whatever you wish like.</p> </li> <li> <p>View the contents of the stash</p> <pre><code>git stash show -p  # For latest stash\ngit stash show -p stash:{index}  # For a particular stash\n</code></pre> </li> <li> <p>Get those changes back</p> <ol> <li> <p>Use <code>git stash apply</code></p> <pre><code>git stash apply  # To apply the latest stash\ngit stash apply stash:{index}  # To see the changes OR\ngit stash list  # But it does not get rid of the stash\ngit checkout -- .  # To return to spec\n</code></pre> </li> <li> <p>Use <code>git stash pop</code></p> <pre><code>git stash pop  # Applies the changes of topmost in git stash list and remove that one OR\ngit stash pop stash:{index}  # Apply and delete\ngit stash list  # That stash has been removed\n</code></pre> </li> </ol> </li> <li> <p>Let's suppose now you do not want to keep that changes</p> <ol> <li> <p>Drop a single stash</p> <pre><code>git stash drop  # For topmost stash\ngit stash drop stash:{index}  # For a specific stash\n</code></pre> </li> <li> <p>Drop all stashes</p> <pre><code>git stash clear\ngit stash list  # Empty list\n</code></pre> </li> </ol> </li> <li> <p>Stashes are carried from branch to branch</p> <pre><code>git stash save \"Message\"\ngit checkout &lt;branch_name&gt;\ngit stash pop\n</code></pre> </li> <li> <p>Create a new branch and apply that stash - The stash is also deleted.</p> <pre><code>git stash branch &lt;branch_name&gt;  # For topmost stash\ngit stash branch &lt;branch_name&gt; stash:{index}  # For a specific stash\n</code></pre> </li> </ol> <p>Note - The <code>git stash save</code> API is deprecated in favour of new <code>git stash push</code> API from 2.16 version onwards.</p>"},{"location":"guides/git/#different-types-of-git-add","title":"Different Types of git add","text":"<ol> <li> <p>Stage all the changes including untracked and .dot files, in the <code>entire working tree</code></p> <pre><code>git add --all\ngit add --all &lt;directory-name&gt;  # It will only stage changes made in that directory\ngit add &lt;directory-name&gt;  # --all or -A option is by default\ngit add --no-all &lt;directory-name&gt;  # It will stage only modified and new files, not the deleted ones in directory\n</code></pre> </li> <li> <p>Stage only modified and deleted files, in the entire working directory, <code>-u</code> or <code>--update</code></p> <pre><code>git add -u\ngit add -u &lt;directory-name&gt;  # It will stage only modified and deleted, not the new ones in the directory\n</code></pre> </li> <li> <p>Stage all the changes including untracked and .dot files, in the <code>current working directory and below</code> not the parent ones.</p> <pre><code>git add .  # Changes will be visible if we run this command in any sub directory\n</code></pre> </li> <li> <p>Stage with \"*\" It will stage the files it can see in ls - This is not recommended as its not including deleted as well as .dot files</p> <pre><code>git add *\n</code></pre> </li> </ol>"},{"location":"guides/git/#extras","title":"Extras","text":"<pre><code># List out the differences\ngit diff  # In working directory\ngit diff --staged  # In staging area\ngit diff hash1 hash2  # Between two commits via hashes\n\n# Discard the changes in a working directory\ngit checkout &lt;filename&gt;  # For a single file\ngit checkout -- .  # All tracked files\ngit clean -df  # All untracked files\n\n# Remove files from staging area and put them back into working directory\ngit reset &lt;filename&gt;  # For a single file\ngit reset  # To remove everything\n\n# Newer APIs\ngit restore &lt;filename&gt;  # To discard changes in working directory\ngit restore --staged &lt;filename&gt;  # To unstage\n\n# Logs\ngit log  # To check the various commits\ngit log -p  # Log with patches\ngit log -2  # For last 2 commits\ngit log --stat  # To see the changes\ngit log --oneline  # All commit messages in single line\n\n# Show the changes done by a commit\ngit show  # For latest commit. This is equivalent to git log -p -1\ngit show &lt;commit-hash&gt;\n\n# Alias\ngit config --global alias.&lt;alias&gt; &lt;command&gt;\ngit config --global alias.st status  # Typing git st will show results of git status\n\n# Remote\ngit remote  # List of all remotes\ngit remote -v  # Be a little more verbose\ngit remote add &lt;remote name&gt; &lt;url&gt;  # To set a remote\ngit remote rename &lt;oldname&gt; &lt;newname&gt;  # To modify remote's name\ngit remote set-url &lt;remote name&gt; &lt;url&gt;  # To modify a remote's url\ngit remote rm &lt;remote name&gt;  # To delete a remote\n# If we set-url or add a new remote, we need to do -\ngit fetch\ngit remote set-head &lt;remote name&gt; master\ngit push --set-upstream &lt;remote name&gt; master  # To track all the branches\n\n# Rename\ngit mv &lt;old filename&gt; &lt;new filename&gt;  # Rename a file\ngit branch -m &lt;newname&gt;  # Rename a branch while working on branch\ngit branch -m &lt;oldname&gt; &lt;newname&gt;  # Rename a branch from outside the branch\n\n# Signing commits\ngit config --global commit.gpgsign true\ngit config --global --edit  # To change the settings\n\n# Forks\ngit remote add upstream &lt;url&gt;\ngit fetch upstream\ngit checkout master\ngit merge upstream/master\n\n# gitignore\ngit rm --cached &lt;file name&gt;  # To ignore a file that is already checked in\ngit rm --cached -r &lt;folder name&gt;  # To ignore a folder that is already checked in\ngit config --global core.excludesfile ~/.gitignore_global  # And put in the files you want to ignore\n\n# Rewriting git history\ngit rebase upstream/master\ngit rebase &lt;branch name&gt;  # To bring all the commits forward\ngit rebase -i HEAD~n  # n for which changes you want to make\n# reword - to rename a commit\n# drop - to delete a commit\n# squash - to combine commits and give a name\n# fixup - to combine commits and keep the name of base commit\n# edit - to stop execution of rebase and if you wish, split the commits\n# To reorder commits, just change their order in interactive rebase prompt\n</code></pre>"},{"location":"talks/2022/intro-to-open-source/","title":"Introduction to Open Source","text":"<p>My lightning talk on \"Introduction to Open Source\" @ Google I/O Extended organised by GDG Jalandhar.</p> <p>  May 15, 2022   LPU, Jalandhar </p> <p>Or click here to view slides from slideshare.</p>"},{"location":"terminal/dotfiles/","title":"Dotfiles","text":"<p>This blog post contains my daily used dotfiles which I do not wish to lose.</p>"},{"location":"terminal/dotfiles/#gitconfig","title":".gitconfig","text":"<pre><code>[user]\n    name = Sayam753\n    email = sayamkumar753@yahoo.in\n    signingkey = 69184FFA974E07E4\n[core]\n    excludesfile = /Users/user/.gitignore_global\n[commit]\n    gpgsign = true\n[gpg]\n    program = gpg2\n</code></pre>"},{"location":"terminal/dotfiles/#gitignore_global","title":".gitignore_global","text":"<pre><code>.history\n.vscode\n.mypy_cache\n.DS_Store\n</code></pre>"},{"location":"terminal/dotfiles/#pylintrc","title":".pylintrc","text":"<pre><code>[DEFAULT]\ninit-hook=\n    import pylint_venv\n    pylint_venv.inithook()\n</code></pre>"},{"location":"terminal/dotfiles/#tmuxconf","title":".tmux.conf","text":"<pre><code>set -g default-terminal \"xterm-256color\"\nset -g prefix C-z\nunbind C-b\nbind C-z send-prefix\n</code></pre>"},{"location":"terminal/dotfiles/#vscode_settingsjson","title":".vscode_settings.json","text":"<pre><code>{\n    // Editor options\n    \"editor.wordWrap\": \"off\",\n    \"editor.minimap.enabled\": false,\n    \"editor.detectIndentation\": false,\n    \"editor.suggestSelection\": \"first\",\n    // Workbench options\n    \"workbench.settings.editor\": \"json\",\n    \"workbench.sideBar.location\": \"left\",\n    \"workbench.iconTheme\": \"vscode-icons\",\n    \"workbench.settings.openDefaultSettings\": true,\n    // Terminal options\n    \"terminal.integrated.shell.osx\": \"/bin/zsh\",\n    \"terminal.integrated.fontFamily\": \"Inconsolata-g for Powerline\",\n    // Python options\n    \"python.pythonPath\": \"/usr/local/bin/python3\",\n    \"python.venvPath\": \"/Users/user/Env\",\n    // Autopep8 options\n    \"python.languageServer\": \"Microsoft\",\n    \"editor.formatOnSave\": true,\n    \"python.formatting.provider\": \"black\",\n    \"python.formatting.blackPath\": \"/usr/local/bin/black\",\n    \"python.formatting.blackArgs\": [\n        \"--line-length=100\"\n    ],\n    // Pylint options\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"python.linting.pylintUseMinimalCheckers\": false,\n    \"python.linting.pylintPath\": \"/usr/local/bin/pylint\",\n    \"python.linting.pylintCategorySeverity.refactor\": \"Information\",\n    \"python.linting.pylintArgs\": [\n        \"--enable=F, E, C, R, W\",\n    ],\n    // Pydocstyle options\n    \"python.linting.pydocstyleEnabled\": true,\n    \"python.linting.pydocstylePath\": \"/usr/local/bin/pydocstyle\",\n    \"python.linting.pydocstyleArgs\": [\n        \"--convention=numpy\"\n    ],\n    // Mypy options\n    \"python.linting.mypyEnabled\": true,\n    \"python.linting.mypyPath\": \"/usr/local/bin/mypy\",\n    \"python.linting.mypyArgs\": [\n        \"--ignore-missing-imports\"\n    ],\n    // Misc\n    \"git.confirmSync\": false,\n    \"C_Cpp.updateChannel\": \"Insiders\",\n    \"files.exclude\": {\n        \"**/.classpath\": true,\n        \"**/.project\": true,\n        \"**/.settings\": true,\n        \"**/.factorypath\": true,\n        \"**/.history\": true,\n        \"**/.idea\": true,\n        \"**/.mypy_cache\": true,\n        \"**/__pycache__\": true\n    },\n}\n</code></pre>"},{"location":"terminal/dotfiles/#zshrc","title":".zshrc","text":"<pre><code># Customizing prompt\nautoload -Uz vcs_info\nautoload -U colors &amp;&amp; colors\nprecmd() { vcs_info }\n\nzstyle ':vcs_info:git:*' formats '%F{011}(\ue0a0 %b)%f'\n\nsetopt PROMPT_SUBST\nPROMPT='%F{166}Sayam%f:%F{040}%1~%f ${vcs_info_msg_0_}%{$reset_color%}$ '\n\n# Personal configurations\nsource /usr/local/bin/virtualenvwrapper.sh\nexport WORKON_HOME=~/Env\nalias p=python3\nalias jn=\"jupyter notebook\"\n\n## Exports to deal with servers\nexport LANG=\"en_US.UTF-8\"\nexport LC_COLLATE=\"en_US.UTF-8\"\nexport LC_CTYPE=\"en_US.UTF-8\"\nexport LC_MESSAGES=\"en_US.UTF-8\"\nexport LC_MONETARY=\"en_US.UTF-8\"\nexport LC_NUMERIC=\"en_US.UTF-8\"\nexport LC_TIME=\"en_US.UTF-8\"\n</code></pre>"}]}