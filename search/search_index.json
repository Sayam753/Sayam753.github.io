{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Here is my website to hold all my learnings and projects documentations.","title":"Home"},{"location":"#home","text":"Here is my website to hold all my learnings and projects documentations.","title":"Home"},{"location":"django/deploy-django/","text":"Deploy Django \u00b6 In this blog, we will learn how to deploy Django Application with uWSGI and nginx on CentOS\u20197. Django is the most popular python based backend framework with the aim of rapid web development. Before installing anything, I recommend to read my previous post where I have discussed about initial server setup with ssh keys. Installing the prerequisites \u00b6 Installing python3.6 \u00b6 First install the latest packages from EPEL and RPM. EPEL(Extra Packages for Enterprise Linux) is an open source repository that contains the latest packages for Red Hat Linux distributions. RPM is also an open source package management system from Red Hat. After all this, lets install python3 \u2013 sudo yum install -y epel-release sudo yum install -y https://centos7.iuscommunity.org/ius-release.rpm sudo yum update sudo yum install -y python36u python36u-libs python36u-devel python36u-pip python-devel Upgrading pip and installing virtualenv \u00b6 Pip is the most popular python package installer. Virtual environments are used for separating the different versions of any package for different projects. sudo pip3.6 install --upgrade pip sudo pip install virtualenv virtualenvwrapper Configuring the shell \u00b6 We will use Env directory to hold all our virtual environments. This can be configured in .bashrc file. echo \"export WORKON_HOME=~/Env\" >> ~/.bashrc echo \"source /usr/bin/virtualenvwrapper.sh\" >> ~/.bashrc Now, open /usr/bin/virtualenvwrapper.sh with either vim or nano. Find the line \u2013 VIRTUALENVWRAPPER_PYTHON=\"$(command \\which python)\" and replace python with python3.6 as \u2013 VIRTUALENVWRAPPER_PYTHON=\"$(command \\which python3.6)\" Now, lets reflect these changes \u2013 source ~/.bashrc Configuring Django project \u00b6 Creating virtual environments \u00b6 mkvirtualenv env_1 The environment env_1 gets automatically activated. The same can be verified by \u2013 which pip # Output: ~/Env/env_1/bin/pip Copying Django project from local to remote \u00b6 Since, we have activated our virtual env, now we will copy our django project to remote server using scp. If you have uploaded it on github, just install git via yum and then git clone the project. But before doing this, lets grab all the requirements of the project. If your project already contains a requirements file, then you can skip this part. \u201ccd\u201d into your project\u2019s directory and after activating virtual env in your local machine, use the following command to list out requirements in requirements.txt file in your local terminal- pip freeze > requirements.txt Now, to copy the project, use the following command in your local terminal. Do remember to put your ip and user_name configured in previous post. Write the complete path of your django project from root in local machine. This will copy the project in the home directory of the server. scp -r /path/to/project user_name@your_ip_here:~ Now, connect to your server and activate the virtual env. ssh user_name@your_ip_here workon env_1 Let\u2019s install all the requirements for the project. Use the path to requirements.txt file. pip install -r /path/to/requirements.txt Installing MySQL from rpm \u00b6 sudo rpm -ivh https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm Check if Mysql repo has been enabled \u2013 sudo yum repolist all | grep mysql | grep enabled # Output : enabled Install and enable mysql \u2013 sudo yum -y install mysql-community-server sudo systemctl start mysqld sudo systemctl enable mysqld sudo systemctl status mysqld Copy the mysql temporary root password from the command given below and paste this while secure installation of mysql. Change the root password and hit Enter for default actions. cat /var/log/mysqld.log | grep -i 'temporary password' mysql_secure_installation We have successfully installed mysql and now, we need a database to run our project. First, open the mysql interface and enter the root password \u2013 mysql -u root -p Now, in mysql, create a database \u2013 mysql> CREATE DATABASE first_db ; mysql> SHOW DATABASES ; mysql> exit After this, we need to install a client to communicate with mysql \u2013 sudo yum install -y mysql-connector-python.x86_64 mysql-community-devel.x86_64 mysql-cluster-community-client.x86_64 mysql-shell.x86_64 mysql-router.x86_64 gcc pip install mysqlclient # inside the virtual environment Changing settings.py file \u00b6 With everything installed, let\u2019s change some settings for the project \u2013 sudo nano ~/project_name/project_name/settings.py Add the following line to the last of the file. As we will be using nginx to deploy the application, this line tells django to place our static files in \u2018static\u2019 directory. This helps nginx to easily serve these static files. STATIC_ROOT = os . path . join ( BASE_DIR , \"static/\" ) Do not forget to change the default database configurations to \u2013 DATABASES = { 'default' : { 'ENGINE' : 'django.db.backends.mysql' , 'NAME' : 'first_db' , 'USER' : 'root' , 'PASSWORD' : 'your-root-password' , 'HOST' : 'localhost' , 'PORT' : '' , } } And also add your ip in the allowed hosts \u2013 ALLOWED_HOSTS = [ 'your_ip_here' ] Opening port 8000 \u00b6 sudo firewall-cmd --permanent --add-service = http sudo firewall-cmd --permanent --add-port = 8000 /tcp sudo firewall-cmd --complete-reload sudo firewall-cmd --list-all Running the application \u00b6 First, flush out the initial migrations and delete the sqlite database. \u201ccd\u201d into your project\u2019s directory and use the following commands \u2013 find . -path \"*/migrations/*.py\" -not -name \"__init__.py\" -delete find . -path \"*/migrations/*.pyc\" -delete rm -f db.sqlite3 Run the migrations to sync up with database \u2013 python manage.py collectstatic python manage.py makemigrations python manage.py migrate So, finally we can run the server and see the application accessible globally \u2013 python manage.py runserver 0 .0.0.0:8000 Go to the web browser and enter your_ip:8000 to access the django application. Setting up uWSGI and nginx \u00b6 Configuring uWSGI globally \u00b6 Store all the configuration files to /etc/uwsgi/sites. You should use your project name for all configurations \u2013 sudo pip install uwsgi sudo mkdir -p /etc/uwsgi/sites cd /etc/uwsgi/sites sudo nano project_name.ini Add the following lines to the .ini file. Do remember to use your project and user name. [ uwsgi ] project = project_name username = user_name base = /home/% ( username ) chdir = % ( base ) /% ( project ) home = % ( base ) /Env/env_1 module = % ( project ) .wsgi:application master = true processes = 5 uid = % ( username ) socket = /run/uwsgi/% ( project ) .sock chown-socket = % ( username ) :nginx chmod-socket = 660 vacuum = true Ctrl+x to exit and press y to save the changes. Base and home contain the full path for the home directory and virtual environment respectively. We have created a master process for loading our app server. Here, we have used Unix Socket. This socket uses uWSGI protocol which helps nginx to reverse proxy. sudo nano /etc/systemd/system/uwsgi.service Add the following lines. Do remember to use your user name in ExecStartPre of Service section. [ Unit ] Description = uWSGI Emperor service [ Service ] ExecStartPre = /usr/bin/bash -c 'mkdir -p /run/uwsgi; chown user_name:nginx /run/uwsgi' ExecStart = /usr/bin/uwsgi --emperor /etc/uwsgi/sites Restart = always KillSignal = SIGQUIT Type = notify NotifyAccess = all [ Install ] WantedBy = multi-user.target [Unit] section describes our service. [Service] section manages various applications. [Install] section ties up multi-user system state. Configuring Nginx \u00b6 Installing Nginx - sudo yum -y install nginx sudo nano /etc/nginx/nginx.conf Add the following lines. Do remember to use your user name and project name in root and uwsgi_pass. server { listen 8000 ; server_name localhost ; location = favicon.ico { access_log off ; log_not_found off ; } location /static/ { root /home/user_name/project_name ; } location / { include uwsgi_params ; uwsgi_pass unix:/run/uwsgi/project_name.sock ; } } Above, we have set up a server block, with an open port to listen from. We have also specified the static file location and passed all the traffic to unix socket. Make sure the syntax of nginx file is correct and change permissions of the user. sudo nginx -t sudo usermod -a -G user_name nginx chmod 710 /home/user_name Start and enable the nginx and uwsgi. sudo systemctl start nginx sudo systemctl start uwsgi sudo systemctl enable nginx sudo systemctl enable uwsgi Now you can directly access the django application from the ip with an open port. Thanks for reading!","title":"Deploy Django"},{"location":"django/deploy-django/#deploy-django","text":"In this blog, we will learn how to deploy Django Application with uWSGI and nginx on CentOS\u20197. Django is the most popular python based backend framework with the aim of rapid web development. Before installing anything, I recommend to read my previous post where I have discussed about initial server setup with ssh keys.","title":"Deploy Django"},{"location":"django/deploy-django/#installing-the-prerequisites","text":"","title":"Installing the prerequisites"},{"location":"django/deploy-django/#installing-python36","text":"First install the latest packages from EPEL and RPM. EPEL(Extra Packages for Enterprise Linux) is an open source repository that contains the latest packages for Red Hat Linux distributions. RPM is also an open source package management system from Red Hat. After all this, lets install python3 \u2013 sudo yum install -y epel-release sudo yum install -y https://centos7.iuscommunity.org/ius-release.rpm sudo yum update sudo yum install -y python36u python36u-libs python36u-devel python36u-pip python-devel","title":"Installing python3.6"},{"location":"django/deploy-django/#upgrading-pip-and-installing-virtualenv","text":"Pip is the most popular python package installer. Virtual environments are used for separating the different versions of any package for different projects. sudo pip3.6 install --upgrade pip sudo pip install virtualenv virtualenvwrapper","title":"Upgrading pip and installing virtualenv"},{"location":"django/deploy-django/#configuring-the-shell","text":"We will use Env directory to hold all our virtual environments. This can be configured in .bashrc file. echo \"export WORKON_HOME=~/Env\" >> ~/.bashrc echo \"source /usr/bin/virtualenvwrapper.sh\" >> ~/.bashrc Now, open /usr/bin/virtualenvwrapper.sh with either vim or nano. Find the line \u2013 VIRTUALENVWRAPPER_PYTHON=\"$(command \\which python)\" and replace python with python3.6 as \u2013 VIRTUALENVWRAPPER_PYTHON=\"$(command \\which python3.6)\" Now, lets reflect these changes \u2013 source ~/.bashrc","title":"Configuring the shell"},{"location":"django/deploy-django/#configuring-django-project","text":"","title":"Configuring Django project"},{"location":"django/deploy-django/#creating-virtual-environments","text":"mkvirtualenv env_1 The environment env_1 gets automatically activated. The same can be verified by \u2013 which pip # Output: ~/Env/env_1/bin/pip","title":"Creating virtual environments"},{"location":"django/deploy-django/#copying-django-project-from-local-to-remote","text":"Since, we have activated our virtual env, now we will copy our django project to remote server using scp. If you have uploaded it on github, just install git via yum and then git clone the project. But before doing this, lets grab all the requirements of the project. If your project already contains a requirements file, then you can skip this part. \u201ccd\u201d into your project\u2019s directory and after activating virtual env in your local machine, use the following command to list out requirements in requirements.txt file in your local terminal- pip freeze > requirements.txt Now, to copy the project, use the following command in your local terminal. Do remember to put your ip and user_name configured in previous post. Write the complete path of your django project from root in local machine. This will copy the project in the home directory of the server. scp -r /path/to/project user_name@your_ip_here:~ Now, connect to your server and activate the virtual env. ssh user_name@your_ip_here workon env_1 Let\u2019s install all the requirements for the project. Use the path to requirements.txt file. pip install -r /path/to/requirements.txt","title":"Copying Django project from local to remote"},{"location":"django/deploy-django/#installing-mysql-from-rpm","text":"sudo rpm -ivh https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm Check if Mysql repo has been enabled \u2013 sudo yum repolist all | grep mysql | grep enabled # Output : enabled Install and enable mysql \u2013 sudo yum -y install mysql-community-server sudo systemctl start mysqld sudo systemctl enable mysqld sudo systemctl status mysqld Copy the mysql temporary root password from the command given below and paste this while secure installation of mysql. Change the root password and hit Enter for default actions. cat /var/log/mysqld.log | grep -i 'temporary password' mysql_secure_installation We have successfully installed mysql and now, we need a database to run our project. First, open the mysql interface and enter the root password \u2013 mysql -u root -p Now, in mysql, create a database \u2013 mysql> CREATE DATABASE first_db ; mysql> SHOW DATABASES ; mysql> exit After this, we need to install a client to communicate with mysql \u2013 sudo yum install -y mysql-connector-python.x86_64 mysql-community-devel.x86_64 mysql-cluster-community-client.x86_64 mysql-shell.x86_64 mysql-router.x86_64 gcc pip install mysqlclient # inside the virtual environment","title":"Installing MySQL from rpm"},{"location":"django/deploy-django/#changing-settingspy-file","text":"With everything installed, let\u2019s change some settings for the project \u2013 sudo nano ~/project_name/project_name/settings.py Add the following line to the last of the file. As we will be using nginx to deploy the application, this line tells django to place our static files in \u2018static\u2019 directory. This helps nginx to easily serve these static files. STATIC_ROOT = os . path . join ( BASE_DIR , \"static/\" ) Do not forget to change the default database configurations to \u2013 DATABASES = { 'default' : { 'ENGINE' : 'django.db.backends.mysql' , 'NAME' : 'first_db' , 'USER' : 'root' , 'PASSWORD' : 'your-root-password' , 'HOST' : 'localhost' , 'PORT' : '' , } } And also add your ip in the allowed hosts \u2013 ALLOWED_HOSTS = [ 'your_ip_here' ]","title":"Changing settings.py file"},{"location":"django/deploy-django/#opening-port-8000","text":"sudo firewall-cmd --permanent --add-service = http sudo firewall-cmd --permanent --add-port = 8000 /tcp sudo firewall-cmd --complete-reload sudo firewall-cmd --list-all","title":"Opening port 8000"},{"location":"django/deploy-django/#running-the-application","text":"First, flush out the initial migrations and delete the sqlite database. \u201ccd\u201d into your project\u2019s directory and use the following commands \u2013 find . -path \"*/migrations/*.py\" -not -name \"__init__.py\" -delete find . -path \"*/migrations/*.pyc\" -delete rm -f db.sqlite3 Run the migrations to sync up with database \u2013 python manage.py collectstatic python manage.py makemigrations python manage.py migrate So, finally we can run the server and see the application accessible globally \u2013 python manage.py runserver 0 .0.0.0:8000 Go to the web browser and enter your_ip:8000 to access the django application.","title":"Running the application"},{"location":"django/deploy-django/#setting-up-uwsgi-and-nginx","text":"","title":"Setting up uWSGI and nginx"},{"location":"django/deploy-django/#configuring-uwsgi-globally","text":"Store all the configuration files to /etc/uwsgi/sites. You should use your project name for all configurations \u2013 sudo pip install uwsgi sudo mkdir -p /etc/uwsgi/sites cd /etc/uwsgi/sites sudo nano project_name.ini Add the following lines to the .ini file. Do remember to use your project and user name. [ uwsgi ] project = project_name username = user_name base = /home/% ( username ) chdir = % ( base ) /% ( project ) home = % ( base ) /Env/env_1 module = % ( project ) .wsgi:application master = true processes = 5 uid = % ( username ) socket = /run/uwsgi/% ( project ) .sock chown-socket = % ( username ) :nginx chmod-socket = 660 vacuum = true Ctrl+x to exit and press y to save the changes. Base and home contain the full path for the home directory and virtual environment respectively. We have created a master process for loading our app server. Here, we have used Unix Socket. This socket uses uWSGI protocol which helps nginx to reverse proxy. sudo nano /etc/systemd/system/uwsgi.service Add the following lines. Do remember to use your user name in ExecStartPre of Service section. [ Unit ] Description = uWSGI Emperor service [ Service ] ExecStartPre = /usr/bin/bash -c 'mkdir -p /run/uwsgi; chown user_name:nginx /run/uwsgi' ExecStart = /usr/bin/uwsgi --emperor /etc/uwsgi/sites Restart = always KillSignal = SIGQUIT Type = notify NotifyAccess = all [ Install ] WantedBy = multi-user.target [Unit] section describes our service. [Service] section manages various applications. [Install] section ties up multi-user system state.","title":"Configuring uWSGI globally"},{"location":"django/deploy-django/#configuring-nginx","text":"Installing Nginx - sudo yum -y install nginx sudo nano /etc/nginx/nginx.conf Add the following lines. Do remember to use your user name and project name in root and uwsgi_pass. server { listen 8000 ; server_name localhost ; location = favicon.ico { access_log off ; log_not_found off ; } location /static/ { root /home/user_name/project_name ; } location / { include uwsgi_params ; uwsgi_pass unix:/run/uwsgi/project_name.sock ; } } Above, we have set up a server block, with an open port to listen from. We have also specified the static file location and passed all the traffic to unix socket. Make sure the syntax of nginx file is correct and change permissions of the user. sudo nginx -t sudo usermod -a -G user_name nginx chmod 710 /home/user_name Start and enable the nginx and uwsgi. sudo systemctl start nginx sudo systemctl start uwsgi sudo systemctl enable nginx sudo systemctl enable uwsgi Now you can directly access the django application from the ip with an open port. Thanks for reading!","title":"Configuring Nginx"},{"location":"django/initial-server-setup/","text":"Initial Server Setup \u00b6 In this blog, we will be going through initial server setup on Centos7. This is generally recommended before diving straight into production. Centos is an open source Linux distribution under RHEL(Red Hat Enterprise Linux). The reason why Centos is preferred over Ubuntu is because of its stability. Its updates can take about more than 7-8 years to come. Configuring Centos 7 \u00b6 After purchasing a server, login into it using the ssh command in the terminal. Remember to use your public ip and the password given by the administrator. ssh root@your_ip_here If you are getting a warning like \u201cLC_CTYPE: cannot change locale (UTF-8): No such file or directory\u201d, then enter the following command in your local terminal by logging out. The LC variables determine the language of encoding the characters. So, we need to export this variable. logout export LC_ALL = \"en_US.UTF-8\" ssh root@your_ip_here Updating centos \u00b6 sudo yum update sudo yum upgrade Setting up hostname \u00b6 hostnamectl set-hostname centos-server hostname # to check the hostname Setting up hostname in the hosts file \u00b6 sudo nano /etc/hosts Add your ip followed by tab and then type centos-server which is the hostname. Then hit CTRL+x to exit and enter to save the changes. Adding a new user \u00b6 Root has the most privileges in the OS. It can be destructive to operate the server under root user. To limit the scope, we will be creating a new user. In future, if any need arises, we will change the permissions for this user. adduser user_name passwd user_name #setting up password for new user gpasswd -a user_name wheel #adding sudo privileges logout Securing the server \u00b6 There are bots all around trying to find vulnerabilities in the servers. Till now, we have used password based authentication which is highly exploitable. These bots try brute force attacks to enter our server. So to fix this, we will disable password based authentication and setup ssh keys. These ssh keys will be stored in the local machine and the server. After this whenever we try to ssh into our server, it will analyse the keys and give access. So, setting up ssh keys for authentication in the local machine in home directory. Hit enter for default actions \u2013 ssh-keygen -t rsa #generating keys ssh-copy-id user_name@your_ip_here #coping the keys to the server logout Now you can ssh login without password for the new user \u2013 ssh user_name@your_ip_here Change configuration in /etc/ssh/sshd_config , thereby making our server more secure \u2013 PermitRootLogin no PasswordAuthentication no sudo nano /etc/ssh/sshd_config sudo systemctl restart sshd Up next \u2013 Deploy Django Applications on Centos Thanks for reading!","title":"Initial Server Setup"},{"location":"django/initial-server-setup/#initial-server-setup","text":"In this blog, we will be going through initial server setup on Centos7. This is generally recommended before diving straight into production. Centos is an open source Linux distribution under RHEL(Red Hat Enterprise Linux). The reason why Centos is preferred over Ubuntu is because of its stability. Its updates can take about more than 7-8 years to come.","title":"Initial Server Setup"},{"location":"django/initial-server-setup/#configuring-centos-7","text":"After purchasing a server, login into it using the ssh command in the terminal. Remember to use your public ip and the password given by the administrator. ssh root@your_ip_here If you are getting a warning like \u201cLC_CTYPE: cannot change locale (UTF-8): No such file or directory\u201d, then enter the following command in your local terminal by logging out. The LC variables determine the language of encoding the characters. So, we need to export this variable. logout export LC_ALL = \"en_US.UTF-8\" ssh root@your_ip_here","title":"Configuring Centos 7"},{"location":"django/initial-server-setup/#updating-centos","text":"sudo yum update sudo yum upgrade","title":"Updating centos"},{"location":"django/initial-server-setup/#setting-up-hostname","text":"hostnamectl set-hostname centos-server hostname # to check the hostname","title":"Setting up hostname"},{"location":"django/initial-server-setup/#setting-up-hostname-in-the-hosts-file","text":"sudo nano /etc/hosts Add your ip followed by tab and then type centos-server which is the hostname. Then hit CTRL+x to exit and enter to save the changes.","title":"Setting up hostname in the hosts file"},{"location":"django/initial-server-setup/#adding-a-new-user","text":"Root has the most privileges in the OS. It can be destructive to operate the server under root user. To limit the scope, we will be creating a new user. In future, if any need arises, we will change the permissions for this user. adduser user_name passwd user_name #setting up password for new user gpasswd -a user_name wheel #adding sudo privileges logout","title":"Adding a new user"},{"location":"django/initial-server-setup/#securing-the-server","text":"There are bots all around trying to find vulnerabilities in the servers. Till now, we have used password based authentication which is highly exploitable. These bots try brute force attacks to enter our server. So to fix this, we will disable password based authentication and setup ssh keys. These ssh keys will be stored in the local machine and the server. After this whenever we try to ssh into our server, it will analyse the keys and give access. So, setting up ssh keys for authentication in the local machine in home directory. Hit enter for default actions \u2013 ssh-keygen -t rsa #generating keys ssh-copy-id user_name@your_ip_here #coping the keys to the server logout Now you can ssh login without password for the new user \u2013 ssh user_name@your_ip_here Change configuration in /etc/ssh/sshd_config , thereby making our server more secure \u2013 PermitRootLogin no PasswordAuthentication no sudo nano /etc/ssh/sshd_config sudo systemctl restart sshd Up next \u2013 Deploy Django Applications on Centos Thanks for reading!","title":"Securing the server"},{"location":"graph-theory/graph-theory-101/","text":"Graph Theory 101 \u00b6 What is Graph Theory? \u00b6 Graph Theory is an important branch of mathematics. Graphs are mathematical structures which is used for modelling pairwise relationships between objects. In graph theory terms, these objects are called vertices and the relationship two objects is given by edges between them. Graphs are primarily of two types \u2013 Directed Graphs or Undirected Graphs . In directed graphs, edges are directed from one vertex to another. While in undirected graphs, edges connect two vertex so that relation is bidirectional. Directed Graph Undirected Graph In above images, circles represent objects while the lines between them represent edges. Edges in directed graph are denoted by arrows while in undirected graph they are simply drawn as lines. In first image, edge \u201ca\u201d relates object \u201c1\u201d to object \u201c2\u201d but not vice versa. While in second image there is no direction associated with edges. Traversing a graph means moving from one vertex to another using the edges. You can go from one vertex to another only if there is an edge between them. In directed graph we can move only in one direction using an edge. But in undirected graph we can move in both directions. An interesting story \u00b6 The city of K\u00f6nigsberg in Prussia (now Kaliningrad, Russia) was situated on both sides of the Pregel River, and included two large islands \u2013 Kneiphof and Lomse \u2013 which were connected to each other, or to the two mainland portions of the city, by seven bridges. When Carl Gottileb Ehler \u2013 a mathematician became mayor of the nearby town, he kept thinking about a single problem: Is there any route that allows you to cross all the seven bridges without crossing any of them more than once? Here, is the picture of K\u00f6nigsberg. The green shapes denote the bridges. If you are trying to solve the problem then you are out of luck because such a route does not exist. Next we will see how Euler proved that this problem is impossible to solve. Graph Theory is discovered \u00b6 While trying to explain why this problem is impossible to solve, Euler invented a field of mathematics which he called Geometry of Positions . Now, know as Graph Theory . But for understanding his solution we would need to reformulate the question in terms of graph theory. Let\u2019s do that first. We can mark the four lands as our objects \u2013 A, B, C, D and the bridges connecting them as edges. Since we can use bridges to move in either direction, we use undirected graph for this representation. Drawing in more familiar way, here is the resulting graph. So, we can reformulate our problem as follows \u2013 Is there any way for traversing the graph such that we use each and every edge exactly once? Indegree and Outdegree \u00b6 By definition, Indegree of a vertex is the number of edges that are coming towards that vertex and similarly, Outdegree is number of edges going out of a vertex. In case of Undirected Graphs, we do not distinguish between indegree and outdegree and simply refer to number of edges connected to a vertex as its Degree . So, in our graph of K\u00f6nigsberg Bridges, degree of A is 5 while that of B, C and D is 3. Finally the solution \u00b6 We can argue that for such a traversal to exist degree of all the vertices except for maybe 2 vertices must be even. That is excluding any two vertices, all the other vertices must have a even degree for the solution to exist. Why? \u00b6 Because if we enter a vertex then we must also leave it but we cannot use the same edge that we used to enter it, so we must use a different edge. By this argument we can see that for every vertex we visit, there are two edges that we use. The vertex from which we start traversing and the vertex where we end it are exceptions. Because we start traversing from one vertex without entering it, and we can end our traversal in another vertex without leaving it. Therefore this two vertex need not have an even degree. In our K\u00f6nigsberg Bridges graph, all the vertices have odd degree, so we cannot traverse the graph in a way that we cross every edge exactly once. In Euler\u2019s honour, we today call such a traversal of graph as Eulerian path . A different but related problem adds extra condition to this traversal that the starting and ending vertex must be the same. This traversal is called a Eulerian circuit . For Eulerian circuit to exist in the graph, all the vertices must have even degree. (Because this time we must enter back to the vertex from where we started the traversal). Conclusion \u00b6 Graph Theory is a very useful branch of mathematics and has numerous applications in different fields of science. Especially, in computer science graph theory is used extensively in many places. Social Networks use it to represent connections among users. In maps, it is used to represent cities as vertices and roads between them as edges. Also standard graph algorithms like DFS, BFS, Topological Sort are used in various programming tasks. References \u00b6 Seven Bridges of K\u00f6nigsberg \u2013 Wikipedia Graph Theory \u2013 Wikipedia How the K\u00f6nigsberg bridge problem changed mathematics \u2013 Dan Van der Vieren Mathematics | Graph Theory Basics \u2013 Set 1 \u2013 GeeksforGeeks This article has been written by my dear friend Ram Nad .","title":"Graph Theory 101"},{"location":"graph-theory/graph-theory-101/#graph-theory-101","text":"","title":"Graph Theory 101"},{"location":"graph-theory/graph-theory-101/#what-is-graph-theory","text":"Graph Theory is an important branch of mathematics. Graphs are mathematical structures which is used for modelling pairwise relationships between objects. In graph theory terms, these objects are called vertices and the relationship two objects is given by edges between them. Graphs are primarily of two types \u2013 Directed Graphs or Undirected Graphs . In directed graphs, edges are directed from one vertex to another. While in undirected graphs, edges connect two vertex so that relation is bidirectional. Directed Graph Undirected Graph In above images, circles represent objects while the lines between them represent edges. Edges in directed graph are denoted by arrows while in undirected graph they are simply drawn as lines. In first image, edge \u201ca\u201d relates object \u201c1\u201d to object \u201c2\u201d but not vice versa. While in second image there is no direction associated with edges. Traversing a graph means moving from one vertex to another using the edges. You can go from one vertex to another only if there is an edge between them. In directed graph we can move only in one direction using an edge. But in undirected graph we can move in both directions.","title":"What is Graph Theory?"},{"location":"graph-theory/graph-theory-101/#an-interesting-story","text":"The city of K\u00f6nigsberg in Prussia (now Kaliningrad, Russia) was situated on both sides of the Pregel River, and included two large islands \u2013 Kneiphof and Lomse \u2013 which were connected to each other, or to the two mainland portions of the city, by seven bridges. When Carl Gottileb Ehler \u2013 a mathematician became mayor of the nearby town, he kept thinking about a single problem: Is there any route that allows you to cross all the seven bridges without crossing any of them more than once? Here, is the picture of K\u00f6nigsberg. The green shapes denote the bridges. If you are trying to solve the problem then you are out of luck because such a route does not exist. Next we will see how Euler proved that this problem is impossible to solve.","title":"An interesting story"},{"location":"graph-theory/graph-theory-101/#graph-theory-is-discovered","text":"While trying to explain why this problem is impossible to solve, Euler invented a field of mathematics which he called Geometry of Positions . Now, know as Graph Theory . But for understanding his solution we would need to reformulate the question in terms of graph theory. Let\u2019s do that first. We can mark the four lands as our objects \u2013 A, B, C, D and the bridges connecting them as edges. Since we can use bridges to move in either direction, we use undirected graph for this representation. Drawing in more familiar way, here is the resulting graph. So, we can reformulate our problem as follows \u2013 Is there any way for traversing the graph such that we use each and every edge exactly once?","title":"Graph Theory is discovered"},{"location":"graph-theory/graph-theory-101/#indegree-and-outdegree","text":"By definition, Indegree of a vertex is the number of edges that are coming towards that vertex and similarly, Outdegree is number of edges going out of a vertex. In case of Undirected Graphs, we do not distinguish between indegree and outdegree and simply refer to number of edges connected to a vertex as its Degree . So, in our graph of K\u00f6nigsberg Bridges, degree of A is 5 while that of B, C and D is 3.","title":"Indegree and Outdegree"},{"location":"graph-theory/graph-theory-101/#finally-the-solution","text":"We can argue that for such a traversal to exist degree of all the vertices except for maybe 2 vertices must be even. That is excluding any two vertices, all the other vertices must have a even degree for the solution to exist.","title":"Finally the solution"},{"location":"graph-theory/graph-theory-101/#why","text":"Because if we enter a vertex then we must also leave it but we cannot use the same edge that we used to enter it, so we must use a different edge. By this argument we can see that for every vertex we visit, there are two edges that we use. The vertex from which we start traversing and the vertex where we end it are exceptions. Because we start traversing from one vertex without entering it, and we can end our traversal in another vertex without leaving it. Therefore this two vertex need not have an even degree. In our K\u00f6nigsberg Bridges graph, all the vertices have odd degree, so we cannot traverse the graph in a way that we cross every edge exactly once. In Euler\u2019s honour, we today call such a traversal of graph as Eulerian path . A different but related problem adds extra condition to this traversal that the starting and ending vertex must be the same. This traversal is called a Eulerian circuit . For Eulerian circuit to exist in the graph, all the vertices must have even degree. (Because this time we must enter back to the vertex from where we started the traversal).","title":"Why?"},{"location":"graph-theory/graph-theory-101/#conclusion","text":"Graph Theory is a very useful branch of mathematics and has numerous applications in different fields of science. Especially, in computer science graph theory is used extensively in many places. Social Networks use it to represent connections among users. In maps, it is used to represent cities as vertices and roads between them as edges. Also standard graph algorithms like DFS, BFS, Topological Sort are used in various programming tasks.","title":"Conclusion"},{"location":"graph-theory/graph-theory-101/#references","text":"Seven Bridges of K\u00f6nigsberg \u2013 Wikipedia Graph Theory \u2013 Wikipedia How the K\u00f6nigsberg bridge problem changed mathematics \u2013 Dan Van der Vieren Mathematics | Graph Theory Basics \u2013 Set 1 \u2013 GeeksforGeeks This article has been written by my dear friend Ram Nad .","title":"References"},{"location":"gsoc/final-week-into-gsoc/","text":"Final Week into GSoC \u00b6 Hello all. So, this blog is meant to summarize the work done in the last week of GSoC. I will write a separate post covering the entire work done in three months. Stay tuned. I have been working on adding features to ADVI. Here is the PR #310 . Add progress bar support. Test progress bar in different OS. Add ParameterConvergence criteria to test convergence. Add LowRank Approximation. Add LowRank ADVI tests. Update quickstart notebook. The only thing left is including a sample notebook playing around with hierarchical models and VI. I have already written half of it. Hope this PR will get merged soon. See you soon. Thanks Sayam Kumar","title":"Week 10-11 into GSoC"},{"location":"gsoc/final-week-into-gsoc/#final-week-into-gsoc","text":"Hello all. So, this blog is meant to summarize the work done in the last week of GSoC. I will write a separate post covering the entire work done in three months. Stay tuned. I have been working on adding features to ADVI. Here is the PR #310 . Add progress bar support. Test progress bar in different OS. Add ParameterConvergence criteria to test convergence. Add LowRank Approximation. Add LowRank ADVI tests. Update quickstart notebook. The only thing left is including a sample notebook playing around with hierarchical models and VI. I have already written half of it. Hope this PR will get merged soon. See you soon. Thanks Sayam Kumar","title":"Final Week into GSoC"},{"location":"gsoc/gsoc-2020-with-numfocus/","text":"GSoC'20 with NumFOCUS \u00b6 I am super excited to say that I have been selected as a Google Summer of Code student by NumFOCUS for PyMC4. I would like to thank my mentors Thomas Wiecki and Maxim Kochurov and the entire NumFOCUS community for giving this opportunity. My project is about adding Variational Inference Interface to PyMC4 . Variational Inference scales better over larger datasets as compared to the traditional MCMC algorithms. First, I had plans to implement OPVI 1 framework as done in PyMC3 this summer. But as corrected by my mentor Maxim Kochurov , it would have taken extra time and more debugging because of the difficulty to deal with symbolic graph manipulations in Tensorflow . Now, the whole plan is to implement two Variational Inference Algorithms - Mean Field ADVI 2 and Full Rank ADVI 2 in PyMC4. Mean Field ADVI posits a Spherical Gaussian family and Full Rank ADVI posits a Multivariate Gaussian family to minimize KL divergence . I will write a blog post upto next week explaining both these algorithms. All in all, I look forward to a great summer. Operator Variational Inference Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei (2016) \u21a9 Automatic Differentiation Variational Inference Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M. Blei (2016). \u21a9 \u21a9","title":"GSoC'20 with NumFOCUS"},{"location":"gsoc/gsoc-2020-with-numfocus/#gsoc20-with-numfocus","text":"I am super excited to say that I have been selected as a Google Summer of Code student by NumFOCUS for PyMC4. I would like to thank my mentors Thomas Wiecki and Maxim Kochurov and the entire NumFOCUS community for giving this opportunity. My project is about adding Variational Inference Interface to PyMC4 . Variational Inference scales better over larger datasets as compared to the traditional MCMC algorithms. First, I had plans to implement OPVI 1 framework as done in PyMC3 this summer. But as corrected by my mentor Maxim Kochurov , it would have taken extra time and more debugging because of the difficulty to deal with symbolic graph manipulations in Tensorflow . Now, the whole plan is to implement two Variational Inference Algorithms - Mean Field ADVI 2 and Full Rank ADVI 2 in PyMC4. Mean Field ADVI posits a Spherical Gaussian family and Full Rank ADVI posits a Multivariate Gaussian family to minimize KL divergence . I will write a blog post upto next week explaining both these algorithms. All in all, I look forward to a great summer. Operator Variational Inference Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei (2016) \u21a9 Automatic Differentiation Variational Inference Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M. Blei (2016). \u21a9 \u21a9","title":"GSoC'20 with NumFOCUS"},{"location":"gsoc/variational-inference/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); Variational Inference \u00b6 Variational Inference is a powerful algorithm for fitting Bayesian networks. In this blog, you will learn about maths and intuition behind Variational Inference, Mean Field approximation and its implementation in Tensorflow Probability. Intro to Bayesian Networks \u00b6 Random Variables \u00b6 Random Variables are simply variables whose values are uncertain. Eg - In case of flipping a coin $n$ times, a random variable $X$ can be number of heads shown up. In COVID-19 pandemic situation, random variable can be number of patients found positive with virus daily. Probability Distributions \u00b6 Probability Distributions governs the amount of uncertainty of random variables. They have a math function with which they assign probabilities to different values taken by random variables. The associated math function is called probability density function (pdf). For simplicity, let's denote any random variable as $X$ and its corresponding pdf as $P\\left (X\\right )$. Eg - Following figure shows the probability distribution for number of heads when an unbiased coin is flipped 5 times. Bayesian Networks \u00b6 Bayesian Networks are graph based representations to acccount for randomness while modelling our data. The nodes of the graph are random variables and the connections between nodes denote the direct influence from parent to child. Bayesian Network Example \u00b6 Let's say a student is taking a class during school. The difficulty of the class and the intelligence of the student together directly influence student's grades . And the grades affects his/her acceptance to the university. Also, the intelligence factor influences student's SAT score. Keep this example in mind. More formally, Bayesian Networks represent joint probability distribution over all the nodes of graph - $P\\left (X_1, X_2, X_3, ..., X_n\\right )$ or $P\\left (\\bigcap_{i=1}^{n}X_i\\right )$ where $X_i$ is a random variable. Also Bayesian Networks follow local Markov property by which every node in the graph is independent on its non-descendants given its parents . In this way, the joint probability distribution can be decomposed as - $$ P\\left (X_1, X_2, X_3, ..., X_n\\right ) = \\prod_{i=1}^{n} P\\left (X_i | Par\\left (X_i\\right )\\right ) $$ Extra: Proof of decomposition First, let's recall conditional probability, $$P\\left (A|B\\right ) = \\frac{P\\left (A, B\\right )}{P\\left (B\\right )}$$ The above equation is so derived because of reduction of sample space of $A$ when $B$ has already occured. Now, adjusting terms - $$P\\left (A, B\\right ) = P\\left (A|B\\right )*P\\left (B\\right )$$ This equation is called chain rule of probability. Let's generalize this rule for Bayesian Networks. The ordering of names of nodes is such that parent(s) of nodes lie above them (Breadth First Ordering). $$P\\left (X_1, X_2, X_3, ..., X_n\\right ) = P\\left (X_n, X_{n-1}, X_{n-2}, ..., X_1\\right )\\\\ = P\\left (X_n|X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) * P \\left (X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) \\left (Chain Rule\\right )\\\\ = P\\left (X_n|X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) * P \\left (X_{n-1}|X_{n-2}, X_{n-3}, X_{n-4}, ..., X_1\\right ) * P \\left (X_{n-2}, X_{n-3}, X_{n-4}, ..., X_1\\right )$$ Applying chain rule repeatedly, we get the following equation - $$P\\left (\\bigcap_{i=1}^{n}X_i\\right ) = \\prod_{i=1}^{n} P\\left (X_i | P\\left (\\bigcap_{j=1}^{i-1}X_j\\right )\\right )$$ Keep the above equation in mind. Let's bring back Markov property. To bring some intuition behind Markov property, let's reuse Bayesian Network Example . If we say, the student scored very good grades , then it is highly likely the student gets acceptance letter to university. No matter how difficult the class was, how much intelligent the student was, and no matter what his/her SAT score was. The key thing to note here is by observing the node's parent, the influence by non-descendants towards the node gets eliminated. Now, the equation becomes - $$P\\left (\\bigcap_{i=1}^{n}X_i\\right ) = \\prod_{i=1}^{n} P\\left (X_i | Par\\left (X_i\\right )\\right )$$ Bingo, with the above equation, we have proved Factorization Theorem in Probability. The decomposition of running Bayesian Network Example can be written as - $$ P\\left (Difficulty, Intelligence, Grade, SAT, Acceptance Letter\\right ) = P\\left (Difficulty\\right )*P\\left (Intelligence\\right )*\\left (Grade|Difficulty, Intelligence\\right )*P\\left (SAT|Intelligence\\right )*P\\left (Acceptance Letter|Grade\\right ) $$ Why care about Bayesian Networks \u00b6 Bayesian Networks allow us to determine the distribution of parameters given the data (Posterior Distribution). The whole idea is to model the underlying data generative process and estimate unobservable quantities. Regarding this, Bayes formula can be written as - $$ P\\left (\\theta | D\\right ) = \\frac{P\\left (D|\\theta\\right ) * P\\left (\\theta\\right )}{P\\left (D\\right )} $$ $\\theta$ = Parameters of the model $P\\left (\\theta\\right )$ = Prior Distribution over the parameters $P\\left (D|\\theta\\right )$ = Likelihood of the data $P\\left (\\theta|D\\right )$ = Posterior Distribution $P\\left (D\\right )$ = Probability of Data. This term is calculated by marginalising out the effect of parameters. $$ P\\left (D\\right ) = \\int P\\left (D, \\theta\\right ) d\\left (\\theta\\right )\\\\ P\\left (D\\right ) = \\int P\\left (D|\\theta\\right ) P\\left (\\theta\\right ) d\\left (\\theta\\right ) $$ So, the Bayes formula becomes - $$ P\\left (\\theta | D\\right ) = \\frac{P\\left (D|\\theta\\right ) * P\\left (\\theta\\right )}{\\int P\\left (D|\\theta\\right ) P\\left (\\theta\\right ) d\\left (\\theta\\right )} $$ The devil is in the denominator. The integration over all the parameters is intractable . So we resort to sampling and optimization techniques. Intro to Variational Inference \u00b6 Information \u00b6 Variational Inference has its origin in Information Theory. So first, let's understand the basic terms - Information and Entropy . Simply, Information quantifies how much useful the data is. It is related to Probability Distributions as - $$ I = -\\log \\left (P\\left (X\\right )\\right ) $$ The negative sign in the formula has high intuitive meaning. In words, it signifies whenever the probability of certain events is high, the related information is less and vica versa. For example - Consider the statement - It never snows in deserts. The probability of this statement being true is significantly high because we already know that it is hardly possible to snow in deserts. So, the related information is very small. Now consider - There was a snowfall in Sahara Desert in late December 2019. Wow, thats a great news because some unlikely event occured (probability was less). In turn, the information is high. Entropy \u00b6 Entropy quantifies how much average Information is present in occurence of events. It is denoted by $H$. It is named Differential Entropy in case of Real Continuous Domain. $$ H = E_{P\\left (X\\right )} \\left [-\\log\\left (P\\left (X\\right )\\right )\\right ]\\\\ H = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx $$ Entropy of Normal Distribution \u00b6 As an exercise, let's calculate entropy of Normal Distribution. Let's denote $\\mu$ as mean nd $\\sigma$ as standard deviation of Normal Distribution. Remember the results, we will need them further. $$ X \\sim Normal\\left (\\mu, \\sigma^2\\right )\\\\ P_X\\left (x\\right ) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\\\ H = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx $$ Only expanding $\\log\\left (P_X\\left (x\\right )\\right )$ - $$ H = -\\int_X P_X\\left (x\\right ) \\log\\left (\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\right ) dx\\\\ H = -\\frac{1}{2}\\int_X P_X\\left (x\\right ) \\log\\left (\\frac{1}{2 \\pi {\\sigma}^2}\\right )dx - \\int_X P_X\\left (x\\right ) \\log\\left (e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\right ) dx\\\\ H = \\frac{1}{2}\\log \\left ( 2 \\pi {\\sigma}^2 \\right)\\int_X P_X\\left (x\\right ) dx + \\frac{1}{2{\\sigma}^2} \\int_X \\left ( x-\\mu \\right)^2 P_X\\left (x\\right ) dx $$ Identifying terms - $$ \\int_X P_X\\left (x\\right ) dx = 1\\\\ \\int_X \\left ( x-\\mu \\right)^2 P_X\\left (x\\right ) dx = \\sigma^2 $$ Substituting back, the entropy becomes - $$ H = \\frac{1}{2}\\log \\left ( 2 \\pi {\\sigma}^2 \\right) + \\frac{1}{2\\sigma^2} \\sigma^2\\\\ H = \\frac{1}{2}\\left ( \\log \\left ( 2 \\pi {\\sigma}^2 \\right) + 1 \\right ) $$ KL divergence \u00b6 This mathematical tool serves as the backbone of Variational Inference. Kullback\u2013Leibler (KL) divergence measures the mutual information between two probability distributions. Let's say, we have two probability distributions $P$ and $Q$, then KL divergence quantifies how much similar these distributions are. Mathematically, it is just the difference between entropies of probabilities distributions. In terms of notation, $KL(Q||P)$ represents KL divergence with respect to $Q$ against $P$. $$ KL(Q||P) = H_P - H_Q\\\\ = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx + \\int_X Q_X\\left (x\\right ) \\log\\left (Q_X\\left (x\\right )\\right ) dx $$ Changing $-\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx$ to $-\\int_X Q_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx$ as the KL divergence is with respect to $Q$. $$ = -\\int_X Q_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx + \\int_X Q_X\\left (x\\right ) \\log\\left (Q_X\\left (x\\right )\\right ) dx\\\\ = \\int_X Q_X\\left (x \\right) \\log \\left( \\frac{Q_X\\left (x \\right)}{P_X\\left (x \\right)} \\right) dx $$ Remember? We were stuck upon Bayesian Equation because of denominator term but now, we can estimate the posterior distribution $p(\\theta|D)$ by another distribution $q(\\theta)$ over all the parameters of the model. $$ KL(q(\\theta)||p(\\theta|D)) = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta|D)} \\right) d\\theta\\\\ $$ Note If two distributions are similar, then their entropies are similar, implies the KL divergence with respect to two distributions will be smaller. And vica versa. In Variational Inference, the whole idea is to minimize KL divergence so that our approximating distribution $q(\\theta)$ can be made similar to $p(\\theta|D)$. Extra: What are latent variables? If you go about exploring any paper talking about Variational Inference, then most certainly, the papers mention about latent variables instead of parameters. The parameters are fixed quantities for the model whereas latent variables are unobserved quantities of the model conditioned on parameters. Also, we model parameters by probability distributions. For simplicity, let's consider the running terminology of parameters only. Evidence Lower Bound \u00b6 There is again an issue with KL divergence formula as it still involves posterior term i.e. $p(\\theta|D)$. Let's get rid of it - $$ KL(q(\\theta)||p(\\theta|D)) = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta|D)} \\right) d\\theta\\\\ KL = \\int q(\\theta) \\log \\left( \\frac{q(\\theta) p(D)}{p(\\theta, D)} \\right) d\\theta\\\\ KL = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta, D)} \\right) d\\theta + \\int q(\\theta) \\log \\left(p(D) \\right) d\\theta\\\\ KL + \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta = \\log \\left(p(D) \\right) \\int q(\\theta) d\\theta\\\\ $$ Identifying terms - $$ \\int q(\\theta) d\\theta = 1 $$ So, substituting back, our running equation becomes - $$ KL + \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta = \\log \\left(p(D) \\right) $$ The term $\\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta$ is called Evidence Lower Bound (ELBO). The right side of the equation $\\log \\left(p(D) \\right)$ is constant. Observe Minimizing the KL divergence is equivalent to maximizing the ELBO. Also, the ELBO does not depend on posterior distribution. Also, $$ ELBO = \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta\\\\ ELBO = E_{q(\\theta)}\\left [\\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) \\right]\\\\ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + E_{q(\\theta)} \\left [-\\log(q(\\theta)) \\right] $$ The term $E_{q(\\theta)} \\left [-\\log(q(\\theta)) \\right]$ is entropy of $q(\\theta)$. Our running equation becomes - $$ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + H_{q(\\theta)} $$ Mean Field ADVI \u00b6 So far, the whole crux of the story is - To approximate the posterior, maximize the ELBO term. ADVI = Automatic Differentiation Variational Inference. I think the term Automatic Differentiation deals with maximizing the ELBO (or minimizing the negative ELBO) using any autograd differentiation library. Coming to Mean Field ADVI (MF ADVI), we simply assume that the parameters of approximating distribution $q(\\theta)$ are independent and posit Normal distributions over all parameters in transformed space to maximize ELBO. Transformed Space \u00b6 To freely optimize ELBO, without caring about matching the support of model parameters, we transform the support of parameters to Real Coordinate Space. In other words, we optimize ELBO in transformed/unconstrained/unbounded space which automatically maps to minimization of KL divergence in original space. In terms of notation, let's denote a transformation over parameters $\\theta$ as $T$ and the transformed parameters as $\\zeta$. Mathematically, $\\zeta=T(\\theta)$. Also, since we are approximating by Normal Distributions, $q(\\zeta)$ can be written as - $$ q(\\zeta) = \\prod_{i=1}^{k} N(\\zeta_k; \\mu_k, \\sigma^2_k) $$ Now, the transformed joint probability distribution of the model becomes - $$ p\\left (D, \\zeta \\right) = p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta) \\right |\\\\ $$ Extra: Proof of transformation equation To simplify notations, let's use $Y=T(X)$ instead of $\\zeta=T(\\theta)$. After reaching the results, we will put the values back. Also, let's denote cummulative distribution function (cdf) as $F$. There are two cases which respect to properties of function $T$. Case 1 - When $T$ is an increasing function $$F_Y(y) = P(Y < = y) = P(T(X) < = y)\\\\ = P\\left(X < = T^{-1}(y) \\right) = F_X\\left(T^{-1}(y) \\right)\\\\ F_Y(y) = F_X\\left(T^{-1}(y) \\right)$$Let's differentiate with respect to $y$ both sides - $$\\frac{\\mathrm{d} (F_Y(y))}{\\mathrm{d} y} = \\frac{\\mathrm{d} (F_X\\left(T^{-1}(y) \\right))}{\\mathrm{d} y}\\\\ P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}$$ Case 2 - When $T$ is a descreasing function $$F_Y(y) = P(Y < = y) = P(T(X) < = y) = P\\left(X >= T^{-1}(y) \\right)\\\\ = 1-P\\left(X < T^{-1}(y) \\right) = 1-P\\left(X < = T^{-1}(y) \\right) = 1-F_X\\left(T^{-1}(y) \\right)\\\\ F_Y(y) = 1-F_X\\left(T^{-1}(y) \\right)$$Let's differentiate with respect to $y$ both sides - $$\\frac{\\mathrm{d} (F_Y(y))}{\\mathrm{d} y} = \\frac{\\mathrm{d} (1-F_X\\left(T^{-1}(y) \\right))}{\\mathrm{d} y}\\\\ P_Y(y) = (-1) P_X\\left(T^{-1}(y) \\right) (-1) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}\\\\ P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}$$Combining both results - $$P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\left | \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y} \\right |$$Now comes the role of Jacobians to deal with multivariate parameters $X$ and $Y$. $$J_{T^{-1}}(Y) = \\begin{vmatrix} \\frac{\\partial (T_1^{-1})}{\\partial y_1} & ... & \\frac{\\partial (T_1^{-1})}{\\partial y_k}\\\\ . & & .\\\\ . & & .\\\\ \\frac{\\partial (T_k^{-1})}{\\partial y_1} & ... &\\frac{\\partial (T_k^{-1})}{\\partial y_k} \\end{vmatrix}$$Concluding - $$P(Y) = P(T^{-1}(Y)) |det J_{T^{-1}}(Y)|\\\\P(Y) = P(X) |det J_{T^{-1}}(Y)| $$Substitute $X$ as $\\theta$ and $Y$ as $\\zeta$, we will get - $$P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\$$ ELBO in transformed Space \u00b6 Let's bring back the equation formed at ELBO . Expressing ELBO in terms of $\\zeta$ - $$ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + H_{q(\\theta)}\\\\ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta) \\right | \\right) \\right] + H_{q(\\zeta)} $$ Since, we are optimizing ELBO by factorized Normal Distributions, let's bring back the results of Entropy of Normal Distribution . Our running equation becomes - $$ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta) \\right | \\right) \\right] + H_{q(\\zeta)}\\\\ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta) \\right | \\right) \\right] + \\frac{1}{2}\\left ( \\log \\left ( 2 \\pi {\\sigma}^2 \\right) + 1 \\right ) $$ Success The above ELBO equation is the final one which needs to be optimized. Let's Code \u00b6 In [1]: # Imports % matplotlib inline import numpy as np import scipy as sp import pandas as pd import tensorflow as tf from scipy.stats import expon , uniform import arviz as az import pymc3 as pm import matplotlib.pyplot as plt import tensorflow_probability as tfp from pprint import pprint plt . style . use ( \"arviz-darkgrid\" ) from tensorflow_probability.python.mcmc.transformed_kernel import ( make_transform_fn , make_transformed_log_prob ) tfb = tfp . bijectors tfd = tfp . distributions dtype = tf . float32 In [2]: # Plot functions def plot_transformation ( theta , zeta , p_theta , p_zeta ): fig , ( const , trans ) = plt . subplots ( nrows = 2 , ncols = 1 , figsize = ( 6.5 , 12 )) const . plot ( theta , p_theta , color = 'blue' , lw = 2 ) const . set_xlabel ( r \"$\\theta$\" ) const . set_ylabel ( r \"$P(\\theta)$\" ) const . set_title ( \"Constrained Space\" ) trans . plot ( zeta , p_zeta , color = 'blue' , lw = 2 ) trans . set_xlabel ( r \"$\\zeta$\" ) trans . set_ylabel ( r \"$P(\\zeta)$\" ) trans . set_title ( \"Transfomed Space\" ); Transformed Space Example-1 \u00b6 Transformation of Standard Exponential Distribution $$ P_X(x) = e^{-x} $$ The support of Exponential Distribution is $x>=0$. Let's use log transformation to map the support to real number line. Mathematically, $\\zeta=\\log(\\theta)$. Now, let's bring back our transformed joint probability distribution equation - $$ P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\ P(\\zeta) = P(e^{\\zeta}) * e^{\\zeta} $$ Converting this directly into Python code - In [3]: theta = np . linspace ( 0 , 5 , 100 ) zeta = np . linspace ( - 5 , 5 , 100 ) dist = expon () p_theta = dist . pdf ( theta ) p_zeta = dist . pdf ( np . exp ( zeta )) * np . exp ( zeta ) plot_transformation ( theta , zeta , p_theta , p_zeta ) Transformed Space Example-2 \u00b6 Transformation of Uniform Distribution (with support $0<=x<=1$) $$ P_X(x) = 1 $$ Let's use logit or inverse sigmoid transformation to map the support to real number line. Mathematically, $\\zeta=logit(\\theta)$. $$ P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\ P(\\zeta) = P(sig(\\zeta)) * sig(\\zeta) * (1-sig(\\zeta)) $$ where $sig$ is the sigmoid function. Converting this directly into Python code - In [4]: theta = np . linspace ( 0 , 1 , 100 ) zeta = np . linspace ( - 5 , 5 , 100 ) dist = uniform () p_theta = dist . pdf ( theta ) sigmoid = sp . special . expit p_zeta = dist . pdf ( sigmoid ( zeta )) * sigmoid ( zeta ) * ( 1 - sigmoid ( zeta )) plot_transformation ( theta , zeta , p_theta , p_zeta ) Mean Field ADVI Example \u00b6 Infer $\\mu$ and $\\sigma$ for Normal distribution. In [5]: # Generating data mu = 12 sigma = 2.2 data = np . random . normal ( mu , sigma , size = 200 ) In [6]: # Defining the model model = tfd . JointDistributionSequential ([ # sigma_prior tfd . Exponential ( 1 , name = 'sigma' ), # mu_prior tfd . Normal ( loc = 0 , scale = 10 , name = 'mu' ), # likelihood lambda mu , sigma : tfd . Normal ( loc = mu , scale = sigma ) ]) In [7]: print ( model . resolve_graph ()) (('sigma', ()), ('mu', ()), ('x', ('mu', 'sigma'))) In [8]: # Let's generate joint log probability joint_log_prob = lambda * x : model . log_prob ( x + ( data ,)) In [9]: # Build Mean Field ADVI def build_mf_advi (): parameters = model . sample ( 1 ) parameters . pop () dists = [] for i , parameter in enumerate ( parameters ): shape = parameter [ 0 ] . shape loc = tf . Variable ( tf . random . normal ( shape , dtype = dtype ), name = f 'meanfield_ { i } _loc' , dtype = dtype ) scale = tfp . util . TransformedVariable ( tf . fill ( shape , value = tf . constant ( 0.02 , dtype = dtype )), tfb . Softplus (), # For positive values of scale name = f 'meanfield_ { i } _scale' ) approx_parameter = tfd . Normal ( loc = loc , scale = scale ) dists . append ( approx_parameter ) return tfd . JointDistributionSequential ( dists ) meanfield_advi = build_mf_advi () TFP handles transformations differently as it transforms unconstrained space to match the support of distributions. In [10]: unconstraining_bijectors = [ tfb . Exp (), tfb . Identity () ] posterior = make_transformed_log_prob ( joint_log_prob , unconstraining_bijectors , direction = 'forward' , enable_bijector_caching = False ) In [11]: opt = tf . optimizers . Adam ( learning_rate =. 1 ) @tf . function ( autograph = False ) def run_approximation (): elbo_loss = tfp . vi . fit_surrogate_posterior ( posterior , surrogate_posterior = meanfield_advi , optimizer = opt , sample_size = 200 , num_steps = 10000 ) return elbo_loss elbo_loss = run_approximation () WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow_probability/python/math/minimize.py:74: calling <lambda> (from tensorflow_probability.python.vi.optimization) with loss is deprecated and will be removed after 2020-07-01. Instructions for updating: The signature for `trace_fn`s passed to `minimize` has changed. Trace functions now take a single `traceable_quantities` argument, which is a `tfp.math.MinimizeTraceableQuantities` namedtuple containing `traceable_quantities.loss`, `traceable_quantities.gradients`, etc. Please update your `trace_fn` definition. In [12]: plt . plot ( elbo_loss , color = 'blue' ) plt . yscale ( \"log\" ) plt . xlabel ( \"No of iterations\" ) plt . ylabel ( \"Negative ELBO\" ) plt . show () In [13]: graph_info = model . resolve_graph () approx_param = dict () free_param = meanfield_advi . trainable_variables for i , ( rvname , param ) in enumerate ( graph_info [: - 1 ]): approx_param [ rvname ] = { \"mu\" : free_param [ i * 2 ] . numpy (), \"sd\" : free_param [ i * 2 + 1 ] . numpy ()} In [14]: print ( approx_param ) {'sigma': {'mu': 0.7740287, 'sd': -0.7494337}, 'mu': {'mu': 11.233825, 'sd': 1.7977774}} We got pretty good estimates of sigma and mu. We need to transform sigma via exp and I believe it will be something close to 2.2 Drawbacks of this blog post \u00b6 I have not used consistent notation for probability density functions (pdfs). Because I like equations handled this way. Coming up with more good examples using minibatches. The ADVI papers also mention Elliptical standardization and Adaptive step size for optimizers. I have not understood those sections well and thus, haven't tried to implement them. References \u00b6 Chapter 1 and 2: Probabilistic Graphical Model Book Blog Post: An Introduction to Probability and Computational Bayesian Statistics by Ericmjl Section 10.1: Variational Inference Pattern Recognition and Machine Learning Book Section 2.5: Transformations Statistical Theory and Inference Book YouTube: Variational Inference in Python by Austin Rochford PyMC4: Basic Usage Notebook TFP: Joint Modelling Notebook Papers: Automatic Differentiation Variational Inference . Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and Blei, D. M. (2016). Automatic Variational Inference in Stan . Kucukelbir, A., Ranganath, R., Gelman, A., & Blei, D. (2015). Special Thanks \u00b6 Website: codecogs.com to help me generate LaTeX equations. Comments: #1 and #2 by Luciano Paz that cleared my all doubts regarding transformations. if (!document.getElementById(\"mathjaxscript_pelican_#%@#$@#\")) { var mathjaxscript = document.createElement(\"script\"); mathjaxscript.id = \"mathjaxscript_pelican_#%@#$@#\"; mathjaxscript.type = \"text/javascript\"; mathjaxscript.src = \"//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\"; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js', 'AMSsymbols.js', 'noErrors.js', 'noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX', 'input/MathML', 'output/HTML-CSS'],\" + \" extensions: ['tex2jax.js', 'mml2jax.js', 'MathMenu.js', 'MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$', '$'] ], \" + \" displayMath: [ ['$$', '$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName(\"head\")[0]).appendChild(mathjaxscript); }","title":"Variational Inference"},{"location":"gsoc/variational-inference/#variational-inference","text":"Variational Inference is a powerful algorithm for fitting Bayesian networks. In this blog, you will learn about maths and intuition behind Variational Inference, Mean Field approximation and its implementation in Tensorflow Probability.","title":"Variational Inference"},{"location":"gsoc/variational-inference/#intro-to-bayesian-networks","text":"","title":"Intro to Bayesian Networks"},{"location":"gsoc/variational-inference/#random-variables","text":"Random Variables are simply variables whose values are uncertain. Eg - In case of flipping a coin $n$ times, a random variable $X$ can be number of heads shown up. In COVID-19 pandemic situation, random variable can be number of patients found positive with virus daily.","title":"Random Variables"},{"location":"gsoc/variational-inference/#probability-distributions","text":"Probability Distributions governs the amount of uncertainty of random variables. They have a math function with which they assign probabilities to different values taken by random variables. The associated math function is called probability density function (pdf). For simplicity, let's denote any random variable as $X$ and its corresponding pdf as $P\\left (X\\right )$. Eg - Following figure shows the probability distribution for number of heads when an unbiased coin is flipped 5 times.","title":"Probability Distributions"},{"location":"gsoc/variational-inference/#bayesian-networks","text":"Bayesian Networks are graph based representations to acccount for randomness while modelling our data. The nodes of the graph are random variables and the connections between nodes denote the direct influence from parent to child.","title":"Bayesian Networks"},{"location":"gsoc/variational-inference/#bayesian-network-example","text":"Let's say a student is taking a class during school. The difficulty of the class and the intelligence of the student together directly influence student's grades . And the grades affects his/her acceptance to the university. Also, the intelligence factor influences student's SAT score. Keep this example in mind. More formally, Bayesian Networks represent joint probability distribution over all the nodes of graph - $P\\left (X_1, X_2, X_3, ..., X_n\\right )$ or $P\\left (\\bigcap_{i=1}^{n}X_i\\right )$ where $X_i$ is a random variable. Also Bayesian Networks follow local Markov property by which every node in the graph is independent on its non-descendants given its parents . In this way, the joint probability distribution can be decomposed as - $$ P\\left (X_1, X_2, X_3, ..., X_n\\right ) = \\prod_{i=1}^{n} P\\left (X_i | Par\\left (X_i\\right )\\right ) $$ Extra: Proof of decomposition First, let's recall conditional probability, $$P\\left (A|B\\right ) = \\frac{P\\left (A, B\\right )}{P\\left (B\\right )}$$ The above equation is so derived because of reduction of sample space of $A$ when $B$ has already occured. Now, adjusting terms - $$P\\left (A, B\\right ) = P\\left (A|B\\right )*P\\left (B\\right )$$ This equation is called chain rule of probability. Let's generalize this rule for Bayesian Networks. The ordering of names of nodes is such that parent(s) of nodes lie above them (Breadth First Ordering). $$P\\left (X_1, X_2, X_3, ..., X_n\\right ) = P\\left (X_n, X_{n-1}, X_{n-2}, ..., X_1\\right )\\\\ = P\\left (X_n|X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) * P \\left (X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) \\left (Chain Rule\\right )\\\\ = P\\left (X_n|X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) * P \\left (X_{n-1}|X_{n-2}, X_{n-3}, X_{n-4}, ..., X_1\\right ) * P \\left (X_{n-2}, X_{n-3}, X_{n-4}, ..., X_1\\right )$$ Applying chain rule repeatedly, we get the following equation - $$P\\left (\\bigcap_{i=1}^{n}X_i\\right ) = \\prod_{i=1}^{n} P\\left (X_i | P\\left (\\bigcap_{j=1}^{i-1}X_j\\right )\\right )$$ Keep the above equation in mind. Let's bring back Markov property. To bring some intuition behind Markov property, let's reuse Bayesian Network Example . If we say, the student scored very good grades , then it is highly likely the student gets acceptance letter to university. No matter how difficult the class was, how much intelligent the student was, and no matter what his/her SAT score was. The key thing to note here is by observing the node's parent, the influence by non-descendants towards the node gets eliminated. Now, the equation becomes - $$P\\left (\\bigcap_{i=1}^{n}X_i\\right ) = \\prod_{i=1}^{n} P\\left (X_i | Par\\left (X_i\\right )\\right )$$ Bingo, with the above equation, we have proved Factorization Theorem in Probability. The decomposition of running Bayesian Network Example can be written as - $$ P\\left (Difficulty, Intelligence, Grade, SAT, Acceptance Letter\\right ) = P\\left (Difficulty\\right )*P\\left (Intelligence\\right )*\\left (Grade|Difficulty, Intelligence\\right )*P\\left (SAT|Intelligence\\right )*P\\left (Acceptance Letter|Grade\\right ) $$","title":"Bayesian Network Example"},{"location":"gsoc/variational-inference/#why-care-about-bayesian-networks","text":"Bayesian Networks allow us to determine the distribution of parameters given the data (Posterior Distribution). The whole idea is to model the underlying data generative process and estimate unobservable quantities. Regarding this, Bayes formula can be written as - $$ P\\left (\\theta | D\\right ) = \\frac{P\\left (D|\\theta\\right ) * P\\left (\\theta\\right )}{P\\left (D\\right )} $$ $\\theta$ = Parameters of the model $P\\left (\\theta\\right )$ = Prior Distribution over the parameters $P\\left (D|\\theta\\right )$ = Likelihood of the data $P\\left (\\theta|D\\right )$ = Posterior Distribution $P\\left (D\\right )$ = Probability of Data. This term is calculated by marginalising out the effect of parameters. $$ P\\left (D\\right ) = \\int P\\left (D, \\theta\\right ) d\\left (\\theta\\right )\\\\ P\\left (D\\right ) = \\int P\\left (D|\\theta\\right ) P\\left (\\theta\\right ) d\\left (\\theta\\right ) $$ So, the Bayes formula becomes - $$ P\\left (\\theta | D\\right ) = \\frac{P\\left (D|\\theta\\right ) * P\\left (\\theta\\right )}{\\int P\\left (D|\\theta\\right ) P\\left (\\theta\\right ) d\\left (\\theta\\right )} $$ The devil is in the denominator. The integration over all the parameters is intractable . So we resort to sampling and optimization techniques.","title":"Why care about Bayesian Networks"},{"location":"gsoc/variational-inference/#intro-to-variational-inference","text":"","title":"Intro to Variational Inference"},{"location":"gsoc/variational-inference/#information","text":"Variational Inference has its origin in Information Theory. So first, let's understand the basic terms - Information and Entropy . Simply, Information quantifies how much useful the data is. It is related to Probability Distributions as - $$ I = -\\log \\left (P\\left (X\\right )\\right ) $$ The negative sign in the formula has high intuitive meaning. In words, it signifies whenever the probability of certain events is high, the related information is less and vica versa. For example - Consider the statement - It never snows in deserts. The probability of this statement being true is significantly high because we already know that it is hardly possible to snow in deserts. So, the related information is very small. Now consider - There was a snowfall in Sahara Desert in late December 2019. Wow, thats a great news because some unlikely event occured (probability was less). In turn, the information is high.","title":"Information"},{"location":"gsoc/variational-inference/#entropy","text":"Entropy quantifies how much average Information is present in occurence of events. It is denoted by $H$. It is named Differential Entropy in case of Real Continuous Domain. $$ H = E_{P\\left (X\\right )} \\left [-\\log\\left (P\\left (X\\right )\\right )\\right ]\\\\ H = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx $$","title":"Entropy"},{"location":"gsoc/variational-inference/#entropy-of-normal-distribution","text":"As an exercise, let's calculate entropy of Normal Distribution. Let's denote $\\mu$ as mean nd $\\sigma$ as standard deviation of Normal Distribution. Remember the results, we will need them further. $$ X \\sim Normal\\left (\\mu, \\sigma^2\\right )\\\\ P_X\\left (x\\right ) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\\\ H = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx $$ Only expanding $\\log\\left (P_X\\left (x\\right )\\right )$ - $$ H = -\\int_X P_X\\left (x\\right ) \\log\\left (\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\right ) dx\\\\ H = -\\frac{1}{2}\\int_X P_X\\left (x\\right ) \\log\\left (\\frac{1}{2 \\pi {\\sigma}^2}\\right )dx - \\int_X P_X\\left (x\\right ) \\log\\left (e^{ - \\frac{1}{2} \\left ({\\frac{x- \\mu}{ \\sigma}}\\right )^2}\\right ) dx\\\\ H = \\frac{1}{2}\\log \\left ( 2 \\pi {\\sigma}^2 \\right)\\int_X P_X\\left (x\\right ) dx + \\frac{1}{2{\\sigma}^2} \\int_X \\left ( x-\\mu \\right)^2 P_X\\left (x\\right ) dx $$ Identifying terms - $$ \\int_X P_X\\left (x\\right ) dx = 1\\\\ \\int_X \\left ( x-\\mu \\right)^2 P_X\\left (x\\right ) dx = \\sigma^2 $$ Substituting back, the entropy becomes - $$ H = \\frac{1}{2}\\log \\left ( 2 \\pi {\\sigma}^2 \\right) + \\frac{1}{2\\sigma^2} \\sigma^2\\\\ H = \\frac{1}{2}\\left ( \\log \\left ( 2 \\pi {\\sigma}^2 \\right) + 1 \\right ) $$","title":"Entropy of Normal Distribution"},{"location":"gsoc/variational-inference/#kl-divergence","text":"This mathematical tool serves as the backbone of Variational Inference. Kullback\u2013Leibler (KL) divergence measures the mutual information between two probability distributions. Let's say, we have two probability distributions $P$ and $Q$, then KL divergence quantifies how much similar these distributions are. Mathematically, it is just the difference between entropies of probabilities distributions. In terms of notation, $KL(Q||P)$ represents KL divergence with respect to $Q$ against $P$. $$ KL(Q||P) = H_P - H_Q\\\\ = -\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx + \\int_X Q_X\\left (x\\right ) \\log\\left (Q_X\\left (x\\right )\\right ) dx $$ Changing $-\\int_X P_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx$ to $-\\int_X Q_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx$ as the KL divergence is with respect to $Q$. $$ = -\\int_X Q_X\\left (x\\right ) \\log\\left (P_X\\left (x\\right )\\right ) dx + \\int_X Q_X\\left (x\\right ) \\log\\left (Q_X\\left (x\\right )\\right ) dx\\\\ = \\int_X Q_X\\left (x \\right) \\log \\left( \\frac{Q_X\\left (x \\right)}{P_X\\left (x \\right)} \\right) dx $$ Remember? We were stuck upon Bayesian Equation because of denominator term but now, we can estimate the posterior distribution $p(\\theta|D)$ by another distribution $q(\\theta)$ over all the parameters of the model. $$ KL(q(\\theta)||p(\\theta|D)) = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta|D)} \\right) d\\theta\\\\ $$ Note If two distributions are similar, then their entropies are similar, implies the KL divergence with respect to two distributions will be smaller. And vica versa. In Variational Inference, the whole idea is to minimize KL divergence so that our approximating distribution $q(\\theta)$ can be made similar to $p(\\theta|D)$. Extra: What are latent variables? If you go about exploring any paper talking about Variational Inference, then most certainly, the papers mention about latent variables instead of parameters. The parameters are fixed quantities for the model whereas latent variables are unobserved quantities of the model conditioned on parameters. Also, we model parameters by probability distributions. For simplicity, let's consider the running terminology of parameters only.","title":"KL divergence"},{"location":"gsoc/variational-inference/#evidence-lower-bound","text":"There is again an issue with KL divergence formula as it still involves posterior term i.e. $p(\\theta|D)$. Let's get rid of it - $$ KL(q(\\theta)||p(\\theta|D)) = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta|D)} \\right) d\\theta\\\\ KL = \\int q(\\theta) \\log \\left( \\frac{q(\\theta) p(D)}{p(\\theta, D)} \\right) d\\theta\\\\ KL = \\int q(\\theta) \\log \\left( \\frac{q(\\theta)}{p(\\theta, D)} \\right) d\\theta + \\int q(\\theta) \\log \\left(p(D) \\right) d\\theta\\\\ KL + \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta = \\log \\left(p(D) \\right) \\int q(\\theta) d\\theta\\\\ $$ Identifying terms - $$ \\int q(\\theta) d\\theta = 1 $$ So, substituting back, our running equation becomes - $$ KL + \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta = \\log \\left(p(D) \\right) $$ The term $\\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta$ is called Evidence Lower Bound (ELBO). The right side of the equation $\\log \\left(p(D) \\right)$ is constant. Observe Minimizing the KL divergence is equivalent to maximizing the ELBO. Also, the ELBO does not depend on posterior distribution. Also, $$ ELBO = \\int q(\\theta) \\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) d\\theta\\\\ ELBO = E_{q(\\theta)}\\left [\\log \\left( \\frac{p(\\theta, D)}{q(\\theta)} \\right) \\right]\\\\ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + E_{q(\\theta)} \\left [-\\log(q(\\theta)) \\right] $$ The term $E_{q(\\theta)} \\left [-\\log(q(\\theta)) \\right]$ is entropy of $q(\\theta)$. Our running equation becomes - $$ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + H_{q(\\theta)} $$","title":"Evidence Lower Bound"},{"location":"gsoc/variational-inference/#mean-field-advi","text":"So far, the whole crux of the story is - To approximate the posterior, maximize the ELBO term. ADVI = Automatic Differentiation Variational Inference. I think the term Automatic Differentiation deals with maximizing the ELBO (or minimizing the negative ELBO) using any autograd differentiation library. Coming to Mean Field ADVI (MF ADVI), we simply assume that the parameters of approximating distribution $q(\\theta)$ are independent and posit Normal distributions over all parameters in transformed space to maximize ELBO.","title":"Mean Field ADVI"},{"location":"gsoc/variational-inference/#transformed-space","text":"To freely optimize ELBO, without caring about matching the support of model parameters, we transform the support of parameters to Real Coordinate Space. In other words, we optimize ELBO in transformed/unconstrained/unbounded space which automatically maps to minimization of KL divergence in original space. In terms of notation, let's denote a transformation over parameters $\\theta$ as $T$ and the transformed parameters as $\\zeta$. Mathematically, $\\zeta=T(\\theta)$. Also, since we are approximating by Normal Distributions, $q(\\zeta)$ can be written as - $$ q(\\zeta) = \\prod_{i=1}^{k} N(\\zeta_k; \\mu_k, \\sigma^2_k) $$ Now, the transformed joint probability distribution of the model becomes - $$ p\\left (D, \\zeta \\right) = p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta) \\right |\\\\ $$ Extra: Proof of transformation equation To simplify notations, let's use $Y=T(X)$ instead of $\\zeta=T(\\theta)$. After reaching the results, we will put the values back. Also, let's denote cummulative distribution function (cdf) as $F$. There are two cases which respect to properties of function $T$. Case 1 - When $T$ is an increasing function $$F_Y(y) = P(Y < = y) = P(T(X) < = y)\\\\ = P\\left(X < = T^{-1}(y) \\right) = F_X\\left(T^{-1}(y) \\right)\\\\ F_Y(y) = F_X\\left(T^{-1}(y) \\right)$$Let's differentiate with respect to $y$ both sides - $$\\frac{\\mathrm{d} (F_Y(y))}{\\mathrm{d} y} = \\frac{\\mathrm{d} (F_X\\left(T^{-1}(y) \\right))}{\\mathrm{d} y}\\\\ P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}$$ Case 2 - When $T$ is a descreasing function $$F_Y(y) = P(Y < = y) = P(T(X) < = y) = P\\left(X >= T^{-1}(y) \\right)\\\\ = 1-P\\left(X < T^{-1}(y) \\right) = 1-P\\left(X < = T^{-1}(y) \\right) = 1-F_X\\left(T^{-1}(y) \\right)\\\\ F_Y(y) = 1-F_X\\left(T^{-1}(y) \\right)$$Let's differentiate with respect to $y$ both sides - $$\\frac{\\mathrm{d} (F_Y(y))}{\\mathrm{d} y} = \\frac{\\mathrm{d} (1-F_X\\left(T^{-1}(y) \\right))}{\\mathrm{d} y}\\\\ P_Y(y) = (-1) P_X\\left(T^{-1}(y) \\right) (-1) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}\\\\ P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y}$$Combining both results - $$P_Y(y) = P_X\\left(T^{-1}(y) \\right) \\left | \\frac{\\mathrm{d} (T^{-1}(y))}{\\mathrm{d} y} \\right |$$Now comes the role of Jacobians to deal with multivariate parameters $X$ and $Y$. $$J_{T^{-1}}(Y) = \\begin{vmatrix} \\frac{\\partial (T_1^{-1})}{\\partial y_1} & ... & \\frac{\\partial (T_1^{-1})}{\\partial y_k}\\\\ . & & .\\\\ . & & .\\\\ \\frac{\\partial (T_k^{-1})}{\\partial y_1} & ... &\\frac{\\partial (T_k^{-1})}{\\partial y_k} \\end{vmatrix}$$Concluding - $$P(Y) = P(T^{-1}(Y)) |det J_{T^{-1}}(Y)|\\\\P(Y) = P(X) |det J_{T^{-1}}(Y)| $$Substitute $X$ as $\\theta$ and $Y$ as $\\zeta$, we will get - $$P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\$$","title":"Transformed Space"},{"location":"gsoc/variational-inference/#elbo-in-transformed-space","text":"Let's bring back the equation formed at ELBO . Expressing ELBO in terms of $\\zeta$ - $$ ELBO = E_{q(\\theta)}\\left [\\log \\left(p(\\theta, D) \\right) \\right] + H_{q(\\theta)}\\\\ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta) \\right | \\right) \\right] + H_{q(\\zeta)} $$ Since, we are optimizing ELBO by factorized Normal Distributions, let's bring back the results of Entropy of Normal Distribution . Our running equation becomes - $$ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta) \\right | \\right) \\right] + H_{q(\\zeta)}\\\\ ELBO = E_{q(\\zeta)}\\left [\\log \\left(p\\left (D, T^{-1}\\left (\\zeta \\right) \\right) \\left | det J_{T^{-1}}(\\zeta) \\right | \\right) \\right] + \\frac{1}{2}\\left ( \\log \\left ( 2 \\pi {\\sigma}^2 \\right) + 1 \\right ) $$ Success The above ELBO equation is the final one which needs to be optimized.","title":"ELBO in transformed Space"},{"location":"gsoc/variational-inference/#lets-code","text":"In [1]: # Imports % matplotlib inline import numpy as np import scipy as sp import pandas as pd import tensorflow as tf from scipy.stats import expon , uniform import arviz as az import pymc3 as pm import matplotlib.pyplot as plt import tensorflow_probability as tfp from pprint import pprint plt . style . use ( \"arviz-darkgrid\" ) from tensorflow_probability.python.mcmc.transformed_kernel import ( make_transform_fn , make_transformed_log_prob ) tfb = tfp . bijectors tfd = tfp . distributions dtype = tf . float32 In [2]: # Plot functions def plot_transformation ( theta , zeta , p_theta , p_zeta ): fig , ( const , trans ) = plt . subplots ( nrows = 2 , ncols = 1 , figsize = ( 6.5 , 12 )) const . plot ( theta , p_theta , color = 'blue' , lw = 2 ) const . set_xlabel ( r \"$\\theta$\" ) const . set_ylabel ( r \"$P(\\theta)$\" ) const . set_title ( \"Constrained Space\" ) trans . plot ( zeta , p_zeta , color = 'blue' , lw = 2 ) trans . set_xlabel ( r \"$\\zeta$\" ) trans . set_ylabel ( r \"$P(\\zeta)$\" ) trans . set_title ( \"Transfomed Space\" );","title":"Let's Code"},{"location":"gsoc/variational-inference/#transformed-space-example-1","text":"Transformation of Standard Exponential Distribution $$ P_X(x) = e^{-x} $$ The support of Exponential Distribution is $x>=0$. Let's use log transformation to map the support to real number line. Mathematically, $\\zeta=\\log(\\theta)$. Now, let's bring back our transformed joint probability distribution equation - $$ P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\ P(\\zeta) = P(e^{\\zeta}) * e^{\\zeta} $$ Converting this directly into Python code - In [3]: theta = np . linspace ( 0 , 5 , 100 ) zeta = np . linspace ( - 5 , 5 , 100 ) dist = expon () p_theta = dist . pdf ( theta ) p_zeta = dist . pdf ( np . exp ( zeta )) * np . exp ( zeta ) plot_transformation ( theta , zeta , p_theta , p_zeta )","title":"Transformed Space Example-1"},{"location":"gsoc/variational-inference/#transformed-space-example-2","text":"Transformation of Uniform Distribution (with support $0<=x<=1$) $$ P_X(x) = 1 $$ Let's use logit or inverse sigmoid transformation to map the support to real number line. Mathematically, $\\zeta=logit(\\theta)$. $$ P(\\zeta) = P(T^{-1}(\\zeta)) |det J_{T^{-1}}(\\zeta)|\\\\ P(\\zeta) = P(sig(\\zeta)) * sig(\\zeta) * (1-sig(\\zeta)) $$ where $sig$ is the sigmoid function. Converting this directly into Python code - In [4]: theta = np . linspace ( 0 , 1 , 100 ) zeta = np . linspace ( - 5 , 5 , 100 ) dist = uniform () p_theta = dist . pdf ( theta ) sigmoid = sp . special . expit p_zeta = dist . pdf ( sigmoid ( zeta )) * sigmoid ( zeta ) * ( 1 - sigmoid ( zeta )) plot_transformation ( theta , zeta , p_theta , p_zeta )","title":"Transformed Space Example-2"},{"location":"gsoc/variational-inference/#mean-field-advi-example","text":"Infer $\\mu$ and $\\sigma$ for Normal distribution. In [5]: # Generating data mu = 12 sigma = 2.2 data = np . random . normal ( mu , sigma , size = 200 ) In [6]: # Defining the model model = tfd . JointDistributionSequential ([ # sigma_prior tfd . Exponential ( 1 , name = 'sigma' ), # mu_prior tfd . Normal ( loc = 0 , scale = 10 , name = 'mu' ), # likelihood lambda mu , sigma : tfd . Normal ( loc = mu , scale = sigma ) ]) In [7]: print ( model . resolve_graph ()) (('sigma', ()), ('mu', ()), ('x', ('mu', 'sigma'))) In [8]: # Let's generate joint log probability joint_log_prob = lambda * x : model . log_prob ( x + ( data ,)) In [9]: # Build Mean Field ADVI def build_mf_advi (): parameters = model . sample ( 1 ) parameters . pop () dists = [] for i , parameter in enumerate ( parameters ): shape = parameter [ 0 ] . shape loc = tf . Variable ( tf . random . normal ( shape , dtype = dtype ), name = f 'meanfield_ { i } _loc' , dtype = dtype ) scale = tfp . util . TransformedVariable ( tf . fill ( shape , value = tf . constant ( 0.02 , dtype = dtype )), tfb . Softplus (), # For positive values of scale name = f 'meanfield_ { i } _scale' ) approx_parameter = tfd . Normal ( loc = loc , scale = scale ) dists . append ( approx_parameter ) return tfd . JointDistributionSequential ( dists ) meanfield_advi = build_mf_advi () TFP handles transformations differently as it transforms unconstrained space to match the support of distributions. In [10]: unconstraining_bijectors = [ tfb . Exp (), tfb . Identity () ] posterior = make_transformed_log_prob ( joint_log_prob , unconstraining_bijectors , direction = 'forward' , enable_bijector_caching = False ) In [11]: opt = tf . optimizers . Adam ( learning_rate =. 1 ) @tf . function ( autograph = False ) def run_approximation (): elbo_loss = tfp . vi . fit_surrogate_posterior ( posterior , surrogate_posterior = meanfield_advi , optimizer = opt , sample_size = 200 , num_steps = 10000 ) return elbo_loss elbo_loss = run_approximation () WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow_probability/python/math/minimize.py:74: calling <lambda> (from tensorflow_probability.python.vi.optimization) with loss is deprecated and will be removed after 2020-07-01. Instructions for updating: The signature for `trace_fn`s passed to `minimize` has changed. Trace functions now take a single `traceable_quantities` argument, which is a `tfp.math.MinimizeTraceableQuantities` namedtuple containing `traceable_quantities.loss`, `traceable_quantities.gradients`, etc. Please update your `trace_fn` definition. In [12]: plt . plot ( elbo_loss , color = 'blue' ) plt . yscale ( \"log\" ) plt . xlabel ( \"No of iterations\" ) plt . ylabel ( \"Negative ELBO\" ) plt . show () In [13]: graph_info = model . resolve_graph () approx_param = dict () free_param = meanfield_advi . trainable_variables for i , ( rvname , param ) in enumerate ( graph_info [: - 1 ]): approx_param [ rvname ] = { \"mu\" : free_param [ i * 2 ] . numpy (), \"sd\" : free_param [ i * 2 + 1 ] . numpy ()} In [14]: print ( approx_param ) {'sigma': {'mu': 0.7740287, 'sd': -0.7494337}, 'mu': {'mu': 11.233825, 'sd': 1.7977774}} We got pretty good estimates of sigma and mu. We need to transform sigma via exp and I believe it will be something close to 2.2","title":"Mean Field ADVI Example"},{"location":"gsoc/variational-inference/#drawbacks-of-this-blog-post","text":"I have not used consistent notation for probability density functions (pdfs). Because I like equations handled this way. Coming up with more good examples using minibatches. The ADVI papers also mention Elliptical standardization and Adaptive step size for optimizers. I have not understood those sections well and thus, haven't tried to implement them.","title":"Drawbacks of this blog post"},{"location":"gsoc/variational-inference/#references","text":"Chapter 1 and 2: Probabilistic Graphical Model Book Blog Post: An Introduction to Probability and Computational Bayesian Statistics by Ericmjl Section 10.1: Variational Inference Pattern Recognition and Machine Learning Book Section 2.5: Transformations Statistical Theory and Inference Book YouTube: Variational Inference in Python by Austin Rochford PyMC4: Basic Usage Notebook TFP: Joint Modelling Notebook Papers: Automatic Differentiation Variational Inference . Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and Blei, D. M. (2016). Automatic Variational Inference in Stan . Kucukelbir, A., Ranganath, R., Gelman, A., & Blei, D. (2015).","title":"References"},{"location":"gsoc/variational-inference/#special-thanks","text":"Website: codecogs.com to help me generate LaTeX equations. Comments: #1 and #2 by Luciano Paz that cleared my all doubts regarding transformations. if (!document.getElementById(\"mathjaxscript_pelican_#%@#$@#\")) { var mathjaxscript = document.createElement(\"script\"); mathjaxscript.id = \"mathjaxscript_pelican_#%@#$@#\"; mathjaxscript.type = \"text/javascript\"; mathjaxscript.src = \"//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\"; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js', 'AMSsymbols.js', 'noErrors.js', 'noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX', 'input/MathML', 'output/HTML-CSS'],\" + \" extensions: ['tex2jax.js', 'mml2jax.js', 'MathMenu.js', 'MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$', '$'] ], \" + \" displayMath: [ ['$$', '$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName(\"head\")[0]).appendChild(mathjaxscript); }","title":"Special Thanks"},{"location":"gsoc/week-1-into-gsoc/","text":"Week 1 into GSoC \u00b6 This blog contains my 1st week's progress (June 1 - June 7) into GSoC. After I have got a fair intuition of how tfp.vi module works, I started implementing Mean Field ADVI in PyMC4. With the guidance of my mentor Maxim Kochurov , we have split the implementation of VI approximations into 3 major parts - Vector LogProb function Approximate Distribution from TFP Integrate with TFP, Optimizers, etc I started implementing ideas into my fork. Within 4-5 days, I have come up with a reasonable design incorporating the above 3 guidelines. And here is the PR #280 and now, I am working on the suggestions to improve API design. On the way of my exploration of tfp.vi module, I see that we can use either tfp.distributions.JointDistributionSequential or tfp.distributions.MultivariateNormalDiag to implement Mean Field ADVI. The PR #280 is based on JointDistributionSequential. I will incorporate use of MultivariateNormalDiag after having a flattened view of parameters. I observe that I am a week ahead of my proposed GSoC timeline. This means I have more time to explore PyMC4. I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Week 1 into GSoC"},{"location":"gsoc/week-1-into-gsoc/#week-1-into-gsoc","text":"This blog contains my 1st week's progress (June 1 - June 7) into GSoC. After I have got a fair intuition of how tfp.vi module works, I started implementing Mean Field ADVI in PyMC4. With the guidance of my mentor Maxim Kochurov , we have split the implementation of VI approximations into 3 major parts - Vector LogProb function Approximate Distribution from TFP Integrate with TFP, Optimizers, etc I started implementing ideas into my fork. Within 4-5 days, I have come up with a reasonable design incorporating the above 3 guidelines. And here is the PR #280 and now, I am working on the suggestions to improve API design. On the way of my exploration of tfp.vi module, I see that we can use either tfp.distributions.JointDistributionSequential or tfp.distributions.MultivariateNormalDiag to implement Mean Field ADVI. The PR #280 is based on JointDistributionSequential. I will incorporate use of MultivariateNormalDiag after having a flattened view of parameters. I observe that I am a week ahead of my proposed GSoC timeline. This means I have more time to explore PyMC4. I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Week 1 into GSoC"},{"location":"gsoc/week-2-into-gsoc/","text":"Week 2 into GSoC \u00b6 Following up the work done in week 1 , I started exploring surrounding packages of PyMC4 i.e. TFP, ArviZ and PyMC3 to gain better insights how things need to be integrated. On the way, I opened a few issues and PRs fixing docstrings. ArviZ Issue #1232 regarding rendering of notebooks. PyMC4 Issue #283 regarding Transformations. I am still wondering whether the transformations are missing or automatically handled. TFP PR #965 that fixes tfb.Chain docstrings. Work done this week \u00b6 Figuring out the solutions over the review of the PR #280 , I have - Added sample method for sampling from posterior distribution. Fixed return type from fit function to include Approximation as well. Updated variational quickstart notebook incorporating both of the changes above. It took me a bit longer to look out for good variable names and to make quick-start notebook similar to PyMC3's Variational API notebook . But the time was worth it. Experiments \u00b6 Most of the time this week, I spent experimenting on ideas building on top of week 1's work. I polished out my experiments and created two gists. Gist 1 - Source \u00b6 Comparison between MeanField ADVI in TFP, PyMC3 and PyMC4. It was fun doing this. Gist 2 - Source \u00b6 Experimenting with ArviZ. I started learning from how trace from tfp.mcmc.sample_chain gets converted to ArviZ InferenceData. Following the same, I soon ran into shape issues while integrating it into sample method. I plan to resolve it in week 3. Tasks for week 3 \u00b6 Tests about convergence checks Add convergence criteria example in quick start notebook Include samples for untransformed variables as well Handle shape issues with ArviZ while sampling More user friendly optimizers I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Week 2 into GSoC"},{"location":"gsoc/week-2-into-gsoc/#week-2-into-gsoc","text":"Following up the work done in week 1 , I started exploring surrounding packages of PyMC4 i.e. TFP, ArviZ and PyMC3 to gain better insights how things need to be integrated. On the way, I opened a few issues and PRs fixing docstrings. ArviZ Issue #1232 regarding rendering of notebooks. PyMC4 Issue #283 regarding Transformations. I am still wondering whether the transformations are missing or automatically handled. TFP PR #965 that fixes tfb.Chain docstrings.","title":"Week 2 into GSoC"},{"location":"gsoc/week-2-into-gsoc/#work-done-this-week","text":"Figuring out the solutions over the review of the PR #280 , I have - Added sample method for sampling from posterior distribution. Fixed return type from fit function to include Approximation as well. Updated variational quickstart notebook incorporating both of the changes above. It took me a bit longer to look out for good variable names and to make quick-start notebook similar to PyMC3's Variational API notebook . But the time was worth it.","title":"Work done this week"},{"location":"gsoc/week-2-into-gsoc/#experiments","text":"Most of the time this week, I spent experimenting on ideas building on top of week 1's work. I polished out my experiments and created two gists.","title":"Experiments"},{"location":"gsoc/week-2-into-gsoc/#gist-1-source","text":"Comparison between MeanField ADVI in TFP, PyMC3 and PyMC4. It was fun doing this.","title":"Gist 1 - Source"},{"location":"gsoc/week-2-into-gsoc/#gist-2-source","text":"Experimenting with ArviZ. I started learning from how trace from tfp.mcmc.sample_chain gets converted to ArviZ InferenceData. Following the same, I soon ran into shape issues while integrating it into sample method. I plan to resolve it in week 3.","title":"Gist 2 - Source"},{"location":"gsoc/week-2-into-gsoc/#tasks-for-week-3","text":"Tests about convergence checks Add convergence criteria example in quick start notebook Include samples for untransformed variables as well Handle shape issues with ArviZ while sampling More user friendly optimizers I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Tasks for week 3"},{"location":"gsoc/week-3-into-gsoc/","text":"Week 3 into GSoC \u00b6 Today (June 21, 2020) marks the ending of week 3 of GSoC coding period. Let me summarize what I did for this week. Following up the work done in week 2 , I went on exploring ArviZ InferenceData and tfp.vi module to learn how to deal with shapes, convergence checks and optimizers. Adding a new axis to samples resolved the shape issues. By this, number of chains parameter is set to 1 and all the priors are handled perfectly. The convergence checks did not seem to work properly. (As pointed out by my mentor, we need to adjust window size). The default tensorflow optimizers also did not lead to convergence really well. Trying out the default values taken from PyMC3, I got really good results. This motivated me to write updates module for Variational Inference Interface. I applied inverse of bijectors to transformed parameters to match support in bounded space. But this approach is wrong. I need to handle this using deterministics callback which I will do in coming week. I had also written tests during this interval. From last week, I did not get quite good results while experimenting with Mean Field ADVI in TFP, PyMC3 and PyMC4. As suggested by my mentor, setting a common random seed and same optimizer leads to very good results . ( gist ) Experiments - Source \u00b6 Whatever experiments I perform, I polish them out and share through GitHub gists. I do not why but I started loving to share code through GitHub gists rather than Colab or GitHub repo. Here is the notebook - Tasks for week 4 \u00b6 Phase 1 Evaluations are coming up. So, I need to sync work with my proposed timeline and spend time summarizing all the results in a single notebook. My tasks for week 4 - Write tests for conjugate normal models with known mean/variance. Configure atol argument for np.testing.assert_allclose . (I misunderstood how this parameter works) Complete docs for optimizers by adding **kwargs option and writing corresponding maths equations. Properly configure convergence checks and add an example to quickstart notebook. Configure autobatching. (I need to understand how this works for mcmc) Integrate Deterministics callbacks. Complete Full Rank/Low Rank Approximation. (This will take time) Configure Minibatches. Update quick_start notebook with respect to all changes above. Extra \u00b6 If I will be able to complete above mentioned tasks in time, I would love to - Have another implementation of Mean Field ADVI using tfd.MultivariateNormalDiag . Play around with Mean Field ADVI on Eight Schools notebook. Resolve a warning from tfp.vi module regarding repetitive use of tf.function in a loop. Maybe using tf.while_loop will solve this but I am not sure. Experiment with MeanField/ FullRank/ LowRank ADVI in TFP, PyMC3, PyMC4 inspired from ColCarroll 's notebook . I am already getting excited to play around with this after having all APIs set correctly. For Evaluations \u00b6 I look forward to learn how Variational AutoEncoders are implemented in PyMC3 and try implementing that in PyMC4. I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Week 3 into GSoC"},{"location":"gsoc/week-3-into-gsoc/#week-3-into-gsoc","text":"Today (June 21, 2020) marks the ending of week 3 of GSoC coding period. Let me summarize what I did for this week. Following up the work done in week 2 , I went on exploring ArviZ InferenceData and tfp.vi module to learn how to deal with shapes, convergence checks and optimizers. Adding a new axis to samples resolved the shape issues. By this, number of chains parameter is set to 1 and all the priors are handled perfectly. The convergence checks did not seem to work properly. (As pointed out by my mentor, we need to adjust window size). The default tensorflow optimizers also did not lead to convergence really well. Trying out the default values taken from PyMC3, I got really good results. This motivated me to write updates module for Variational Inference Interface. I applied inverse of bijectors to transformed parameters to match support in bounded space. But this approach is wrong. I need to handle this using deterministics callback which I will do in coming week. I had also written tests during this interval. From last week, I did not get quite good results while experimenting with Mean Field ADVI in TFP, PyMC3 and PyMC4. As suggested by my mentor, setting a common random seed and same optimizer leads to very good results . ( gist )","title":"Week 3 into GSoC"},{"location":"gsoc/week-3-into-gsoc/#experiments-source","text":"Whatever experiments I perform, I polish them out and share through GitHub gists. I do not why but I started loving to share code through GitHub gists rather than Colab or GitHub repo. Here is the notebook -","title":"Experiments - Source"},{"location":"gsoc/week-3-into-gsoc/#tasks-for-week-4","text":"Phase 1 Evaluations are coming up. So, I need to sync work with my proposed timeline and spend time summarizing all the results in a single notebook. My tasks for week 4 - Write tests for conjugate normal models with known mean/variance. Configure atol argument for np.testing.assert_allclose . (I misunderstood how this parameter works) Complete docs for optimizers by adding **kwargs option and writing corresponding maths equations. Properly configure convergence checks and add an example to quickstart notebook. Configure autobatching. (I need to understand how this works for mcmc) Integrate Deterministics callbacks. Complete Full Rank/Low Rank Approximation. (This will take time) Configure Minibatches. Update quick_start notebook with respect to all changes above.","title":"Tasks for week 4"},{"location":"gsoc/week-3-into-gsoc/#extra","text":"If I will be able to complete above mentioned tasks in time, I would love to - Have another implementation of Mean Field ADVI using tfd.MultivariateNormalDiag . Play around with Mean Field ADVI on Eight Schools notebook. Resolve a warning from tfp.vi module regarding repetitive use of tf.function in a loop. Maybe using tf.while_loop will solve this but I am not sure. Experiment with MeanField/ FullRank/ LowRank ADVI in TFP, PyMC3, PyMC4 inspired from ColCarroll 's notebook . I am already getting excited to play around with this after having all APIs set correctly.","title":"Extra"},{"location":"gsoc/week-3-into-gsoc/#for-evaluations","text":"I look forward to learn how Variational AutoEncoders are implemented in PyMC3 and try implementing that in PyMC4. I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"For Evaluations"},{"location":"gsoc/week-4-into-gsoc/","text":"Week 4 into GSoC \u00b6 So, the phase 1 coding period ends and let me summarize what I did for the last week. Let's bring back the tasks proposed at the end of week 3 and figure out how many I have completed - Write tests for conjugate normal models with known mean/variance. Configure atol argument for np.testing.assert_allclose . Complete docs for optimizers by adding **kwargs option and writing corresponding maths equations. Properly configure convergence checks and add an example to quickstart notebooks. Configure autobatching. Integrate Deterministics callbacks. (But I did it wrong) Complete remaining Approximations. Full Rank Approximation. Low Rank Approximation. Configure Minibatches. Update quick_start notebook with respect to all changes above. I was unable to complete a few tasks because I got stuck for many days figuring out how to correctly handle shapes in Full Rank ADVI. Finally I was able to come up with a new _build_logfn to handle shapes (on the guidelines provided by my mentor). Experiments \u00b6 Gist 1 - Source \u00b6 I started the fourth week with a plan to include deterministics callbacks. Here are my experiments doing the same with PyMC4. All the determinitics are included in trace function. But when I opted for the same strategy to include deterministics while sampling, I got many shapes errors. The reason determinitics_callback failed because it assumes sample size = 1 . We need to change this API. Gist 2 - Source \u00b6 My experiments involving how to configure Full Rank ADVI in TFP. Gist 3 - Source \u00b6 Comparisons drawn between PyMC3 and PyMC4 for 2-d Gaussians. TODO - complete comparisons for Mixture Distributions as well. Tasks for the remaining GSoC period \u00b6 From the API point of view, only 5 tasks are left for the GSoC - Configure autobatching. Configure Minibatches. Add an option of progressbar. Include deterministics samples. Add convergence checks. From the view of adding examples, all the notebooks from PyMC3 need to be ported to PyMC4. For week 5, I look forward to add progressbar and convergence checks to PyMC4. Comparing with timeline \u00b6 I have already completed all the tasks proposed for phase 1 evaluations. Also I have added Mean Field and Full Rank Approximations which were proposed for phase 2 and phase 3 of GSoC coding period respectively. I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Week 4 into GSoC"},{"location":"gsoc/week-4-into-gsoc/#week-4-into-gsoc","text":"So, the phase 1 coding period ends and let me summarize what I did for the last week. Let's bring back the tasks proposed at the end of week 3 and figure out how many I have completed - Write tests for conjugate normal models with known mean/variance. Configure atol argument for np.testing.assert_allclose . Complete docs for optimizers by adding **kwargs option and writing corresponding maths equations. Properly configure convergence checks and add an example to quickstart notebooks. Configure autobatching. Integrate Deterministics callbacks. (But I did it wrong) Complete remaining Approximations. Full Rank Approximation. Low Rank Approximation. Configure Minibatches. Update quick_start notebook with respect to all changes above. I was unable to complete a few tasks because I got stuck for many days figuring out how to correctly handle shapes in Full Rank ADVI. Finally I was able to come up with a new _build_logfn to handle shapes (on the guidelines provided by my mentor).","title":"Week 4 into GSoC"},{"location":"gsoc/week-4-into-gsoc/#experiments","text":"","title":"Experiments"},{"location":"gsoc/week-4-into-gsoc/#gist-1-source","text":"I started the fourth week with a plan to include deterministics callbacks. Here are my experiments doing the same with PyMC4. All the determinitics are included in trace function. But when I opted for the same strategy to include deterministics while sampling, I got many shapes errors. The reason determinitics_callback failed because it assumes sample size = 1 . We need to change this API.","title":"Gist 1 - Source"},{"location":"gsoc/week-4-into-gsoc/#gist-2-source","text":"My experiments involving how to configure Full Rank ADVI in TFP.","title":"Gist 2 - Source"},{"location":"gsoc/week-4-into-gsoc/#gist-3-source","text":"Comparisons drawn between PyMC3 and PyMC4 for 2-d Gaussians. TODO - complete comparisons for Mixture Distributions as well.","title":"Gist 3 - Source"},{"location":"gsoc/week-4-into-gsoc/#tasks-for-the-remaining-gsoc-period","text":"From the API point of view, only 5 tasks are left for the GSoC - Configure autobatching. Configure Minibatches. Add an option of progressbar. Include deterministics samples. Add convergence checks. From the view of adding examples, all the notebooks from PyMC3 need to be ported to PyMC4. For week 5, I look forward to add progressbar and convergence checks to PyMC4.","title":"Tasks for the remaining GSoC period"},{"location":"gsoc/week-4-into-gsoc/#comparing-with-timeline","text":"I have already completed all the tasks proposed for phase 1 evaluations. Also I have added Mean Field and Full Rank Approximations which were proposed for phase 2 and phase 3 of GSoC coding period respectively. I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Comparing with timeline"},{"location":"gsoc/week-5-into-gsoc/","text":"Week 5 into GSoC \u00b6 Today (July 12, 2020) marks mid-way between phase 1 and phase 2 of GSoC coding period. Some good news, I have cleared Phase 1 evaluations. My mentor Maxim Kochurov is amazing. Not only we explore different ideas related to the project but also we have great conversations not related to GSoC. Most of the time spent this week was into resolving issues with Full Rank ADVI PR #289 . Here is the long story short - Mean Field ADVI PR uses JointDistributionSequential and on the same paths, I added an interface of Full Rank ADVI. That approach was wrong because I misunderstood model flattening as variable flattening. And this leads to all sort of shape issues. After learning about model flattening, I integrated it into the PR. ADVI worked but losses contains lots of nans. As suspected, it was due to missing transformations. It took time to figure out how to setup transformations but later selected manually doing it. I was unable to make transformations generalized enough. Conditional statements and tf.cond does not seem to work either because at hand, we do not have values to transform inside bijectors. Later I plan to write a proposal mentioning the issue to PyMC-devs. Then after transformations, the major issue was of cholesky decomposition failure. And this comment is a life saviour one. It took time to experiment with each option with park_bias_model and the last one worked. As stated, cholesky decomposition is unstable to tf.float32 . So casting all the parameters to tf.float64 resolved the issue. I also want to thank Tirth Patel for helping me on the way to resolve dtype issues. Finally, I am all set to see the Full Rank ADVI PR getting merged. Experiments \u00b6 Gist 1 - Source \u00b6 Flattening and Full Rank ADVI in PyMC4 Gist 2 - Source \u00b6 Testing issues with transformations Gist 3 - Source \u00b6 Failed at an attempt to generalize transformations Experiment 4 \u00b6 Good news regarding the progress bars but I cannot do tf.print(\".\"*trace.step) num_steps = 10_000 def trace_fn ( trace ): tf . cond ( tf . math . mod ( trace . step , 100 ) == 0 , lambda : tf . print ( trace . step , \"/\" , num_steps , \"Loss:\" , trace . loss ), lambda : tf . print ( \"\" , end = \" \\r \" ) ) return trace . loss # Pass this trace_fn to pm.fit() I am planning to add convergence checks and progress bars for next week. Stay tuned. I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Week 5 into GSoC"},{"location":"gsoc/week-5-into-gsoc/#week-5-into-gsoc","text":"Today (July 12, 2020) marks mid-way between phase 1 and phase 2 of GSoC coding period. Some good news, I have cleared Phase 1 evaluations. My mentor Maxim Kochurov is amazing. Not only we explore different ideas related to the project but also we have great conversations not related to GSoC. Most of the time spent this week was into resolving issues with Full Rank ADVI PR #289 . Here is the long story short - Mean Field ADVI PR uses JointDistributionSequential and on the same paths, I added an interface of Full Rank ADVI. That approach was wrong because I misunderstood model flattening as variable flattening. And this leads to all sort of shape issues. After learning about model flattening, I integrated it into the PR. ADVI worked but losses contains lots of nans. As suspected, it was due to missing transformations. It took time to figure out how to setup transformations but later selected manually doing it. I was unable to make transformations generalized enough. Conditional statements and tf.cond does not seem to work either because at hand, we do not have values to transform inside bijectors. Later I plan to write a proposal mentioning the issue to PyMC-devs. Then after transformations, the major issue was of cholesky decomposition failure. And this comment is a life saviour one. It took time to experiment with each option with park_bias_model and the last one worked. As stated, cholesky decomposition is unstable to tf.float32 . So casting all the parameters to tf.float64 resolved the issue. I also want to thank Tirth Patel for helping me on the way to resolve dtype issues. Finally, I am all set to see the Full Rank ADVI PR getting merged.","title":"Week 5 into GSoC"},{"location":"gsoc/week-5-into-gsoc/#experiments","text":"","title":"Experiments"},{"location":"gsoc/week-5-into-gsoc/#gist-1-source","text":"Flattening and Full Rank ADVI in PyMC4","title":"Gist 1 - Source"},{"location":"gsoc/week-5-into-gsoc/#gist-2-source","text":"Testing issues with transformations","title":"Gist 2 - Source"},{"location":"gsoc/week-5-into-gsoc/#gist-3-source","text":"Failed at an attempt to generalize transformations","title":"Gist 3 - Source"},{"location":"gsoc/week-5-into-gsoc/#experiment-4","text":"Good news regarding the progress bars but I cannot do tf.print(\".\"*trace.step) num_steps = 10_000 def trace_fn ( trace ): tf . cond ( tf . math . mod ( trace . step , 100 ) == 0 , lambda : tf . print ( trace . step , \"/\" , num_steps , \"Loss:\" , trace . loss ), lambda : tf . print ( \"\" , end = \" \\r \" ) ) return trace . loss # Pass this trace_fn to pm.fit() I am planning to add convergence checks and progress bars for next week. Stay tuned. I am thankful to my mentor for his constant guidance and pymc-devs for being such a supportive community. Thank you for reading! With , Sayam","title":"Experiment 4"},{"location":"gsoc/week-6-7-into-gsoc/","text":"Week 6 and 7 into GSoC \u00b6 All these days I have experimenting with Full Rank ADVI testing on rugby, radon notebooks and park_bias_model. Now, I am planning to draw general conclusions in what scenarios Variational Inference leads to Cholesky Decomposition errors when the model contains Gaussian Processes. Its mainly due to dtypes and high learning rates. Gist - Source \u00b6 Also, I have written a short proposal to explore ways of adding transforms to PyMC4. Thank you for reading! With , Sayam","title":"Week 6-7 into GSoC"},{"location":"gsoc/week-6-7-into-gsoc/#week-6-and-7-into-gsoc","text":"All these days I have experimenting with Full Rank ADVI testing on rugby, radon notebooks and park_bias_model. Now, I am planning to draw general conclusions in what scenarios Variational Inference leads to Cholesky Decomposition errors when the model contains Gaussian Processes. Its mainly due to dtypes and high learning rates.","title":"Week 6 and 7 into GSoC"},{"location":"gsoc/week-6-7-into-gsoc/#gist-source","text":"Also, I have written a short proposal to explore ways of adding transforms to PyMC4. Thank you for reading! With , Sayam","title":"Gist - Source"},{"location":"gsoc/week-9-into-gsoc/","text":"Week 9 into GSoC \u00b6 The second evaluation for GSoC is cleared. I am happy. Also, the PR #289 for Full Rank ADVI is merged in. This week's time is spent adding small but useful features to PyMC4. The PR #310 includes addition of progress bar. I spent most of the time digging deep into TFP codebase learning how vi module is integrated. Some more features to be included are - Parameter Convergence checks Low Rank Approximation Add a notebook playing around with mcmc and VI approximations over hierarchial models. I plan to complete all these features before 15 August so that I can spend time writing documentation for PyMC4. I am loving my time with PyMC community. Thank you for reading! With , Sayam","title":"Week 9 into GSoC"},{"location":"gsoc/week-9-into-gsoc/#week-9-into-gsoc","text":"The second evaluation for GSoC is cleared. I am happy. Also, the PR #289 for Full Rank ADVI is merged in. This week's time is spent adding small but useful features to PyMC4. The PR #310 includes addition of progress bar. I spent most of the time digging deep into TFP codebase learning how vi module is integrated. Some more features to be included are - Parameter Convergence checks Low Rank Approximation Add a notebook playing around with mcmc and VI approximations over hierarchial models. I plan to complete all these features before 15 August so that I can spend time writing documentation for PyMC4. I am loving my time with PyMC community. Thank you for reading! With , Sayam","title":"Week 9 into GSoC"},{"location":"gsoc/work-summary/","text":"Google Summer of Code'20 Highlights with NumFOCUS \u00b6 This post is meant to summarize the work done over the GSoC coding period. Let's get started real quick. About the project \u00b6 My GSoC proposal was about adding a Variational Inference interface to PyMC4. Apart from MCMC algorithms, VI proposes an approximating distribution to fit the posterior. The whole plan was to implement two Variational Inference algorithms - Mean Field ADVI and Full Rank ADVI. Resolving Key challenges \u00b6 Key Challenges Solutions proposed How its resolved theano.clone equivalent for TF2 Model execution with replaced inputs Normal distribution's sample method is executed over flattened view of parameters Flattened view of parameters Use tf.reshape() Used tf.concat() with tf.reshape() Optimizers for ELBO Use tf.keras.optimizers Optimizers either from TFv1 or TFv2 with defaults from pymc3.updates can be used Initialization of MeanField and Full Rank ADVI Manually set bijectors Relied on tfp.TransformedVariable Progress bar Use tqdm or \u200btf.keras.utils.Progbar A small hack over tf.print Minibatch processing of data Capture slice in memory This is the only incomplete feature. Maybe tf.Dataset API has to explored more or implement our own tfp.vi.fit_surrogate_posterior function. Community Bounding Period \u00b6 This was a super interesting period. I got to know about many PyMC core developers through slack. I spent the entire time learning about the basics of Bayesian statistics, prior, posterior predictive checks, and the theory of Variational Inference. I had also written a blog post during this interval about the nuts and bolts of VI and the implementation of Mean Field ADVI as well in Tensorflow Probability. Here is the blog post - Demystify Variational Inference . The most difficult part of learning VI was to understand the transformations because PyMC3 and TFP handle transformations differently. Month 1 \u00b6 The coding period started from June 1 and my intention for this period was to add a very basic and general Variational Inference interface to PyMC4. Here is the PR #280 and workflow of the basic interface was - Get the vectorized log prob of the model. For each parameter of the model, have a Normal Distribution with the same shape and then build a posterior using tfd.JointDistributionSequential . Add optimizers with defaults from PyMC3 and perform VI using tfp.fit_surrogate_posterior . Sample from tfd.JointDistributionSequential and there is no need of equivalent of theano.clone . Transform the samples by quering the SamplingState but Deterministics have to be added as well. Resolve shape issues with ArviZ. In short, making chains=1 . I got the basic interface merged by late June and now, it was time to work upon Full Rank ADVI. I managed to open a PR #289 with Full Rank ADVI interface by the end of June. Month 2 \u00b6 This was the most dramatic month of GSoC coding period. Because Full Rank ADVI proposed in PR #289 resulted in errors most of the time. Here is the gist of workflow that was followed to get some useful insights about the errors - Instead of solving the shape issues independently and posing a MvNormal distribution for each parameter, build the posterior using flattened view of parameters. There were lots of NaNs in the ELBO, because of improper handling of transformations. As a result, Interval , LowerBounded and UpperBounded transformations were added as well. Then came the issue of Cholesky Decomposition errors while working with Gaussian Processes and Variational Inference. Here are my few insights after rigorous testing with different inputs - Use dtype tf.float64 with FullRank ADVI to maintain positive definiteness of covariance matrix. Avoid aggressive optimization of ELBO. Maintain learning rates around 1e-3 . Stabilize the diagonal of covariance matrix by adding a small jitter. Double check for NaNs in the data. Here the results after trying reparametrization and different jitter amounts while doing VI. I got this PR merged by the end of July. And now, it was time to work on adding some features to ADVI. Month 3 \u00b6 After adding missing transformations in PR #289 , my mentor asked me to write a proposal so as the Bounded Distributions are inherited instead of we applying transformations manually to each distribution. I explored each possibility to make a generalized version of transformations as it is done in PyMC3 using tf.cond . Since, we do not have values before model execution, it was difficult to use tf.cond . Here is the proposal's source . After getting an interface to use MeanField and FullRank ADVI, some features that are included in the PR #310 - Add a progress bar. (This is small hack over tf.print ) Test progress bar in different OS. Add ParameterConvergence criteria to test convergence. Add LowRank Approximation. I am still working on adding examples on hierarchical models and I hope to get it merged soon. Contributions \u00b6 The Pull Requests I have opened and got merged during GSoC. I have explained each one above but here I try to summarize. Add Variational Inference Interface: #280 Add Full Rank Approximation: #289 Add features to ADVI: #310 (WIP) Remove transformations for Discrete distributions: #314 Gists created \u00b6 Whatever experiments I perform to aid my learnings, I polish them out and share through GitHub gists. I do not why but I started loving to share code through GitHub gists rather than Colab or GitHub repo. Here are all the experiments I performed with ADVI during this summer. Comparison of MeanField ADVI in TFP, PyMC3, PyMC4: Source Demonstration of shape issues while working with InferenceData: Source Playing around Convergence and Optimizers: Source Tracking all parameters including deterministics: Source Implementation of FullRank ADVI in TFP: Source Comparison of MeanField and FullRank ADVI over correlated Gaussians: Source Model flattening and Full Rank ADVI in PyMC4: Source Missing transformations in PyMC4: Source Testing transformations in PyMC4: Source Distribution Enhancement Proposal: Source Hacking tf.print for progress bar: Source Parameter Convergence Checks in TFP: Source Future Goals \u00b6 Some future tasks I would like to work upon - Configure Mini Batch processing of data. Add Normalizing Flows to variational inference interface. Add support of Variational AutoEncoders to PyMC4. Conclusion \u00b6 It was an incredible experience contributing to open source. I have improved my Python skills. I want to thank my mentors @ferrine and @twiecki for being extremely supportive throughout this entire journey. I am loving my time with the PyMC community. Next, I also want to thank @numfocus community for sharing this opportunity via Google Summer of Code. Thank you for being a part of this fantastic summer. With , Sayam Kumar","title":"Work Summary"},{"location":"gsoc/work-summary/#google-summer-of-code20-highlights-with-numfocus","text":"This post is meant to summarize the work done over the GSoC coding period. Let's get started real quick.","title":"Google Summer of Code'20 Highlights with NumFOCUS"},{"location":"gsoc/work-summary/#about-the-project","text":"My GSoC proposal was about adding a Variational Inference interface to PyMC4. Apart from MCMC algorithms, VI proposes an approximating distribution to fit the posterior. The whole plan was to implement two Variational Inference algorithms - Mean Field ADVI and Full Rank ADVI.","title":"About the project"},{"location":"gsoc/work-summary/#resolving-key-challenges","text":"Key Challenges Solutions proposed How its resolved theano.clone equivalent for TF2 Model execution with replaced inputs Normal distribution's sample method is executed over flattened view of parameters Flattened view of parameters Use tf.reshape() Used tf.concat() with tf.reshape() Optimizers for ELBO Use tf.keras.optimizers Optimizers either from TFv1 or TFv2 with defaults from pymc3.updates can be used Initialization of MeanField and Full Rank ADVI Manually set bijectors Relied on tfp.TransformedVariable Progress bar Use tqdm or \u200btf.keras.utils.Progbar A small hack over tf.print Minibatch processing of data Capture slice in memory This is the only incomplete feature. Maybe tf.Dataset API has to explored more or implement our own tfp.vi.fit_surrogate_posterior function.","title":"Resolving Key challenges"},{"location":"gsoc/work-summary/#community-bounding-period","text":"This was a super interesting period. I got to know about many PyMC core developers through slack. I spent the entire time learning about the basics of Bayesian statistics, prior, posterior predictive checks, and the theory of Variational Inference. I had also written a blog post during this interval about the nuts and bolts of VI and the implementation of Mean Field ADVI as well in Tensorflow Probability. Here is the blog post - Demystify Variational Inference . The most difficult part of learning VI was to understand the transformations because PyMC3 and TFP handle transformations differently.","title":"Community Bounding Period"},{"location":"gsoc/work-summary/#month-1","text":"The coding period started from June 1 and my intention for this period was to add a very basic and general Variational Inference interface to PyMC4. Here is the PR #280 and workflow of the basic interface was - Get the vectorized log prob of the model. For each parameter of the model, have a Normal Distribution with the same shape and then build a posterior using tfd.JointDistributionSequential . Add optimizers with defaults from PyMC3 and perform VI using tfp.fit_surrogate_posterior . Sample from tfd.JointDistributionSequential and there is no need of equivalent of theano.clone . Transform the samples by quering the SamplingState but Deterministics have to be added as well. Resolve shape issues with ArviZ. In short, making chains=1 . I got the basic interface merged by late June and now, it was time to work upon Full Rank ADVI. I managed to open a PR #289 with Full Rank ADVI interface by the end of June.","title":"Month 1"},{"location":"gsoc/work-summary/#month-2","text":"This was the most dramatic month of GSoC coding period. Because Full Rank ADVI proposed in PR #289 resulted in errors most of the time. Here is the gist of workflow that was followed to get some useful insights about the errors - Instead of solving the shape issues independently and posing a MvNormal distribution for each parameter, build the posterior using flattened view of parameters. There were lots of NaNs in the ELBO, because of improper handling of transformations. As a result, Interval , LowerBounded and UpperBounded transformations were added as well. Then came the issue of Cholesky Decomposition errors while working with Gaussian Processes and Variational Inference. Here are my few insights after rigorous testing with different inputs - Use dtype tf.float64 with FullRank ADVI to maintain positive definiteness of covariance matrix. Avoid aggressive optimization of ELBO. Maintain learning rates around 1e-3 . Stabilize the diagonal of covariance matrix by adding a small jitter. Double check for NaNs in the data. Here the results after trying reparametrization and different jitter amounts while doing VI. I got this PR merged by the end of July. And now, it was time to work on adding some features to ADVI.","title":"Month 2"},{"location":"gsoc/work-summary/#month-3","text":"After adding missing transformations in PR #289 , my mentor asked me to write a proposal so as the Bounded Distributions are inherited instead of we applying transformations manually to each distribution. I explored each possibility to make a generalized version of transformations as it is done in PyMC3 using tf.cond . Since, we do not have values before model execution, it was difficult to use tf.cond . Here is the proposal's source . After getting an interface to use MeanField and FullRank ADVI, some features that are included in the PR #310 - Add a progress bar. (This is small hack over tf.print ) Test progress bar in different OS. Add ParameterConvergence criteria to test convergence. Add LowRank Approximation. I am still working on adding examples on hierarchical models and I hope to get it merged soon.","title":"Month 3"},{"location":"gsoc/work-summary/#contributions","text":"The Pull Requests I have opened and got merged during GSoC. I have explained each one above but here I try to summarize. Add Variational Inference Interface: #280 Add Full Rank Approximation: #289 Add features to ADVI: #310 (WIP) Remove transformations for Discrete distributions: #314","title":"Contributions"},{"location":"gsoc/work-summary/#gists-created","text":"Whatever experiments I perform to aid my learnings, I polish them out and share through GitHub gists. I do not why but I started loving to share code through GitHub gists rather than Colab or GitHub repo. Here are all the experiments I performed with ADVI during this summer. Comparison of MeanField ADVI in TFP, PyMC3, PyMC4: Source Demonstration of shape issues while working with InferenceData: Source Playing around Convergence and Optimizers: Source Tracking all parameters including deterministics: Source Implementation of FullRank ADVI in TFP: Source Comparison of MeanField and FullRank ADVI over correlated Gaussians: Source Model flattening and Full Rank ADVI in PyMC4: Source Missing transformations in PyMC4: Source Testing transformations in PyMC4: Source Distribution Enhancement Proposal: Source Hacking tf.print for progress bar: Source Parameter Convergence Checks in TFP: Source","title":"Gists created"},{"location":"gsoc/work-summary/#future-goals","text":"Some future tasks I would like to work upon - Configure Mini Batch processing of data. Add Normalizing Flows to variational inference interface. Add support of Variational AutoEncoders to PyMC4.","title":"Future Goals"},{"location":"gsoc/work-summary/#conclusion","text":"It was an incredible experience contributing to open source. I have improved my Python skills. I want to thank my mentors @ferrine and @twiecki for being extremely supportive throughout this entire journey. I am loving my time with the PyMC community. Next, I also want to thank @numfocus community for sharing this opportunity via Google Summer of Code. Thank you for being a part of this fantastic summer. With , Sayam Kumar","title":"Conclusion"},{"location":"guides/git/","text":"Git \u00b6 Git is a distributed version control system. Svn is a central control system. In central control system, all the files are stored in the server's database. In distributed vcs, everybody has a local repository. There are three areas - Working Directory - Where we put up all the files and codes in the local repository. We need to add them. Staging Area - Where we commit our files Repo - Contains all the files Basic Commands \u00b6 git --version git config --global user.name \"<user-name>\" git config --global user.email \"<user-email-ID>\" git config --global --list git config --local --list git help <verb> git <verb> --help git init # For initialising a repository with git rm -rf .git # To stop tracking touch .gitignore # To put all the unwanted files which we do not want to git git remote -v # Info about remote repo Save changes \u00b6 git status git add -A # Adds files to staging area git commit -m \"Message\" git pull origin master # If remote is set git push origin master Create a new git repo with existing code \u00b6 git init # In the folder. Also create a new repo in GitHub git add -A git commit -m \"message\" git remote add origin link_to_repo git push -u origin master Branches \u00b6 Common Overflow - create a branch for desire feature git branch # To check the current branch git branch my_branch # To initialise a branch git checkout my_branch # To change the branch git checkout -b my_branch # Single command for above two operations After adding and commiting, push branch to remote git push -u origin my_branch # To create a separate branch in the repo with the modified changes git branch -a # Check all branches at local as well as remote repo Merge a branch \u00b6 git checkout master git pull origin master git branch --merged git merge my_branch git push origin master Delete a branch \u00b6 git branch --merged # To check if everything is merged correctly git branch -d my_branch # Deleted locally git branch -D my_branch # Delete branch forcefully, if not merged git branch -a # To check git push origin --delete my_branch Fixing Mistakes and Bad Commits \u00b6 Rename a message after a commit \u00b6 But this changes the hash of the commit as the hash depends of message. This also changes the git history so it is not a good option. git commit --amend -m \"Added add function\" Add a file to previous commit \u00b6 First add that file to staging area. This again changes the git history. touch new.py git add new.py git commit --amend # Amend the file to last commit. Vim is opened asking for any changes in commit message. OR git commit --amend --no-edit # To keep the last commit message same git log --stat # To see changes in a commit Move a commit to different branch \u00b6 Copy the hash of commit you want to move - git checkout <branch-name> git cherry-pick <hash-of-commit> # The changes are reflected but the commit hash is changed Remove that commit from previous branch - # Remove from branch git checkout <branch-name> # Three different types of reset - git reset --soft <previous-commit-hash-of-branch> # It will keep the changes in staging area. git reset <previous-commit-hash-of-branch> # Mixed reset - It will keep the changes in the working directory. git reset --hard <previous-commit-hash-of-branch> # Remove all the changes from tracked files. All new files added will be there in working directory. Untracked files are left Warning - Use of git reset --hard , will reset to previous commit only when files are staged or commited. If they are in working directory, git reset --hard will reset only tracked files. To delete untracked files/directories forcefully - git clean -df Retrieve critical files that were lost, and you want them back - git reflog # Copy the hash git checkout <hash-copied-from-reflog> # Now the head is detached. You need to create a branch if you want those changes git branch backup # Create a backup branch Git revert \u00b6 Creates new commit on top of earlier commits to revert the changes. git revert <hash-of-commit> Using stash command \u00b6 Git stash helps when you have some uncommitted changes. You want to revert or switch between branches. Stash will temporarily save your changes. Save you changes before moving around. git stash save \"Message for your changes\" git stash save -u \"Message for your changes\" # For including all untracked as well git stash save --all \"Message for your changes\" # For including all untracked and ignored files as well git stash list # To list out all our stashes Explore whatever you wish like. View the contents of the stash git stash show -p # For latest stash git stash show -p stash: { index } # For a particular stash Get those changes back Use git stash apply git stash apply # To apply the latest stash git stash apply stash: { index } # To see the changes OR git stash list # But it does not get rid of the stash git checkout -- . # To return to spec Use git stash pop git stash pop # Applies the changes of topmost in git stash list and remove that one OR git stash pop stash: { index } # Apply and delete git stash list # That stash has been removed Let's suppose now you do not want to keep that changes Drop a single stash git stash drop # For topmost stash git stash drop stash: { index } # For a specific stash Drop all stashes git stash clear git stash list # Empty list Stashes are carried from branch to branch git stash save \"Message\" git checkout <branch_name> git stash pop Create a new branch and apply that stash - The stash is also deleted. git stash branch <branch_name> # For topmost stash git stash branch <branch_name> stash: { index } # For a specific stash Note - The git stash save API is deprecated in favour of new git stash push API from 2.16 version onwards. Different Types of git add \u00b6 Stage all the changes including untracked and .dot files, in the entire working tree git add --all git add --all <directory-name> # It will only stage changes made in that directory git add <directory-name> # --all or -A option is by default git add --no-all <directory-name> # It will stage only modified and new files, not the deleted ones in directory Stage only modified and deleted files, in the entire working directory, -u or --update git add -u git add -u <directory-name> # It will stage only modified and deleted, not the new ones in the directory Stage all the changes including untracked and .dot files, in the current working directory and below not the parent ones. git add . # Changes will be visible if we run this command in any sub directory Stage with \"*\" It will stage the files it can see in ls - This is not recommended as its not including deleted as well as .dot files git add * Extras \u00b6 # List out the differences git diff # In working directory git diff --staged # In staging area git diff hash1 hash2 # Between two commits via hashes # Discard the changes in a working directory git checkout <filename> # For a single file git checkout -- . # All tracked files git clean -df # All untracked files # Remove files from staging area and put them back into working directory git reset <filename> # For a single file git reset # To remove everything # Newer APIs git restore <filename> # To discard changes in working directory git restore --staged <filename> # To unstage # Logs git log # To check the various commits git log -p # Log with patches git log -2 # For last 2 commits git log --stat # To see the changes git log --oneline # All commit messages in single line # Show the changes done by a commit git show # For latest commit. This is equivalent to git log -p -1 git show <commit-hash> # Alias git config --global alias.<alias> <command> git config --global alias.st status # Typing git st will show results of git status # Remote git remote # List of all remotes git remote -v # Be a little more verbose git remote add <remote name> <url> # To set a remote git remote rename <oldname> <newname> # To modify remote's name git remote set-url <remote name> <url> # To modify a remote's url git remote rm <remote name> # To delete a remote # If we set-url or add a new remote, we need to do - git fetch git remote set-head <remote name> master git push --set-upstream <remote name> master # To track all the branches # Rename git mv <old filename> <new filename> # Rename a file git branch -m <newname> # Rename a branch while working on branch git branch -m <oldname> <newname> # Rename a branch from outside the branch # Signing commits git config --global commit.gpgsign true git config --global --edit # To change the settings # Forks git remote add upstream <url> git fetch upstream git checkout master git merge upstream/master # gitignore git rm --cached <file name> # To ignore a file that is already checked in git rm --cached -r <folder name> # To ignore a folder that is already checked in git config --global core.excludesfile ~/.gitignore_global # And put in the files you want to ignore # Rewriting git history git rebase upstream/master git rebase <branch name> # To bring all the commits forward git rebase -i HEAD~n # n for which changes you want to make # reword - to rename a commit # drop - to delete a commit # squash - to combine commits and give a name # fixup - to combine commits and keep the name of base commit # edit - to stop execution of rebase and if you wish, split the commits # To reorder commits, just change their order in interactive rebase prompt","title":"Git"},{"location":"guides/git/#git","text":"Git is a distributed version control system. Svn is a central control system. In central control system, all the files are stored in the server's database. In distributed vcs, everybody has a local repository. There are three areas - Working Directory - Where we put up all the files and codes in the local repository. We need to add them. Staging Area - Where we commit our files Repo - Contains all the files","title":"Git"},{"location":"guides/git/#basic-commands","text":"git --version git config --global user.name \"<user-name>\" git config --global user.email \"<user-email-ID>\" git config --global --list git config --local --list git help <verb> git <verb> --help git init # For initialising a repository with git rm -rf .git # To stop tracking touch .gitignore # To put all the unwanted files which we do not want to git git remote -v # Info about remote repo","title":"Basic Commands"},{"location":"guides/git/#save-changes","text":"git status git add -A # Adds files to staging area git commit -m \"Message\" git pull origin master # If remote is set git push origin master","title":"Save changes"},{"location":"guides/git/#create-a-new-git-repo-with-existing-code","text":"git init # In the folder. Also create a new repo in GitHub git add -A git commit -m \"message\" git remote add origin link_to_repo git push -u origin master","title":"Create a new git repo with existing code"},{"location":"guides/git/#branches","text":"Common Overflow - create a branch for desire feature git branch # To check the current branch git branch my_branch # To initialise a branch git checkout my_branch # To change the branch git checkout -b my_branch # Single command for above two operations After adding and commiting, push branch to remote git push -u origin my_branch # To create a separate branch in the repo with the modified changes git branch -a # Check all branches at local as well as remote repo","title":"Branches"},{"location":"guides/git/#merge-a-branch","text":"git checkout master git pull origin master git branch --merged git merge my_branch git push origin master","title":"Merge a branch"},{"location":"guides/git/#delete-a-branch","text":"git branch --merged # To check if everything is merged correctly git branch -d my_branch # Deleted locally git branch -D my_branch # Delete branch forcefully, if not merged git branch -a # To check git push origin --delete my_branch","title":"Delete a branch"},{"location":"guides/git/#fixing-mistakes-and-bad-commits","text":"","title":"Fixing Mistakes and Bad Commits"},{"location":"guides/git/#rename-a-message-after-a-commit","text":"But this changes the hash of the commit as the hash depends of message. This also changes the git history so it is not a good option. git commit --amend -m \"Added add function\"","title":"Rename a message after a commit"},{"location":"guides/git/#add-a-file-to-previous-commit","text":"First add that file to staging area. This again changes the git history. touch new.py git add new.py git commit --amend # Amend the file to last commit. Vim is opened asking for any changes in commit message. OR git commit --amend --no-edit # To keep the last commit message same git log --stat # To see changes in a commit","title":"Add a file to previous commit"},{"location":"guides/git/#move-a-commit-to-different-branch","text":"Copy the hash of commit you want to move - git checkout <branch-name> git cherry-pick <hash-of-commit> # The changes are reflected but the commit hash is changed Remove that commit from previous branch - # Remove from branch git checkout <branch-name> # Three different types of reset - git reset --soft <previous-commit-hash-of-branch> # It will keep the changes in staging area. git reset <previous-commit-hash-of-branch> # Mixed reset - It will keep the changes in the working directory. git reset --hard <previous-commit-hash-of-branch> # Remove all the changes from tracked files. All new files added will be there in working directory. Untracked files are left Warning - Use of git reset --hard , will reset to previous commit only when files are staged or commited. If they are in working directory, git reset --hard will reset only tracked files. To delete untracked files/directories forcefully - git clean -df Retrieve critical files that were lost, and you want them back - git reflog # Copy the hash git checkout <hash-copied-from-reflog> # Now the head is detached. You need to create a branch if you want those changes git branch backup # Create a backup branch","title":"Move a commit to different branch"},{"location":"guides/git/#git-revert","text":"Creates new commit on top of earlier commits to revert the changes. git revert <hash-of-commit>","title":"Git revert"},{"location":"guides/git/#using-stash-command","text":"Git stash helps when you have some uncommitted changes. You want to revert or switch between branches. Stash will temporarily save your changes. Save you changes before moving around. git stash save \"Message for your changes\" git stash save -u \"Message for your changes\" # For including all untracked as well git stash save --all \"Message for your changes\" # For including all untracked and ignored files as well git stash list # To list out all our stashes Explore whatever you wish like. View the contents of the stash git stash show -p # For latest stash git stash show -p stash: { index } # For a particular stash Get those changes back Use git stash apply git stash apply # To apply the latest stash git stash apply stash: { index } # To see the changes OR git stash list # But it does not get rid of the stash git checkout -- . # To return to spec Use git stash pop git stash pop # Applies the changes of topmost in git stash list and remove that one OR git stash pop stash: { index } # Apply and delete git stash list # That stash has been removed Let's suppose now you do not want to keep that changes Drop a single stash git stash drop # For topmost stash git stash drop stash: { index } # For a specific stash Drop all stashes git stash clear git stash list # Empty list Stashes are carried from branch to branch git stash save \"Message\" git checkout <branch_name> git stash pop Create a new branch and apply that stash - The stash is also deleted. git stash branch <branch_name> # For topmost stash git stash branch <branch_name> stash: { index } # For a specific stash Note - The git stash save API is deprecated in favour of new git stash push API from 2.16 version onwards.","title":"Using stash command"},{"location":"guides/git/#different-types-of-git-add","text":"Stage all the changes including untracked and .dot files, in the entire working tree git add --all git add --all <directory-name> # It will only stage changes made in that directory git add <directory-name> # --all or -A option is by default git add --no-all <directory-name> # It will stage only modified and new files, not the deleted ones in directory Stage only modified and deleted files, in the entire working directory, -u or --update git add -u git add -u <directory-name> # It will stage only modified and deleted, not the new ones in the directory Stage all the changes including untracked and .dot files, in the current working directory and below not the parent ones. git add . # Changes will be visible if we run this command in any sub directory Stage with \"*\" It will stage the files it can see in ls - This is not recommended as its not including deleted as well as .dot files git add *","title":"Different Types of git add"},{"location":"guides/git/#extras","text":"# List out the differences git diff # In working directory git diff --staged # In staging area git diff hash1 hash2 # Between two commits via hashes # Discard the changes in a working directory git checkout <filename> # For a single file git checkout -- . # All tracked files git clean -df # All untracked files # Remove files from staging area and put them back into working directory git reset <filename> # For a single file git reset # To remove everything # Newer APIs git restore <filename> # To discard changes in working directory git restore --staged <filename> # To unstage # Logs git log # To check the various commits git log -p # Log with patches git log -2 # For last 2 commits git log --stat # To see the changes git log --oneline # All commit messages in single line # Show the changes done by a commit git show # For latest commit. This is equivalent to git log -p -1 git show <commit-hash> # Alias git config --global alias.<alias> <command> git config --global alias.st status # Typing git st will show results of git status # Remote git remote # List of all remotes git remote -v # Be a little more verbose git remote add <remote name> <url> # To set a remote git remote rename <oldname> <newname> # To modify remote's name git remote set-url <remote name> <url> # To modify a remote's url git remote rm <remote name> # To delete a remote # If we set-url or add a new remote, we need to do - git fetch git remote set-head <remote name> master git push --set-upstream <remote name> master # To track all the branches # Rename git mv <old filename> <new filename> # Rename a file git branch -m <newname> # Rename a branch while working on branch git branch -m <oldname> <newname> # Rename a branch from outside the branch # Signing commits git config --global commit.gpgsign true git config --global --edit # To change the settings # Forks git remote add upstream <url> git fetch upstream git checkout master git merge upstream/master # gitignore git rm --cached <file name> # To ignore a file that is already checked in git rm --cached -r <folder name> # To ignore a folder that is already checked in git config --global core.excludesfile ~/.gitignore_global # And put in the files you want to ignore # Rewriting git history git rebase upstream/master git rebase <branch name> # To bring all the commits forward git rebase -i HEAD~n # n for which changes you want to make # reword - to rename a commit # drop - to delete a commit # squash - to combine commits and give a name # fixup - to combine commits and keep the name of base commit # edit - to stop execution of rebase and if you wish, split the commits # To reorder commits, just change their order in interactive rebase prompt","title":"Extras"},{"location":"terminal/dotfiles/","text":"Dotfiles \u00b6 This blog post contains my daily used dotfiles which I do not wish to lose. .gitconfig \u00b6 [user] name = Sayam753 email = sayamkumar753@yahoo.in signingkey = 69184FFA974E07E4 [core] excludesfile = /Users/user/.gitignore_global [commit] gpgsign = true [gpg] program = gpg2 .gitignore_global \u00b6 .history .vscode .mypy_cache .DS_Store .pylintrc \u00b6 [DEFAULT] init-hook= import pylint_venv pylint_venv.inithook() .tmux.conf \u00b6 set -g default-terminal \"xterm-256color\" set -g prefix C-z unbind C-b bind C-z send-prefix .vscode_settings.json \u00b6 { // Editor options \"editor.wordWrap\" : \"off\" , \"editor.minimap.enabled\" : false , \"editor.detectIndentation\" : false , \"editor.suggestSelection\" : \"first\" , // Workbench options \"workbench.settings.editor\" : \"json\" , \"workbench.sideBar.location\" : \"left\" , \"workbench.iconTheme\" : \"vscode-icons\" , \"workbench.settings.openDefaultSettings\" : true , // Terminal options \"terminal.integrated.shell.osx\" : \"/bin/zsh\" , \"terminal.integrated.fontFamily\" : \"Inconsolata-g for Powerline\" , // Python options \"python.pythonPath\" : \"/usr/local/bin/python3\" , \"python.venvPath\" : \"/Users/user/Env\" , // Autopep8 options \"python.languageServer\" : \"Microsoft\" , \"editor.formatOnSave\" : true , \"python.formatting.provider\" : \"black\" , \"python.formatting.blackPath\" : \"/usr/local/bin/black\" , \"python.formatting.blackArgs\" : [ \"--line-length=100\" ], // Pylint options \"python.linting.enabled\" : true , \"python.linting.pylintEnabled\" : true , \"python.linting.pylintUseMinimalCheckers\" : false , \"python.linting.pylintPath\" : \"/usr/local/bin/pylint\" , \"python.linting.pylintCategorySeverity.refactor\" : \"Information\" , \"python.linting.pylintArgs\" : [ \"--enable=F, E, C, R, W\" , ], // Pydocstyle options \"python.linting.pydocstyleEnabled\" : true , \"python.linting.pydocstylePath\" : \"/usr/local/bin/pydocstyle\" , \"python.linting.pydocstyleArgs\" : [ \"--convention=numpy\" ], // Mypy options \"python.linting.mypyEnabled\" : true , \"python.linting.mypyPath\" : \"/usr/local/bin/mypy\" , \"python.linting.mypyArgs\" : [ \"--ignore-missing-imports\" ], // Misc \"git.confirmSync\" : false , \"C_Cpp.updateChannel\" : \"Insiders\" , \"files.exclude\" : { \"**/.classpath\" : true , \"**/.project\" : true , \"**/.settings\" : true , \"**/.factorypath\" : true , \"**/.history\" : true , \"**/.idea\" : true , \"**/.mypy_cache\" : true , \"**/__pycache__\" : true }, } .zshrc \u00b6 # Customizing prompt autoload -Uz vcs_info autoload -U colors && colors precmd () { vcs_info } zstyle ':vcs_info:git:*' formats '%F{011}(\ue0a0 %b)%f' setopt PROMPT_SUBST PROMPT = '%F{166}Sayam%f:%F{040}%1~%f ${vcs_info_msg_0_}%{$reset_color%}$ ' # Personal configurations source /usr/local/bin/virtualenvwrapper.sh export WORKON_HOME = ~/Env alias p = python3 alias jn = \"jupyter notebook\" ## Exports to deal with servers export LANG = \"en_US.UTF-8\" export LC_COLLATE = \"en_US.UTF-8\" export LC_CTYPE = \"en_US.UTF-8\" export LC_MESSAGES = \"en_US.UTF-8\" export LC_MONETARY = \"en_US.UTF-8\" export LC_NUMERIC = \"en_US.UTF-8\" export LC_TIME = \"en_US.UTF-8\"","title":"Dotfiles"},{"location":"terminal/dotfiles/#dotfiles","text":"This blog post contains my daily used dotfiles which I do not wish to lose.","title":"Dotfiles"},{"location":"terminal/dotfiles/#gitconfig","text":"[user] name = Sayam753 email = sayamkumar753@yahoo.in signingkey = 69184FFA974E07E4 [core] excludesfile = /Users/user/.gitignore_global [commit] gpgsign = true [gpg] program = gpg2","title":".gitconfig"},{"location":"terminal/dotfiles/#gitignore_global","text":".history .vscode .mypy_cache .DS_Store","title":".gitignore_global"},{"location":"terminal/dotfiles/#pylintrc","text":"[DEFAULT] init-hook= import pylint_venv pylint_venv.inithook()","title":".pylintrc"},{"location":"terminal/dotfiles/#tmuxconf","text":"set -g default-terminal \"xterm-256color\" set -g prefix C-z unbind C-b bind C-z send-prefix","title":".tmux.conf"},{"location":"terminal/dotfiles/#vscode_settingsjson","text":"{ // Editor options \"editor.wordWrap\" : \"off\" , \"editor.minimap.enabled\" : false , \"editor.detectIndentation\" : false , \"editor.suggestSelection\" : \"first\" , // Workbench options \"workbench.settings.editor\" : \"json\" , \"workbench.sideBar.location\" : \"left\" , \"workbench.iconTheme\" : \"vscode-icons\" , \"workbench.settings.openDefaultSettings\" : true , // Terminal options \"terminal.integrated.shell.osx\" : \"/bin/zsh\" , \"terminal.integrated.fontFamily\" : \"Inconsolata-g for Powerline\" , // Python options \"python.pythonPath\" : \"/usr/local/bin/python3\" , \"python.venvPath\" : \"/Users/user/Env\" , // Autopep8 options \"python.languageServer\" : \"Microsoft\" , \"editor.formatOnSave\" : true , \"python.formatting.provider\" : \"black\" , \"python.formatting.blackPath\" : \"/usr/local/bin/black\" , \"python.formatting.blackArgs\" : [ \"--line-length=100\" ], // Pylint options \"python.linting.enabled\" : true , \"python.linting.pylintEnabled\" : true , \"python.linting.pylintUseMinimalCheckers\" : false , \"python.linting.pylintPath\" : \"/usr/local/bin/pylint\" , \"python.linting.pylintCategorySeverity.refactor\" : \"Information\" , \"python.linting.pylintArgs\" : [ \"--enable=F, E, C, R, W\" , ], // Pydocstyle options \"python.linting.pydocstyleEnabled\" : true , \"python.linting.pydocstylePath\" : \"/usr/local/bin/pydocstyle\" , \"python.linting.pydocstyleArgs\" : [ \"--convention=numpy\" ], // Mypy options \"python.linting.mypyEnabled\" : true , \"python.linting.mypyPath\" : \"/usr/local/bin/mypy\" , \"python.linting.mypyArgs\" : [ \"--ignore-missing-imports\" ], // Misc \"git.confirmSync\" : false , \"C_Cpp.updateChannel\" : \"Insiders\" , \"files.exclude\" : { \"**/.classpath\" : true , \"**/.project\" : true , \"**/.settings\" : true , \"**/.factorypath\" : true , \"**/.history\" : true , \"**/.idea\" : true , \"**/.mypy_cache\" : true , \"**/__pycache__\" : true }, }","title":".vscode_settings.json"},{"location":"terminal/dotfiles/#zshrc","text":"# Customizing prompt autoload -Uz vcs_info autoload -U colors && colors precmd () { vcs_info } zstyle ':vcs_info:git:*' formats '%F{011}(\ue0a0 %b)%f' setopt PROMPT_SUBST PROMPT = '%F{166}Sayam%f:%F{040}%1~%f ${vcs_info_msg_0_}%{$reset_color%}$ ' # Personal configurations source /usr/local/bin/virtualenvwrapper.sh export WORKON_HOME = ~/Env alias p = python3 alias jn = \"jupyter notebook\" ## Exports to deal with servers export LANG = \"en_US.UTF-8\" export LC_COLLATE = \"en_US.UTF-8\" export LC_CTYPE = \"en_US.UTF-8\" export LC_MESSAGES = \"en_US.UTF-8\" export LC_MONETARY = \"en_US.UTF-8\" export LC_NUMERIC = \"en_US.UTF-8\" export LC_TIME = \"en_US.UTF-8\"","title":".zshrc"}]}